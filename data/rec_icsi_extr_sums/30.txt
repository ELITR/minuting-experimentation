F:  I was gonna ask Adam to , uh , say if he thought anymore about the demo stuff 
F:  because <inbreath> it occurred to me that this is late May and the DARPA meeting is in  mid July . 

F:  I know that we were gonna do something with the transcriber interface is one thing , 

G:  Well , we were gonna do a mock - up , like , question answering or something , I thought , 

C:  um , it looks like the vocal tract length normalization is working beautifully , actually , 

D:  Now the  the  You saw the note that the PLP now is getting basically the same as the MFCC . 

F:  Is there stuff that 's happened about , um , uh , the  SRI recognizer et cetera , 

F:  Y y you guys were doing a bunch of experiments with different front - ends and then with  

C:  Because in all our previous experiments , we had the  uh , you know , we were essentially cheating by having the , um , you know , the h the hand - segmentations as the basis of the recognition . 
C:  And so now with Thilo 's segmenter working so well , I think we should  consider doing a  

G:  And even  The good thing is that since you , um , have high recall ,  even if you have low precision cuz you 're over - generating , that 's good 

G:  but <breath> I know that if we run recognition unconstrained on a whole waveform , we do very poorly because we 're  we 're getting insertions in places what  that you may well be cutting out . 
G:  So we do need some kind of pre - segmentation . 

G:  And , yeah , using Thilo 's , you know , posteriors or some kind of  or  

C:  We should  we should consider doing some extra things , like , um , you know , retraining or adapting the  <breath> the models for background noise to the  to this environment , for instance . 

D:  And so he talked it over with the transcriber 
D:  and the transcriber thought that the easiest thing for them would be if there was a beep and then the nu a number , a digit , and then a beep , uh , at the beginning of each one 

D:  And , um , <inbreath> so Adam wrote a little script to generate those style , uh , beeps 

D:  So , um , we  We 're gonna send them one more sample meeting , 
D:  uh , and Thilo has run his segmentation . 
D:  Adam 's gonna generate the chunked file . 
D:  And then , um , I 'll give it to Brian and they can try that out . 
D:  And when we get that back we 'll see if that sort of fixes the problem we had with , uh , too many beeps in the last transcription . 

F:  OK . Do w do  what  Do you have any idea of the turn - around on  on those steps you just said ? 

D:  The last one seemed like it took a couple of weeks . 
D:  Um , maybe even three . 
D:  Uh , that 's just the I B M side . 

E:  It should @ @ be finished today or something . Yeah . 

F:  e e u u  The reason I 'm asking is because , uh , Jane and I have just been talking , and she 's just been doing . <laugh> Uh , e a , you know , further hiring of transcribers . 
F:  And so we don't sort of really know exactly what they 'll be doing , how long they 'll be doing it , and so forth , 
F:  because right now she has no choice but to operate in the mode that we already have working . 

F:  I mean in particular I would  I would really hope that when we do this DARPA meeting in July that we sort of have  we 're  we 're into production mode , somehow  
F:  You know , that we  we actually <breath> have a stream going and we know how  how well it does and how  and how it operates . 

A:  OK . So , um , we  <breath> Uh , the transcribers have continued to work past what I 'm calling " set one " , 

A:  but , uh , they 've gotten five meetings done in that set . 

A:  I hired two transcribers today . I 'm thinking of hiring another one , 
A:  which will  because we 've had a lot of attrition . 

F:  but if we hire f you know , f we have five on staff  five or six on staff at any given time , then <inbreath> it 's a small enough number so we can be flexible either way . 

A:  The  the  the pre - segmentations are so much  are s so extremely helpful . 

A:  But  but actually i it 's so correct for so much of the time , that it 's an enormous time saver 
A:  and it just gets tweaked a little around the boundaries . 

A:  Wha - what you 'd really like is that they started with pre - segmented and were pre - segmented all the way through . 

G:  Yeah . As long as we have a record , I guess , of the original  automatic one , we can always find out how well  we would do fr from the recognition side by using those boundaries . 

D:  Yeah . So , um , uh , Jane and Adam and I had a meeting where we talked about the reorganization of the  directory structure for all of the meeting  

D:  For all the Meeting Recorder data . 

D:  And then , Jane also s prepared a  um , started getting all of the  the meetings organized , so she prepared a  <breath> a spreadsheet , which I spent the last couple of days adding to . 

D:  been putting it into , uh , a spreadsheet <breath> with start time , the date , the old meeting name , the new meeting name , the number of speakers , the duration of the meeting , comments , you know , what its transcription status is , all that kind of stuff . 
D:  And so , the idea is that we can take this and then export it as HTML and put it on the Meeting Recorder web page 
D:  so we can keep people updated about what 's going on . 

D:  but <inbreath> so far , um , <inbreath> as of Monday , the fourteenth , um , we 've had a total number of meeting sixty - two hours of meetings that we have collected . 

D:  Uh , some other interesting things , average number of speakers per meeting is six . 

D:  and I 'm gonna have on here the total amount that 's been transcribed so far , 

D:  And <breath> it 'll also list , uh , like under the status , if it 's at IBM or if it 's at ICSI , uh , or if it 's completed or which ones we 're excluding 

F:  Um , we are gonna have this DARPA  meeting in the middle of July , 

F:  Well , le let me put in another sort of a milestone kind of  as  as I did with the , uh , uh  the  the pipeline . 

F:  given that we 've been  we 've given a couple public talks about it already , spaced by months and months , I think it 'd be pretty bad if we continued to say none of this is available . 

F:  Right . So we can s we  we wanna be able to say " here is a subset that is available right now " 
F:  and that 's has been through the legal issues and so forth . 

C:  And they don't have to approve , you know , th an edited version , they can just give their approval to whatever version 

C:  Yeah . But th I mean , the editing will continue . Presumably if  if s errors are found , they will be fixed , 

F:  Well , in principle , yes . But , I mean , i if  if  if somebody actually did get into some legal issue with it then we 

F:  I it  you know , there there is a point at which I agree it becomes ridiculous 

F:  so let  let me just suggest that  <inbreath> uh , off - line that , uh , the people involved figure it out and take care of it before it 's July . 

A:  Unfortunately , uh , in  in the sign thing that they signed , it says " transcripts " . 
A:  " You 'll be  you 'll be provided the transcripts when they 're available . " 

G:  because we couldn't use the non - native  all non - native meetings 
G:  and <breath> it 's , well , probably below threshold on enough data for us for the things we 're looking at 
G:  because the <breath> prosodic features are  very noisy and so you  you need a lot of data in order to model them . 
G:  Um , so we 're starting to see some patterns 

G:  But we did find that some of the features that , I gue Jane would know about , that are expressing sort of the <breath> distance of , um , <mouth> boundaries from peaks in the utterance and <breath> some  local , um , range  pitch range effects , like how close people are to their floor , are showing up in these classifiers , 
G:  which are also being given some word features that are cheating , cuz they 're true words . 
G:  Um , so these are based on forced alignment . 

G:  spurts is not cheating except that of course you know the real words , 

G:  I guess you reported on some te punctuation type  
G:  finding sentence boundaries , finding disfluency boundaries , 

B:  Like , there 's a lot of different features you could just pull out . 

C:  I mean that wouldn't be cheating because you can detect pause  pretty well within the time . 

G:  and then I had done some work on finding from the foreground speech whether or not someone was likely to interrupt , 
G:  so where  you know , if I 'm talking now and someone  and  and Andreas is about to interrupt me , is he gonna choose a certain place in my speech , either prosodically or word - based . 
G:  And there the prosodic features actually showed up 

G:  So that means that overall , um , it wasn't just modeling Morgan , or it wasn't just modeling a single person , 

G:  So , I gave everybody <breath> a short version of their name . 

G:  even though the word features were available . 
G:  And a neat thing there too is I tried some  <breath> putting the speaker  

G:  The other thing that was interesting to me is that the pitch features are better than in Switchboard . 
G:  And I think that really is from the close - talking mikes , 
G:  cuz the pitch processing that was done has much cleaner behavior than  than the Switchboard telephone bandwidth . 
G:  Um . Well , first of all , the pitch tracks are m have less , um , halvings and doublings than  than Switchboard 
G:  and there 's a lot less dropout , 
G:  so if you ask how many regions where you would normally expect some vowels to be occurring <breath> are completely devoid of pitch information , 
G:  in other words the pitch tracker just didn't get a high enough probability of voicing for words  

D:  Could it have to do with the  the lower frequency cut - off on the Switchboard ? 

G:  Ma - maybe . I mean , the tele we had telephone bandwidth for Switchboard 
G:  and we had the an annoying sort of telephone handset movement problem that I think may also affect it . 
G:  So we 're just getting better signals in  in this data . 

C:  you know , Chuck and I talked and the  @ @  next thing to do is probably to tune the  um , the size of the Gaussian system , um , @ @  to  to this  to this feature vector , which we haven't done at all . 
C:  We just used the same <breath> configuration as we used for the  <breath> for the standard system . 
C:  And , <breath> for instance , uh , Dan  @ @  Dan just sent me a message saying that CMU used , um , <mouth> something like ten Gaussians per cluster  
C:  You know , each  each mixture has ten  Gaussians 

D:  Hmm . We 're using sixty - four , 

C:  so that 's <breath> obviously a big difference 

C:  and give very poorly trained , uh , you know , Gaussians that way , 

C:  The turn - around time on the training when we train only the  a male system with , uh , you know , our small training set , is <breath> less than twenty - four hours , 

C:  But the PLP features work  um , uh , you know , continue to improve the , 

C:  As I said before , the  uh using Dan 's , uh , uh , vocal tract normalization option works very well . 
C:  So , um , @ @  I ran one experiment where we 're just <breath> did the vocal tract le normalization only in the test data , 
C:  so I didn't bother to retrain  the models at all , 

C:  which is about what we get with  <breath> uh , with , you know , just @ @  actually doing both training and test normalization , um , with , um , <mouth> the , uh  <mouth> uh , with the standard system . 
C:  So , in a few hours we 'll have the numbers for the  for retraining everything with vocal tract length normalization 

C:  So , it looks like the P L - fea P features  do very well now with  after having figured out all these little tricks to  to get it to work . 

C:  Right . And  and what that suggests also is of course that the current Switchboard  MLP isn't trained on very good features . 

C:  Well , but if you add them all up you have , uh , almost five percent difference now . 

F:  And I think I agree with you that if we fixed lots of different things and they would all add up , we would probably have a  a  a competitive system . 
F:  But I think not that much of it is due to the front - end per se . 
F:  I think maybe a couple percent of it is , as far as I can see from this . 

C:  eh At this point I 'm as I mean , you know  e I 'm wondering is it  Can we expect , uh , a tandem system to do better than a properly trained  you know , a Gaussian system trained directly on the features with , you know , the right ch choice of  parameters ? 

C:  But there the main point is that , um , you know , it took us a while but we have the procedure for coupling the two systems <inbreath> debugged now 
C:  and  I mean , there 's still conceivably some bug somewhere in the way we 're feeding the tandem features  
C:  uh , either generating them or feeding them to this  to the <breath> SRI system , 

D:  Yeah . There could be a bug in  in the  somewhere before that . 

C:  that  that  I think that 's  this  that 's essentially the same as we use with the ce with the P L P fe features . 

F:  Yeah . <inbreath> I  I would actually double check with Stephane at this point , 

D:  Yeah . It 's hard with features , 
D:  cuz you don't know what they should look like . 
D:  I mean , you can't just , like , print the  the values out in ASCII and , you know , look at them , see if they 're  

F:  Not unless you had a lot of time 

F:  The other thing , uh , just to mention that Stephane  this was an innovation of Stephane 's , which was a pretty neat one , 
F:  uh , and might particularly apply <clears throat> here , given all these things we 're mentioning . 
F:  Um , Stephane 's idea was that , um , discriminant , uh , approaches are great . 
F:  Even the local ones , given , you know , these potential outer loops which , you know , you can convince yourself turn into the global ones . 
F:  Um , however , there 's times when it  is not good . 
F:  Uh , when  something about the test set is different enough from the training set that  that , uh , the discrimination that you 're learning is  is  is not a good one . 
F:  So , uh , his idea was to take as the input feature vector to the , uh , Gaussian mixture system , <breath> uh , a concatenation of the neural net outputs and the regular features . 

G:  to  to try to get a non - cheating version of how all this would work . 

F:  Has  has , uh  ? We just  I think , just talked about this the other day , but h has  has anybody had a chance to try changing , uh , insertion penalty sort of things with the  with the , uh  <breath> uh , using the tandem system input for the  ? 

G:  I mean , are we trying to do them <mike noise> in synchrony ? 
G:  That might be fun . 

F:  Well , it 's  <breath-laugh> it 's  it 's not  You know , it 's not gonna work out 
F:  but we could  we could just , uh , uh , see if we find a rhythm , 

