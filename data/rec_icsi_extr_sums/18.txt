D:  Um , so , uh , let 's see , we were having a discussion the other day , maybe we should bring that up , about uh , the nature of the data that we are collecting . 

F:  um , and was thinking of this mostly just so that we could do research on this data um , since we 'll have a new  this new student di does wanna work with us , 

D:  uh @ @ that uh , we should have a fair amount of data that is um , collected for the same meeting , so that we can , 

F:  he 's  comes from a signal - processing background , 

F:  cuz he 's very interested in higher level things , like language , and disfluencies and all kinds of eb maybe prosody , 

F:  um , but I 'd rather try to get more regular meetings of types that we know about , and hear , then sort of a mish - mosh of a bunch of one  one - time  

F:  So . Then I was um , talking to Morgan about some  new proposed work in this area , sort of a separate issue from what the student would be working on where I was thinking of doing some kind of summarization of meetings or trying to find cues in both the utterances and in the utterance patterns , 
F:  like in numbers of overlaps and amount of speech , 
F:  sort of raw cues from the interaction that can be measured from the signals and from the diff different microphones that point to sort of hot spots in the meeting , or things where stuff is going on that might be important for someone who didn't attend to  listen to . 
F:  And in that uh , regard , I thought we definitely w will need  it 'd b it 'd be nice for us to have a bunch of data from a few different domains , or a few different kinds of meetings . 

F:  like the front - end meeting  and maybe a networking  group meeting . 
F:  So  So if that were the case then I think we 'd have enough . 

F:  but  rather we should have different meetings by the same group but hopefully that have different summaries . 

F:  but also data where we hold some parameters constant or fairly similar , 
F:  like a meeting about of people doing a certain kind of work where at least half the participants each time are the same . 

D:  Uh , for other kinds of research , particularly the acoustic oriented research , I actually feel the opposite need . 

D:  I 'd like to have many different speakers . 
D:  So , um I think I would also very much like us to have a fair amount of really random scattered meetings , of somebody coming down from campus , and  and uh , 

D:  but if we only get one or two from each group , that still could be useful acoustically just because we 'd have close and distant microphones with different people . 

F:  It has to be a  a pre - existing meeting ,  like a meeting that would otherwise happen anyway . 

D:  So I was  I was thinking more in terms of talking to professors uh , and  and  and uh , senior uh , uh , d and uh , doctoral students who are leading projects and offering to them that they have their  hold their meeting down here . 

B:  Well , I think that , um  I think that the only thing we should say in the advertisement is that the meeting should be held in English . 

E:  If you have people who are using English as a  as an interlanguage because they  they don't  uh , they can't speak in their native languages and  but their interlanguage isn't really a match to any existing , uh , language model , 

E:  And I 'm not objecting to accents . 

E:  I 'm  I  what I think is that why not have the corpus , since it 's so expensive to put together , uh , useful for the widest range of  of central corp things that people generally use corpora for and which are , you know , used in computational linguistics . 

D:  I mean , it  it  I think that if we 're aiming at  at uh , groups of graduate students and professors and so forth who are talking about things together , and it 's from the Berkeley campus , probably most of it will be OK , 

E:  And my point in m in my note to Liz was I think that undergrads are an iff iffy population . 

F:  Well , Morgan , you were mentioning that Mari may not use the k equipment from IBM if they found something else , cuz there 's a  
F:  Cuz I mean , one remote possibility is that if we st if we inherited that equipment , if she weren't using it , could we set up a room in the linguistics department ? 
F:  And  and I mean , there  there may be a lot more  or  or in psych , or in comp wherever , in another building where we could um , record people there . 

D:  Well , the other thing  Yeah , I mean the other thing that I was hoping to do in the first place was to turn it into some kind of portable thing so you could wheel it around . 

D:  Uh , i We realized in discussion that the other thing is , what about this business of distant and close microphones ? 
D:  I mean , we really wanna have a substantial amount recorded this way , 

D:  But  what about  For th for these issues of summarization , a lot of these higher level things you don't really need the distant microphone . 

B:  And you don't really need the close microphone , you mean . 

F:  you know , each person who 's interested in  I mean , we have a cou we have a bunch of different , um , slants and perspectives on what it 's useful for , um , they need to be taking charge of making sure they 're getting enough of the kind of data that they want . 
F:  And  So in my case , um , I think there w there is enough data for some kinds of projects and not enough for others . 

F:  And other people need to do that for themselves , uh , h or at least discuss it so that we can find some optimal  

F:  So I  do think that long term you should always try to satisfy the greatest number of  of interests and have this parallel information , which is really what makes this corpus powerful . 

D:  Uh but I  I think that the uh <inbreath> i We can't really underestimate the difficulty  shouldn't really u underestimate the difficulty of getting a setup like this up . 

D:  If you 're talking about something simple , where you throw away a lot of these dimensions , then you can do that right away . 

D:  I think the first priority should be to pry  to get  try to get people to come here . 

F:  But the issue is you definitely wanna make sure that the kind of group you 're getting is the right group 

B:  Um , I had a  I spoke with some people up at Haas Business School who volunteered . 
B:  Should I pursue that ? 

F:  Oh , definitely , yeah . 

F:  I 'd love to get people that are not linguists or engineers , cuz these are both weird  

D:  The o the o the other  The other thing is , uh , that we  we talked about is give to them  uh , burn an extra CD - ROM . 

E:  We could burn it after it 's been cleared with the transcript stage . 

E:  So , after the transcript screening phase . 

D:  and <breath> then um , I guess another topic would be <breath> where are we in the whole disk resources  question 

D:  We are getting , uh , another disk rack and  and four thirty - six gigabyte disks . 
D:  Uh  so  uh  but that 's not gonna happen instantaneously . 

D:  Uh , OK , @ @  So , uh , then I guess th the last thing I 'd had on my  my agenda was just to hear  hear an update on <inbreath> what  what Jose has been doing , 

C:  I have , eh , <clears throat> The result of my work during the last days . 

C:  But for me , eh is interesting because , eh , eh , here 's i is the demonstration of the overlap , eh ,  problem . 

C:  Eh , this information is very  very useful . 
C:  Because <laugh> you have the  the  the distribution , now . 

C:  It 's a real problem ,  a frequently problem  uh , because you have overlapping zones eh , eh , eh , all the time . 

B:  Throughout the meeting . 

C:  Eh , by a moment I have , eh , nnn , the , eh ,  n I  I did a mark of all the overlapped zones in the meeting recording , 

C:  Heh ? That 's eh , yet b b Yeah , by  b b by hand  by hand because , eh , <mouth> eh  " Why . " 

C:  but , eh , my idea is , eh , is very interesting to  to work  in  in the line of , eh , automatic segmenter . 

B:  And so are you planning to do that or have you done that already ? 

C:  No , I  I  plan to do that . 

C:  Now , <mouth> eh , I need ehm , <mouth> to detect eh all the overlapping zones exactly . 

C:  Eh , um , <breath> <mouth> eh  This information eh , with eh , exactly time marks eh , for the overlapping zones <inbreath> eh  overlapping zone , and eh , a speaker  a  a pure speech eh , eh , speaker zone . 
C:  I mean , eh zones eh of eh speech of eh , one speaker without any  any eh , noise eh , any  any acoustic event eh that eh , eh , w eh , is not eh , speech , real speech . 

C:  for that , because my  my idea is to  to study the nnn  the  <mouth> the set of parameters eh , what , eh , are more m more discriminant to eh , classify . 
C:  the overlapping zones in cooperation with the speech  eh zones . 
C:  The idea is  to eh  to use  eh , I 'm not sure to  eh yet , but eh my idea is to use a  a cluster  <mouth> eh algorithm or , nnn , a person strong in neural net algorithm to eh  to eh study 
C:  what is the , eh , the property of the different feat eh feature , eh , to classify eh speech and overlapping eh speech . 

C:  And my control set eh , will be the eh , silence , silence without eh , any  any noise . 

C:  eh , eh , event eh , which , eh , eh , has , eh eh , a hard effect of distorti spectral distortion in the  in the eh  speech . 

C:  I have , eh ,  two hundred and thirty , more or less , overlapping zones , and is similar to  to this information , 

C:  because I  I want to put , eh , for eh , each frame a label  indicating . It 's a sup supervised and , eh , hierarchical clustering process . 
C:  I  I  I put , eh , eh , for each frame <noises from writing on whiteboard> a label indicating what is th the type , what is the class , eh , which it belong . 

C:  a I  I  I ha I h I  I put the mark by hand , 
C:  because , eh , <mouth> my idea is , eh , in  in the first session , I need , eh ,  I  I need , eh , to be sure that the information eh , that , eh , I  I will cluster , is  is right . Because , eh , eh , if not , eh , I will  I will , eh , return to the speech file to analyze eh , what is the problems , 

E:  So you 're ignoring overlapping events unless they 're speech with speech . 

C:  Yeah . 

C:  In the  in the future , the  the idea is to  to extend  the class , 

D:  But steady - state noises are part of the background . 

D:  Cuz you 're calling  what you 're calling " event " is somebody coughing <breath> or clicking , or rustling paper , or hitting something , which are impulsive noises . 

D:  Right , it 's  I mean , it 's  " Background " might be  might be a better word than " silence " . 

C:  And eh I am going  to prepare a test bed , eh , well , eh , a  a set of  feature structure eh , eh , models . 

C:  so  so  on  because I have a pitch extractor yet . 

C:  And in a first eh , nnn , step in the investi in the research in eh , my idea is try to , eh , to prove , what is the performance of the difference parameter , eh  to classify  the different , eh , what is the  the  the  the front - end approach to classify eh , the different , eh , frames of each class  eh and what is the  the , nnn , nnn , nnn , eh , what is the , the error  eh , of the data 

B:  Supervised clustering . 

C:  and the second  is try to  eh , to use  some ideas eh , similar to the linear discriminant analysis . 

C:  Eh , the  the  the classifier is  nnn by the moment is eh  is eh , similar , nnn , that the classifier used eh , in a quantifier  vectorial quantifier is eh , used to  to eh , some distance  to  to put eh , a vector eh , in  in a class different . 

C:  A another possibility it to use eh a netw netw a neural network . 

D:  but the thing is here he 's  he 's not  he 's not like he has one you know , a bunch of very distinct variables , like pitch and this  he 's talking about , like , a all these cepstral coefficients , and so forth , 
D:  in which case a a any reasonable classifier is gonna be a mess , and it 's gonna be hard to figure out what  what uh  

D:  I would take just a few features . Instead of taking all the MFCC 's , or all the PLP 's or whatever , I would just take a couple . 

D:  Like  like C - one , C - two , something like that , so that you can visualize it . 
D:  and look at these different examples and look at scatter plots . 
D:  OK , so before you do  build up any kind of fancy classifiers , just take a look in two dimensions , at how these things are split apart . 

D:  So if you 're just looking at a frame and a time , you don't know anything about , you know , the structure of it over time , and so you may wanna build @ @  build a Markov model of some sort uh , or  or else have features that really are based on um on  on some bigger chunk of time . 

D:  But don't uh anyway , this is my suggestion , is don't just , you know , throw in twenty features at it , the deltas , and the delta del and all that into some classifier , even  even if it 's K - nearest - neighbors , you still won't know 

D:  look at  at som some picture that shows you , " Here 's  These things uh , uh are  offer some separation . " <breath> And , uh , in LPC , uh , the thing to particularly look at is , I think  is something <breath> like , uh , the residual  

C:  But eh eh I  I understand that you  your objective is  to eh classify , to know that eh that zone  not is only  a new zone in the  in the file , that eh you have eh , but you have to  to  to know that this is overlap zone . 
C:  because in the future you will eh try to  to process that zone with a non - regular eh eh speech recognizer model , I suppose . 
C:  you  you will pretend  to  to  to process the overlapping z eh zone with another kind of algorithm 
C:  because it 's very difficult to  to  to obtain the transcription  from eh using eh eh a regular , normal speech recognizer . 

C:  A model to detect more acc the mor most accurately possible that is p uh , will be possible the , eh  the mark , the change 
C:  and another  another model will @ @  or several models , to try s but  eh several model eh robust models , sample models to try to classify the difference class . 

D:  Because what we had before for  for uh , speaker change detection did not include these overlaps . 
D:  So the first thing is for you to  to build up something that will detect the overlaps . 

D:  so if you look at  <breath> Suppose you look at first and second - order cepstral coefficients for some one of these kinds of things and you find that the first - order is much more effective than the second , <breath> and then you look at the third and there 's not  and not too much there , <breath> you may just take first and second - order cepstral coefficients , 

D:  And with LPC , I think LPC per se isn't gonna tell you much more than  than  than the other , maybe . 
D:  Uh , and uh on the other hand , the LPC residual , the energy in the LPC residual , <breath> will say how well , uh <breath> the low - order LPC <breath> model 's fitting it , which should be <breath> pretty poorly for two two or more <breath> people speaking at the same time , and it should be pretty well , for w for  for one . 

D:  And then you can do decision trees or whatever to see how they combine . 

D:  And so , if you can get  @ @  Uh again , my prescription would be that you would , with a mixed signal , you would take a collection of possible uh , features <breath> look at them , look at how these different classes that you 've marked , separate themselves ,  <breath> and then collect , uh in pairs , <breath> and then collect ten of them or something , and then proceed <breath> with a bigger classifier . 
D:  And then if you can get that to work well , then you go to the other signal . 

C:  but eh what is the  the  the relation of eh  of the <mouth> performance when eh you use eh the , eh eh speech file the PDA speech files . 

D:  Well , that  that  that 's another reason why very simple features , things like energy , and things  things like harmonicity , and <breath> residual energy are uh , yeah are  are better to use than very complex ones because they 'll be more reliable . 

A:  because can't you , couldn't you like use beam - forming or something to detect speaker overlaps ? 

D:  I think  <laugh> I  I think  I think it 's  it 's  it 's a  it 's an additional interesting question . 

B:  Well , if you used the array , rather than the signal from just one . 

D:  That 's  a good thing to consider . 

D:  So if there 's a distributed beam pattern , then it looks more like it 's  it 's uh , multiple people . 

B:  And if there are multiple people talking , you 'll see two peaks . 

D:  It 's spread out . 

D:  But  <laugh> but  but the thing is , uh , one of the  at least one of the things I was hoping to get at with this is what can we do with what we think would be the normal situation if some people get together and one of them has a PDA . 

D:  but  you know if you can instrument a room , this is really minor league compared with what some people are doing , right ? 

D:  But , the reason why I haven't focused on that as the fir my first concern is because um , I 'm interested in what happens for people , random people out in some random place where they 're p having an impromptu discussion . 

D:  The other thing actually , that gets at this a little bit of something else I 'd like to do , is what happens if you have two P D 
D:  and they communicate with each other ? And then  You know , they 're in random positions , the likelihood that  I mean , basically there wouldn't be any  l likely to be any kind of nulls , if you even had two . If you had three or four it 's  Yeah . 

B:  I mean , not only can you do microphone arrays , but you can do all sorts of um multi - band as well . 

B:  Um and then also anonymity , how we want to anonymize the data . 

E:  the question becomes what symbol are you gonna put in there for everybody 's name , 
E:  and whether you 're gonna put it in the text where he says " Hey Roger " or are we gonna put that person 's anonymized name in instead ? 

E:  OK , well , but then there 's this issue of if we 're gonna use this for a discourse type of thing , then  and , you know , Liz was mentioning stuff in a previous meeting about gaze direction and who 's  who 's the addressee and all , then to have " Roger " be the thing in the utterance and then actually have the speaker identifier who was " Roger " be " Frank " , that 's going to be really confusing and make it pretty much useless for discourse analysis . 

E:  Um , how important is it for a person to be identified by first name versus full name ? 

E:  On the other hand , this is a small  this is a small pool , and people who say things about topic X e who are researchers and well - known in the field , they 'll be identifiable and simply from the  from the first name . 
E:  However , taking one step further back , they 'd be identifiable anyway , even if we changed all the names . 

E:  Now , it would be very possible for me to take those data put them in a  in a study , and just change everybody 's name for the purpose of the publication . 

D:  So again , th the issue is if you 're tracking discourse things , you know , if someone says , uh , uh , " Frank said this " and then you wanna connect it to something later , you 've gotta have this part where that 's " Frank colon " . 

E:  Yeah , and  and  you know , even more i i uh , immediate than that just being able to , uh  Well , it just seems like to track  track from one utterance to the next utterance who 's speaking and who 's speaking to whom , cuz that can be important . 

B:  Well , I would sug I  I  don't wanna change the names in the transcript , 
B:  but that 's because I 'm focused so much on the acoustics instead of on the discourse , and so I think that 's a really good point . 
B:  You 're right , this is going to require more thought . 

B:  How will we  how would the person who 's doing the transcript even know who they 're talking about ? 

B:  I mean , so so  how is that information gonna get labeled anyway ? 

B:  Well , the current one they don't do speaker identity . 
B:  because in NaturallySpeaking , or , excuse me , in ViaVoice , it 's only one person . 
B:  and so in their current conventions there are no multiple speaker conventions . 

E:  And within that , it may be that it 's sufficient to not uh change the  to not incorporate anonymization yet , but always , always in the publications we have to . 

D:  I  I think that we  we have a  need to have a consistent licensing policy of some sort , and  

D:  And th and the other thing is if  if  if Liz were here , <laugh> what she might say is that she wants to look if things that cut across between the audio and the dialogue , 

E:  Well , you see ? So , it 's complicated . 

E:  I think we have to think about w @ @  how . I think that this can't be decided today . 

B:  Because that would give you a mapping between the speaker 's real name and the tag we 're using , and we don't want  

E:  Then , uh , it seems to me that  Well , maybe I  uh it seems to me that if you change the name , the transcript 's gonna disagree with the audio , and you won't be able to use that . 

B:  Because if we made the  the transcript be the tag that we 're using for Roger , someone who had the transcript and the audio would then have a mapping between the anonymized name and the real name , and we wanna avoid that . 

