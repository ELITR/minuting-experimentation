F:  and <breath> the main thing that I was gonna ask people to help with today is  to give input on what kinds of database format we should  use in starting to link up things like word transcripts and annotations of word transcripts , 

C:  I mean , we  I sort of already have developed an XML format for this sort of stuff . 

F:  so anything that transcribers or discourse coders or whatever put in the signal , <breath> with time - marks for , like , words and phone boundaries and all the stuff we get out of the forced alignments and the recognizer . 

C:  So tha it has a single time - line , 

C:  And then you have a bunch of times . 

C:  And then , <writing on whiteboard> I also wanted to  to be i to be able to not specify specifically what the time was and just have a stamp . 

C:  And then somewhere la further down you might have something like an utterance tag 
C:  which has " start equals T - seventeen , 

C:  So what that 's saying is , we know it starts at this particular time . 

A:  So you 're essentially defining a lattice . 

C:  So you could  you could have some sort of other  other tag later in the file that would be something like , um , oh , I don't know ,  uh , <writing on whiteboard> " noise - type equals <writing on whiteboard> door - slam " . 

C:  And then , uh , <writing on whiteboard> you could either say " time equals a particular time - mark " or you could do other sorts of references . 

F:  and what do you do if you  just conceptually , if you get , um , transcriptions where the words are staying but the time boundaries are changing , cuz you 've got a new recognition output , 
F:  or s sort of  what 's the , um , sequence of going from the waveforms that stay the same , the transcripts that may or may not change , and then the utterance which  where the time boundaries that may or may not change  ? 

A:  because you could  you could  all you 'd have to change is the , <breath> um , time - stamps in the time - line without  without , uh , changing the I Ds . 

A:  That 's actually very nicely handled here 

A:  Except the time - line is gonna be huge . 

A:  suppose you have a phone - level alignment . 

C:  It 's parsing limitations . 
C:  I don't want to have this text file that you have to read in the whole thing to do something very simple for . 

C:  I think for word - level , this would be OK . 

C:  For lower than word - level , you 're talking about so much data that I just  I don't know . 

C:  or  or any frame - level stuff I would use P - file . 

C:  It 's ICS uh , ICSI has a format for frame - level representation of features . 

F:  That you could call  that you would tie into this representation with like an ID . 

C:  Or  or there 's a  there 's a particular way in XML to refer to external resources . 

D:  But what  what 's the advantage of doing that versus just putting it into this format ? 

C:  More compact , 

F:  I mean these are long meetings 

C:  Because you have a two - gigabyte limit on most O Ss . 

C:  So I think it  it 's debatable whether you want to do phone - level in the same thing . 
C:  But I think , a anything at frame - level , even P - file , is too verbose . 

D:  P - file for each frame is storing a vector of cepstral or PLP values , 

C:  It 's whatever you want , actually . 

C:  Built into it is the concept of  frames , utterances , sentences , that sort of thing , that structure . 
C:  And then also attached to it is an arbitrary vector of values . 

C:  And we have a lot of tools already to deal with it . 

C:  Man - pages and , uh , source code , and me . 

F:  but something sort of standard enough that , you know , if we start using this we can give it out , 
F:  other people can work on it , 

C:  I mean , it 's something that we developed at ICSI . 

C:  and  and , you know , we have a  well - configured system that you can distribute for free , 

F:  But I thought it would be good to get something that we can  that other people can use or adopt for their own kinds of encoding . 

F:  And especially for the prosody work , what  what it ends up being is you get features from the signal , 

F:  you want to recompute your features , um , and then keep the database up to date . 
F:  Or you change a word , or you change a <mouth> utterance boundary segment , which is gonna happen a lot . 
F:  And so I wanted something where  all of this can be done in a elegant way 
F:  and that if somebody wants to try something or compute something else , that it can be done flexibly . 

F:  it just has to be , you know , easy to use , and  

C:  We should look at ATLAS , 
C:  the NIST thing , 

C:  Th - there are sort of two choices . 
C:  Your  your file format can know about  know that you 're talking about language  and speech , which is what I chose , and time , 
C:  or your file format can just be a graph representation . 
C:  And then the application has to impose the structure on top . 
C:  So what it looked like ATLAS chose is , they chose the other way , 
C:  which was their file format is just nodes and links , 
C:  and you have to interpret what they mean yourself . 

C:  and I thought it was better if you 're looking at a raw file to be  t for the tags to say " it 's an utterance " , as opposed to the tag to say " it 's a link " . 

C:  I mean , I  I chose this for a couple reasons . 
C:  One of them is that it 's easy to parse . 

F:  Can you  But you can add to those structures if you  

C:  Sure . 

C:  So what  What NIST would say is that instead of doing this , you would say something like " link <writing on whiteboard> start equals , um , you know , some node ID , 

C:  and then " type " would be " utterance " . 

C:  end equals some other node ID " , 

C:  they 're developing a big infrastructure . 

A:  Do they already have something that 's  that would be useful for us in place ? 

C:  probably a year ago , 

C:  The last time I looked at it was a while ago , 

C:  And so , specifically they didn't have any external format representation at that time . 
C:  They just had the sort of conceptual  node  uh , annotated transcription graph , 

C:  Since then , they 've developed their own external file format , 

C:  Um , and apparently they 've also developed a lot of tools , 
C:  but I haven't looked at them . 
C:  Maybe I should . 
C:  Um , th what would  would  would  what would worry me is that maybe we might miss a little detail 

A:  I  I think if it 's conceptually close , and they already have or will have tools that everybody else will be using , I mean , <breath> it would be crazy to do something s you know , separate that  

C:  The other thing  the other way that I sort of established this was as easy translation to and from the Transcriber format . 

C:  So I 'll  I 'll take a closer look at it . 

C:  The  the  the  the point is  with this , though , is that you can't really add any supplementary information . 

F:  You have to make a different type . 

A:  Now  now how would you  how would you represent , um , multiple speakers in this framework ? 

C:  the way I had it is each turn or each utterance ,  I don't even remember now , had a speaker ID tag attached to it . 

C:  there 's a spea speaker tag up at the top which identifies them 

A:  But  but  so how in the NIST format do we express <breath> a hierarchical relationship between , um , say , an utterance and the words within it ? 

C:  Um , you would have another structure lower down than this that would be saying they 're all belonging to this ID . 

C:  Well , I think I 'm  I think w I had better look at it again 

A:  f Say , one segmentation is in terms of , um , you know , uh , sentences . 
A:  And another segmentation is in terms of , um , <mouth> I don't know ,  prosodic phrases . 
A:  And let 's say that they don't  nest . 

F:  So we would basically be sort of , um , taking the format and enriching it with things that we wanna query in relation to the words that are already in the file , 

C:  There 's a  standard again in XML , specifically for searching XML documents  structured X - XML documents , where you can specify both the content and the structural position . 

C:  It 's  it 's  you would use that to build your tool to do that sort of search . 

C:  No matter what format you choose , you 're gonna have the trou you 're gonna have the difficulty of relating the  the frame - level features  

D:  what about , um , the idea of using a relational database to , uh , store the information from the XML ? 

D:  which allows you to do all kinds of  good search things in there . 

C:  One of the things that ATLAS is doing is they 're trying to define an API which is independent of the back store , 
C:  so that , uh , you could define a single API and the  the storage could be flat XML files or a database . 
C:  My opinion on that is for the s sort of stuff that we 're doing ,  I suspect it 's overkill to do a full relational database , that , um , just a flat file and , uh , search tools I bet will be enough . 

C:  I guess I 'm just a little hesitant to try to go whole hog on sort of the  the whole framework that  that NIST is talking about , with ATLAS and a database and all that sort of stuff , 
C:  cuz it 's a big learning curve , just to get going . 

C:  If you wanted to use the structured XML query language , that 's a different thing . 

C:  What you would do is , someone would build a tool that used that as a library . 

F:  Um , one that Chuck mentioned is starting out with something that we don't have to start over , that we don't have to throw away if other people want to extend it for other kinds of questions , 
F:  and being able to at least get enough , uh , information out on  where we condition the location of features on information that 's in the kind of file that you  put up there . 

F:  Well , I guess , the two goals . 

C:  I mean , I have to look at it again to see whether it can really do what we want , 
C:  but if we use the ATLAS external file representation , um , it seems like it 's rich enough that you could do quick tools just as I said in Perl , 
C:  and then later on if we choose to go up the learning curve , we can use the whole ATLAS inter infrastructure , 

C:  I mean , th the reason I like P - file is I 'm already familiar with it , 

C:  But , I mean , it is just something we developed at ICSI . 

C:  There 's an API for it . 

C:  a bunch of libraries , P - file utilities . 

E:  if we  if we tie the overlap code to the first word in the overlap , then you 'll have a time - marking . 
E:  It won't  it 'll be independent of the time bins , 
E:  however these e evolve , shrink , or whatever , 

A:  but , when it comes to annotations , um , you often find yourself in the situation where you have  different annotations  of the same , say , word sequence . 

A:  And sometimes the word sequences even differ slightly because they were edited s at one place but not the other . 

A:  But , see , if you 'd annotate dialogue acts , you don't necessarily want to  or topics  you don't really want to be dealing with time - marks . 

A:  And  you want some tool , uh , that is able to sort of merge these different annotations back into a single , uh , version . 

C:  I  I don't see any way that file formats are gonna help us with that . 

A:  But once you have a file format , I can imagine writing  not personally , but someone writing a tool that is essentially an alignment tool , um , that mediates between various versions , 

C:  The hard part isn't the file format . 
C:  The hard part is specifying what you mean by " merge " . 

F:  These are big files . 

A:  It seems to me you want to keep the frame - level stuff separate . 

C:  I I 'm pretty sure that you can do that , 
C:  but I 'm forgetting the exact level of nesting . 

