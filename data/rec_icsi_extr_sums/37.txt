A:  OK , s so there is kind of summary of what has been done  

A:  Summary of experiments since , well , since last week 
A:  and also since the  we 've started to run  work on this . 
A:  Um .  So since last week we 've started to fill the column with um <mouth> uh features w with nets trained on PLP with on - line normalization 
A:  but with delta also , 

A:  but  we have more results to compare with network using without PLP 
A:  and  finally , hhh ,  um  ehhh  PL - uh delta seems very important . 

A:  uh  when we use the large training set using French , Spanish , and English , you have one hundred and six without delta 
A:  and eighty - nine with the delta . 

D:  a And again all of these numbers are with a hundred percent being , uh , the baseline performance , 

A:  Yeah , 

A:  But , actually , um , for English training on TIMIT is still better than the other languages . 

A:  so , it 's multi - English , 
A:  we have a ninety - one number , 
A:  and training with other languages is a little bit worse . 

A:  And , yeah , and here the gap is still more important between using delta and not using delta . 
A:  If y if I take the training s the large training set , it 's  we have one hundred and seventy - two , 
A:  and one hundred and four when we use delta . 
A:  Uh .  Even if the contexts used is quite the same , 

A:  except for the multi - English , which is always one of the best . 

A:  then we started to work on a large dat database containing , uh , sentences from the French , from the Spanish , from the TIMIT , from SPINE , uh from  uh English digits , and from Italian digits . 

A:  and  uh , actually we did this before knowing the result of all the data , 
A:  uh , so we have to to redo the uh  the experiment training the net with , uh PLP , but with delta . 

A:  We have also started feature combination experiments . 
A:  Uh many experiments using features and net outputs together . 
A:  And this is  The results are on the other document . 

D:  so , uh , you 've got some , uh , Xerox things to pass out ? 

A:  Yeah , I 'm sorry for the table , 
A:  but as it grows in size , uh , it . 

D:  Uh , so for th the last column we use our imagination . 

A:  So it 's the  kind of similar to the tandem that was proposed for the first . 
A:  The multi - stream tandem for the first proposal . 
A:  The second is using features and KLT transformed MLP outputs . 
A:  And the third one is to u use a single KLT trans transform features as well as MLP outputs . 

A:  Yeah , we ju just to be clear , the numbers here are uh recognition accuracy . 

B:  Yes , and the baseline  the baseline have  i is eighty - two . 

D:  Baseline is eighty - two . 

B:  And first in the experiment - one I  I do  I  I use different MLP , 
B:  and is obviously that the multi - English MLP is the better . 
B:  Um . for the ne  rest of experiment I use multi - English , 

B:  And I try to combine different type of feature , 
B:  but the result is that the MSG - three feature doesn't work for the Italian database 
B:  because never help to increase the accuracy . 

A:  Yeah , eh , actually , if w we look at the table , 
A:  the huge table , 
A:  um , we see that for TI - digits MSG perform as well as the PLP , 
A:  but this is not the case for Italian what  where the error rate is c is almost uh twice the error rate of PLP . 

A:  I don't know what exactly . 
A:  Perhaps the fact that the  the  there 's no low - pass filter , 

A:  or no pre - emp pre - emphasis filter 
A:  and that there is some DC offset in the Italian , 

A:  But  that we need to sort out if want to uh get improvement by combining PLP and MSG 
A:  because for the moment MSG do doesn't bring much information . 

D:  when you said the baseline system was uh , uh eighty - two percent , that was trained on what and tested on what ? 
D:  That was , uh Italian mismatched d uh , uh , digits , uh , is the testing , 
D:  and the training is Italian digits ? 

B:  Yeah . 

D:  So the " mismatch " just refers to the noise and  and , uh microphone and so forth , 

D:  So , um  So what that says is that in a matched condition , <sniff> we end up with a fair amount worse putting in the uh PLP . 
D:  Now w would  do we have a number , I suppose for the matched  
D:  I  I don't mean matched , 
D:  but uh use of Italian  training in Italian digits for PLP only ? 

A:  so this is  basically this is in the table . 
A:  Uh  so the number is fifty - two , 

B:  Another table . 

D:  Fifty - two percent . 

A:  Fift - So  No , it 's  it 's the  

D:  No , fifty - two percent of eighty - two ? 

A:  Of  of  of uh  eighteen  

B:  Eighty . 

A:  of eighteen . 
A:  So it 's  it 's error rate , basically . 

B:  It 's plus six . 

A:  It 's er error rate ratio . 

D:  Oh this is accuracy ! 

B:  Yeah . 

A:  Uh , so we have nine  nine  let 's say ninety percent . 

D:  Oh , I 'm sorry , 
D:  I k I keep getting confused 
D:  because this is accuracy . 

A:  Yeah , sorry . 

D:  And then adding the MSG does nothing , basically . 

A:  No . 

D:  So , um  So actually , the answer for experiments with one is that adding MSG , if you  uh does not help in that case . 

A:  Mm - hmm . 

D:  So then you 're assuming multi - English is closer to the kind of thing that you could use 
D:  since you 're not gonna have matching , uh , data for the  uh for the new  for the other languages and so forth . 

D:  So , it it 's still  it hurts you  seems to hurt you a fair amount to add in this French and Spanish . 
D:  I wonder why 

C:  Well Stephane was saying that they weren't hand - labeled , 
C:  the French and the Spanish . 

B:  first the feature are without delta and delta - delta , 
B:  and we can see that in the situation , uh , the MSG - three , the same help nothing . 
B:  And then I do the same 
B:  but with the delta and delta - delta  PLP delta and delta - delta . 

D:  What  what we 're saying is that one o one of the things that  
D:  I mean my interpretation of your  your s original suggestion is something like this , as motivation . 
D:  When we train on data that is in one sense or another , similar to the testing data , then we get a win by having discriminant training . 
D:  When we train on something that 's quite different , we have a potential to have some problems . 

D:  So I guess the other thing is to take  you know  if one were to take , uh , you know , a couple of the most successful of these , 

A:  Yeah , and test across everything . 

D:  Yeah , try all these different tests . 

A:  Um , discussion with Hynek , Sunil and Pratibha for trying to plug in their our  our networks with their  within their block diagram , 
A:  uh , where to plug in the  the network , uh , after the  the feature , 
A:  before as um a as a plugin or as a anoth another path , 

A:  actually Hynek would like to see , 
A:  perhaps if you remember the block diagram there is , uh , temporal LDA followed b by a spectral LDA for each uh critical band . 
A:  And he would like to replace these by a network 
A:  which would , uh , make the system look like a TRAP . 

A:  uh , I started multi - band MLP trainings , 

A:  So I take exactly the same configurations , 
A:  seven bands with nine frames of context , 
A:  and we just train on TIMIT , 
A:  and on the large database , 
A:  so , with SPINE and everything . 

A:  mmm , I 'm starting to train also , networks with larger contexts . 
A:  So , this would  would be something between TRAPS and multi - band 
A:  because we still have quite large bands , 
A:  and  but with a lot of context also . 

A:  we still have to work on Finnish , 
A:  um , basically , to make a decision on which MLP can be the best across the different languages . 
A:  For the moment it 's the TIMIT network , and perhaps the network trained on everything . 

A:  Uh , well , the next part of the document is , well , basically , a kind of summary of what  everything that has been done . 
A:  So . We have seventy - nine M L Ps trained on 

A:  ten  on ten different databases . 

A:  yeah !  As we mentioned , TIMIT is the only that 's hand - labeled , 
A:  and perhaps this is what makes the difference . 

A:  Yeah , the other are just Viterbi - aligned . 

A:  Yeah , basically the observation is what we discussed already . 
A:  The MSG problem , 

A:  the fact that the MLP trained on target task decreased the error rate . 
A:  but when the M - MLP is trained on the um  is not trained on the target task , it increased the error rate compared to using straight features . 

A:  Uh , so the fourth point is , yeah , the TIMIT plus noise seems to be the training set that gives better  the best network . 

D:  So , on the MSG uh problem 
D:  um , I think that in  in the  um , in the short  time  solution 
D:  um , that is , um , trying to figure out what we can proceed forward with to make the greatest progress , 

D:  I think it 's kind of in category that it 's , it  it may be complicated . 
D:  And uh it might be  if someone 's interested in it , uh , certainly encourage anybody to look into it in the longer term , 
D:  once we get out of this particular rush  uh for results . 
D:  But in the short term , unless you have some  some s strong idea of what 's wrong , 

D:  but  but my  my guess would be that it 's something that is a simple thing that could take a while to find . 

D:  That 's  that  what we were concerned about is that if it 's not on the target task  
D:  If it 's on the target task then it  it  it helps to have the MLP transforming it . 
D:  If it uh  if it 's not on the target task , then , depending on how different it is , uh you can get uh , a reduction in performance . 

A:  Yeah . So ,  the  the reason  Yeah , the reason is that the  perhaps the target  the  the task dependency  the language dependency , <inbreath> and the noise dependency  
A:  Well , the e e But this is still not clear 

A:  I  I  I don't think we have enough result to talk about the  the language dependency . 
A:  Well , the TIMIT network is still the best 

A:  the fact that it 's  it 's hand - labeled . 

D:  yes there 's what you would expect in terms of a language dependency and a noise dependency . That is , uh , when the neural net is trained on one of those and tested on something different , we don't do as well as in the target thing . 

A:  But you have two  two effects , the effect of changing language 
A:  and the effect of training on something that 's  Viterbi - aligned instead of hand  hand - labeled . 

D:  Do you think the alignments are bad ? 
D:  I mean , have you looked at the alignments at all ? 
D:  What the Viterbi alignment 's doing ? 
D:  Might be interesting to look at it . 

A:  Yeah . But  Yeah . But , perhaps it 's not really the  the alignment that 's bad 
A:  but the  just the ph phoneme string that 's used for the alignment 

D:  The pronunciation models and so forth 

A:  there  there might be errors just in the  in  in the ph string of phonemes . 

A:  Yeah , so this is not really the Viterbi alignment , 

A:  We can , we can tell which training set gives the best result , 
A:  but <mouth> we don't know exactly why . 

D:  Uh . Right , I mean the multi - English so far is  is the best . 

A:  Yeah . 

D:  " Multi - multi - English " just means " TIMIT " , 

A:  So perhaps , I think something like multi - band trained on a lot of noises with uh , features - based targets could  could  could help . 

A:  The future work is ,  well , try to connect to the  to make  to plug in the system to the OGI 

A:  Um , there are still open questions there , 
A:  where to put the MLP basically . 

D:  but one of the core quote  " open questions " for that is um , um , if we take the uh  you know , the best ones here , 
D:  maybe not just the best one , 
D:  but the best few or something  
D:  You want the most promising group from these other experiments . 

D:  We know that there 's a mis there 's a uh  a  a loss in performance when the neural net is trained on conditions that are different than  than , uh we 're gonna test on , 
D:  but well , if you look over a range of these different tests um , how well do these different ways of combining the straight features with the MLP features , uh stand up over that range ? 

D:  look at these different ways of combining it . 

D:  And just look  take that case and then look over all the different things . 
D:  How does that  How does that compare between the  
D:  All the different test sets , 
D:  and for  and for the couple different ways that you have of  of  of combining them . 
D:  Um .  How well do they stand up , over the  

A:  Yeah , so thi this sh would be more working on the MLP as an additional path instead of an insert to the  to their diagram . 

A:  Perhaps the insert idea is kind of strange 

D:  So . Uh , we  we wanna get their path running here , 

D:  If so , we can add this other stuff . 
D:  as an additional path 

A:  So they will send us the  Well , provide us with the feature files , 

A:  Yeah , the way we want to do it perhaps is to  just to get the VAD labels and the final features . 

D:  So we  So . First thing of course we 'd wanna do there is to make sure that when we get those labels of final features is that we get the same results as them . 
D:  Without putting in a second path . 
D:  Yeah just th w i i Just to make sure that we  have  we understand properly what things are , our very first thing to do is to  is to double check that we get the exact same results as them on HTK . 

D:  Barry , you 've been pretty quiet . 

D:  but  <laugh> That  what  what  what were you involved in in this primarily ? 

C:  Well , they 've been kind of running all the experiments and stuff 
C:  and I 've been uh , uh w doing some work on the  on the  preparing all  all the data for them to  to um , train and to test on . 

C:  Yeah . Right now , I 'm  I 'm focusing mainly on this final project I 'm working on in Jordan 's class . 

C:  And um , <mouth> I 'm just gonna see if  if that  that better models  um , uh asynchrony in any way 

D:  Here the problem seems to be is that we don't have a hug a really huge net with a really huge amount of training data . 
D:  But we have s f  for this kind of task , I would think ,  sort of a modest amount . 
D:  I mean , a million frames actually isn't that much . 
D:  We have a modest amount of  of uh training data from a couple different conditions , 
D:  and then uh  in  yeah , that  and the real situation is that there 's enormous variability that we anticipate in the test set in terms of language , 
D:  and noise type 
D:  uh , and uh ,  uh , channel characteristic , 

D:  Yeah , that  seems like a good thing to do , probably , 

D:  not uh again a short - term sort of thing . 

A:  the two of the main issues perhaps are still the language dependency <inbreath> and the noise dependency . 
A:  And perhaps to try to reduce the language dependency , we should focus on finding some other kind of training targets . 

A:  For moment you use  we use phonetic targets 
A:  but we could also use articulatory targets , soft targets , 
A:  and perhaps even , um use networks that doesn't do classification 
A:  but just regression 

A:  and well , basically com com compute features and noit not , nnn , features without noise . I mean uh , transform the fea noisy features <inbreath> in other features that are not noisy . 

