A:  Uh , right , so  so I  I was just gonna talk briefly about the NSF ITR . 

B:  I have a short thing about digits 
B:  and then uh I wanna talk a little bit about naming conventions , 

B:  So the only thing I wanna say about digits is , we are pretty much done with the first test set . 

B:  The first is what should we do about digits that were misread ? 

B:  What the transcribers did with that is if they did a correction , and they eventually did read the right string ,  you extract the right string . 

G:  Oh , you 're talking about where they completely read the wrong string and didn't correct it ? 

G:  Seems like we should just change the transcripts 

G:  So you  um , how many digits have been transcribed now ? 

B:  Four thousand lines . 
B:  And each line is between one and about ten digits . 

A:  So that 's a couple hours of  of , uh , speech , probably . 

B:  And then the other thing is that , uh , the forms in front of us here that we 're gonna read later , were suggested by Liz 
B:  because she wanted to elicit some different prosodics from digits . 

A:  And the decision here , uh , was to continue with uh the words rather than the  the numerics . 

B:  The problem was O and zero . 

A:  Or neither . 
A:  But it 's just two thing  ways that you can say it . 

A:  She 's trying to get at natural groupings , 
A:  but it  there 's  there 's nothing natural about reading numbers this way . 

B:  But , um , the other problem we were thinking about is if you just put the numerals ,  they might say forty - three instead of four three . 

A:  OK . Well , we 're probably gonna be collecting meetings for a while 

A:  OK , so uh e l I guess , let me , uh , get my  my short thing out about the NSF . 

A:  before on  on more  more higher level , uh , issues in meetings , 

A:  so this was , uh , a , uh , proposal that we put in 

A:  so is i for  it was a <mouth> proposal for the ITR program , 

A:  It 's the  second year of their doing , uh , these grants . 

A:  So , they 're very competitive , 
A:  and they have a first phase where you put in pre - proposals , 
A:  and we  we , uh , got through that . 
A:  And so th the  the next phase will be  we 'll actually be doing a larger proposal . 

A:  They have three size grants . 

A:  And  and we 're in the middle  
A:  middle category . 

A:  It 's extending the research , 

B:  Yeah it 's go higher level stuff than we 've been talking about for Meeting Recorder . 

A:  this is dealing with , uh , mapping on the level of  of , um , the conversation  
A:  of mapping the conversations 
A:  to different kind of planes . 

A:  Yeah , there  there would be  there would be new hires , 
A:  and  and there  there would be expansion , 

A:  and , <breath> it 'll mean some more work , uh , you know , in  in March in getting the proposal out , 

A:  Uh , the last one was  that you had there ,  was about naming ? 

B:  So , uh , one thing she would like to have is for all the names to be the same length 

B:  same number of characters 

B:  And I don't think we have so many meetings that that 's a big deal just to change the names . 

B:  So for  for m the meetings we were thinking about three letters and three numbers 

B:  Uh , for speakers , M or F and then three numbers , 

B:  that also brings up the point that we have to start assembling a speaker database so that we get those links back and forth 
B:  and keep it consistent . 

B:  and then , uh , the microphone issues . 
B:  We want some way of specifying , more than looking in the " key " file , what channel and what mike . What channel , what mike , and what broadcaster . 

B:  And so we just need some naming conventions on that . 

B:  So I was just gonna do a fixed list of , uh , microphones and types . 

A:  Um ,  <clears throat> since we have such a short agenda list I guess I wi I will ask how  how are the transcriptions going ? 

F:  I  I switched to doing the channel - by - channel transcriptions to provide , uh , the  uh , tighter time bins for  partly for use in Thilo 's work 
F:  and also it 's of relevance to other people in the project . 

F:  one of them is that , um , it seems that there are time lags involved in doing this , 

F:  using an interface that has so much more complexity to it . 

F:  Maybe  I was thinking maybe the best way to do this in the long run may be to give them single channel parts 
F:  and then piece them together later . 

F:  And it may be that it 's faster to transcribe a channel at a time with only one , uh , sound file and one , uh , set of  of , uh , utterances to check through . 

F:  But , um , with the mixed , when you have an overlap , you only have a  a choice of one start and end time for that entire overlap , 

F:  And for purposes of  of , uh , things like  
F:  well , so things like training the speech - nonspeech segmentation thing . 
F:  Th - it 's necessary to have it more tightly tuned than that . 

F:  having the interface that doesn't require them to do the ti uh , the time bins for every single channel at a t uh , through the entire interaction . 

F:  I really do think that it 's wise that we 've had them start the way we have 
F:  with , uh , m y working off the mixed signal , 

F:  once in a while a backchannel will be overlooked by the transcriber . 

F:  And if we 're gonna study types of overlaps , 

F:  then that really does require listening  to every single channel all the way through the entire  length for all the different speakers . 

F:  And I think again it 's like this  it 's really valuable that Thilo 's working on the speech - nonspeech segmentation 
F:  because maybe , um , we can close in on that wi without having to actually go to the time that it would take to listen to every single channel from start to finish through every single meeting . 

E:  Yeah , but those backchannels will always be a problem I think . 
E:  Uh especially if they 're really short 
E:  and they 're not very loud 
E:  and so it  it can  it  it will always happen that also the automatic s detection system will miss some of them , 

F:  Well so then  then , maybe the answer is to , uh , listen especially densely in places of overlap , 

C:  I  I 'm continue working with the mixed signal now ,  after the  the last experience . 
C:  And  and I 'm tried to  to , uh , adjust the  to  to improve , eh , an harmonicity , eh , detector that , eh , I  I implement . 

C:  eh , and now I 'm  I 'm  I 'm trying to  to find , eh , some kind of a , um  <breath> of h of help , eh , using the energy to  to distinguish between possible harmonics , and  and other fre frequency peaks , that , eh , corres not harmonics . 

C:  eh , I have to  to talk with y with you , with the group , eh , about the instantaneous frequency , 

C:  No . I  I  I  I don't proth process the  the fundamental . 
C:  I  <breath> I , ehm  I calculate the  the phase derivate using the FFT . 

A:  I mean you could make some guesses from , uh  from the auto - correlation or something 
A:  but  but then , given those guesses , try , 

A:  only looking at the energy at multiples of the  of that frequency , 

A:  take the one that 's maximum . 

A:  You 're trying distinguish between the case where there is , uh  where  where there are more than  uh , where there 's more than one speaker 
A:  and the case where there 's only one speaker . 

A:  other than that I guess as far as the one person versus two persons , it would be  primarily a low frequency phenomenon . 
A:  And if you looked at the low frequencies , yes the higher frequencies are gonna  there 's gonna be a spectral slope . 
A:  The higher frequencies will be lower energy . 

C:  I will prepare for the next week eh , all my results about the harmonicity 

A:  I guess  so  so y I think maybe you do need a voiced - unvoiced determination too . 
A:  But if it 's voiced , 

A:  e the fraction of the energy that 's in the harmonic sequence that you 're looking at is relatively low , then it should be  then it 's more likely to be an overlap . 

F:  I I did i it did occur to me that this is  uh , the return to the transcription , 
F:  that there 's one third thing I wanted to  to ex raise as a to as an issue 
F:  which is , um , how to handle breaths . 

F:  um , aside from the fact that they 're obviously very time - consuming to encode , 

F:  but the question of whether it 'd be possible to eliminate them from the audio signal , 

A:  I don't know  think it 'd be ideal . 
A:  We - See , we 're  we 're dealing with real speech 
A:  and we 're trying to have it be as real as possible 
A:  and breaths are part of real speech . 

A:  If  if it gets in the way of what somebody is doing with it then you might wanna have some method which will allow you to block it , 
A:  but you  it 's real data . 

F:  because i it basically has a i it shows very clearly the contrast between , uh , speech recognition research and discourse research 

F:  OK , so now , I had a discussion with Chuck about the data structure 

F:  there 'll be a master transcript 
F:  which has in it everything that 's needed for both of these uses . 
F:  And the one that 's used for speech recognition will be processed via scripts . 

F:  So what I would  r r what I would wonder is would it be possible to encode those automatically ? 
F:  Could we get a breath detector ? 

A:  Yeah I mean I 'm think if it 's too  if it 's too hard for us to annotate the breaths per se , <breath> we are gonna be building up models for these things 
A:  and these things are somewhat self - aligning , 
A:  so if  so , <breath> we  i i if we say there is some kind of a thing which we call a " breath " or a " breath - in " or " breath - out " , <breath> the models will learn that sort of thing . 
A:  Uh , so  but you  but you do want them to point them at some region where  where the breaths really are . 

A:  what if you put it in but didn't put the boundaries ? 

G:  So I would say don't tell them to transcribe anything that 's outside of a grouping of words . 

E:  Yeah , and that 's  that  that quite co corresponds to the way I  I try to train the speech - nonspeech detector , 

A:  In fact , the  I think I would say the core thing that we 're trying to do is to recognize the actual , meaningful components in the midst of other things that are not meaningful . 

A:  it 's critical for us to get these other components that are not meaningful . 
A:  Because that 's what we 're trying to pull the other out of . 

F:  which is that as we 've improved our microphone technique , we have a lot less breath in the  in the more recent , uh , recordings , 
F:  so it 's  in a way it 's an artifact that there 's so much on the  on the earlier ones . 

