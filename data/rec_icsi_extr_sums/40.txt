B:  Do you have news from the conference talk ? 

C:  No , nobody 's told me anything . 

C:  No , that would have been a good thing to find out before this meeting , 

C:  I mean , let 's  let 's assume for right now that we 're just kind of plugging on ahead , 
C:  because even if they tell us that , uh , the rules are different , uh , we 're still interested in doing what we 're doing . 
C:  So what are you doing ? 

B:  Uh , well , we 've  a little bit worked on trying to see , uh , what were the bugs and the problem with the latencies . 

B:  We took  first we took the LDA filters 
B:  and , <swallow> uh , we designed new filters , 
B:  using uh recursive filters actually . 

B:  So we took the filters  the FIR filters <swallow> and we  designed , uh , IIR filters that have the same frequency response . 
B:  Well , similar , but that have shorter delays . 

C:  You  you had a discussion with Sunil about this though ? 

B:  No . 

C:  Yeah , you should talk with him . 

C:  No , I mean , because the  the  the  the whole problem that happened before was coordination , 

C:  So  so you need to discuss with him what we 're doing , 
C:  uh , cuz they could be doing the same thing and  or something . 

C:  We just  we just have to be in contact more . 
C:  I think that  the  the fact that we  we did that with  had that thing with the latencies was indicative of the fact that there wasn't enough communication . 

B:  And the low frequency band has sixty - four milliseconds of delay , 
B:  and the high frequency band filter has something like eleven milliseconds compared to the two hundred milliseconds of the IIR filters . 

C:  So that would be , uh , a reduction of a hundred and thirty - six milliseconds , 

C:  What was the total we ended up with through the whole system ? 

B:  Three hundred and thirty . 

C:  So that would be within  ? 

B:  Yeah , 

B:  so , sixty - five , plus ten , plus  for the downsampling , 
B:  plus eighty - five for the on - line normalization . 

B:  plus  plus eighty for the neural net and PCA . 
B:  So it would be around two hundred and forty  

C:  Just  just barely in there . 

A:  What 's the allowable ? 

C:  Two - fifty , 
C:  unless they changed the rules . 
C:  Which there is  there 's some discussion of . 

A:  What were they thinking of changing it to ? 

C:  Uh , well the people who had very low latency want it to be low  uh , very  <laugh> very very narrow , uh , latency bound . 
C:  And the people who have longer latency don't . 

C:  Unfortunately we 're the main ones with long latency , 

B:  and basically the best proposal had something like thirty or forty milliseconds of latency . 

C:  To get down to forty or fifty milliseconds we 'd have to throw out most of what we 're doing . 

A:  A person  I don't think a person can tell the difference between , uh , you know , a quarter of a second and a hundred milliseconds , 

A:  I 'm not even sure if we can tell the difference between a quarter of a second and half a second . 
A:  I mean it just  it feels so quick . 

C:  we could cut  we know what else , we could cut down on the neural net time 
C:  by  by , uh , playing around a little bit , 
C:  going more into the past , 

A:  So is the latency from the neural net caused by how far ahead you 're looking ? 

C:  Mm - hmm . 

D:  Also we were thinking to  to , uh , apply the eh , spectral subtraction from Ericsson 
D:  and to  to change the contextual KLT for LDA . 

D:  Uh , to change and use LDA discriminative . 

C:  uh , one thing that would be no  good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , um , run a good VAD , and  and determine boundaries . 
C:  And then given those boundaries , then have everybody do the recognition . 

C:  uh , I guess they argued about that yesterday 

C:  I don't  don't know the answer 
C:  but we should find out . 

C:  Well , there 's a lot of different ways of computing the noise spectrum . 

A:  it seems like this kind of thing could add to the latency . 
A:  I mean , depending on where the window was that you used to calculate  the signal - to - noise ratio . 

C:  Not necessarily . 
C:  Cuz if you don't look into the future , 

B:  Actually , it 's a mmm  If - if you want to have a good estimation on non - stationary noise you have to look in the  in the future . 

C:  but what does  what  what  what does Alcatel do ? 
C:  And  and France Telecom . 

B:  They just look in the past . 
B:  I guess it works because the noise are , uh pret uh , almost stationary 

C:  Yeah , y I mean , you 're talking about non - stationary noise 
C:  but I think that spectral subtraction is rarely  is  is not gonna work really well for  for non - stationary noise , 

C:  But it 's hard to  

C:  Uh , maybe we can talk about a couple other things briefly , 

C:  So you 're coming up with your quals proposal , 

E:  Um , but I 'm , uh , looking into extending the work done by Larry Saul and John Allen and uh Mazin Rahim . 
E:  Um , they  they have a system that 's , uh , a multi - band , um , system 
E:  but their multi - band is  is a little different than the way that we 've been doing multi - band in the past , 
E:  where um  Where we 've been @ @  uh taking  um  <mouth> sub - band features and i training up these neural nets and  on  on phonetic targets , 
E:  and then combining them some somehow down the line , 

E:  they 're  they 're taking sub - band features and , um , training up a detector that detects for , um , these phonetic features 

E:  and h he shows that using this  this method to detect sonorance is it 's very robust compared to , um  <mouth> to typical , uh , full - band Gaussian mixtures um estimations of  of sonorance . 

E:  so that 's just  that 's just one detector . 
E:  So you can imagine building many of these detectors on different features . 
E:  You get enough of these detectors together , um , then you have enough information to do , um , higher level discrimination , 

E:  So , um , that 's  that 's the direction which I 'm  I 'm thinking about going in my quals . 

E:  for example , discriminating between phones 
E:  and then you keep working your way up until you  you build a full recognizer . 

C:  so , uh , y you want to talk maybe a c two or three minutes about what we 've been talking about today and other days ? 

F:  we 're interested in , um , methods for far mike speech recognition , 
F:  um ,  mainly , uh , methods that deal with the reverberation  in the far mike signal . 

F:  one approach would be , um , say MSG and PLP , like was used in Aurora one 
F:  and , um , there are other approaches which actually attempt to  remove the reverberation , instead of being robust to it like MSG . 
F:  And so we 're interested in , um , comparing the performance of  um , a robust approach like MSG with these , um , speech enhancement or de - reverber de - reverberation approaches . 

C:  There 's also this , uh , um , uh , echo cancellation stuff that we 've sort of been chasing , 

C:  and when we 're saying these digits now we do have a close microphone signal 
C:  and then there 's the distant microphone signal . 
C:  And you could as a kind of baseline say , " OK , given that we have both of these , uh , we should be able to do , uh , a cancellation . " 
C:  So that , uh , um , we  we , uh , essentially identify the system in between  the linear time invariant system between the microphones and  and  and  and re and invert it , 
C:  uh , or  or cancel it out to  to some  some reasonable approximation 

D:  Yesterday morning on video conference . 

B:  But , well , when we add up everything it 's  it will be alright . 

B:  that 's hard to do . 

