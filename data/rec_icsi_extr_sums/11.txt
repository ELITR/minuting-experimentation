D:  and the other bit of news is we had  you know , uh , I was visited by my German project manager 

D:  and B , is planning to come here either three weeks in July or three weeks in August , to actually work . 

D:  and he came up  we came up  with a pretty strange idea . 

D:  Imagine if you will , <outbreath> that we have a system that does all that understanding that we want it to do based on utterances . 
D:  It should be possible to make that system produce questions . 
D:  So if you have the knowledge of how to interpret " where is X ? " under given conditions , situational , user , discourse and ontological <inbreath> conditions , you should also be able to make that same system ask " where is X ? " 

D:  If you get the system to speak to itself , you may find n break downs and errors and you may be able to learn . 

E:  e I 'm sort of  have the impression that getting it to say the right thing in the right circumstances is much more difficult than getting it to understand something given the circumstances and so on , 

D:  But maybe one could do some learning . 

E:  just the fact that we 'll get  The point is that getting it to understand one construction doesn't mean that it will n always know exactly when it 's correct to use that construction . Right ? 

D:  it 's not the same as the understanding . 

B:  The basic idea I guess would be to give  allow the system to have intentions , basically ? 

D:  If we don't have  Let 's assume we don't have any input from the language . Right ? 
D:  So there 's also nothing we could query the ontology , 
D:  but we have a certain user setting . 
D:  If you just ask , what is the likelihood of that person wanting to enter some  something , it 'll give you an answer . 

D:  Well you can observe some user and context stuff and ask , what 's the posterior probabilities of all of our decision nodes . 

B:  Well , it will d r assign values to all the nodes . Yes . 

D:  Yeah . So what we actually then need to do is  is write a little script that changes all the settings , 
D:  you know , go goes through all the permutations , 

D:  didn't we calculate that once ? 

B:  Let 's see , so it would be two to the thirty for every output node ? 
B:  Which is very th very large . 

E:  Yeah , it 's g Anyway , the point is that given all of these different factors , it 's uh e it 's  it 's still going to be impossible to run through all of the possible situations or whatever . 

B:  But we can do randomized testing . 
B:  Which probabilistically will be good enough . 

D:  Yeah . So , it be it it 's an idea that one could n for  for example run  run past , um , 

D:  Well we fixed some more things from the SmartKom system , 

D:  How is the generation XML thing ? 

B:  I 'm gonna finish it today , uh hopefully . 

D:  It 's  it 's sort of t cognitive , neural , psycho , linguistic , 
D:  but all for the sake of doing computer science . 

B:  Wait , is this a computer science conference 

D:  So it 's sort of cognitive , psycho , neural , plausibly motivated , architectures of natural language processing . 

B:  I mean we just  I mean it wouldn't hurt to write up a paper , 

D:  Might be more interesting to do something like let 's assume um , we 're right , 

D:  and take a " where is X " sentence , 

D:  and how we cognitively , neurally , psycho - linguistically , construction grammar - ally , motivated , envision uh , understanding that " . 

D:  That should be able to  we should be able to come up with , you know , a sort of a  a parse . 

D:  Well , I  I also think that if we sort of write about what we have done in the past six months , we  we  we could sort of craft a nice little paper that  if it gets rejected , which could happen , doesn't hurt 

D:  I mean , it 's obvious that we can't do any kind of evaluation , 
D:  and have no  you know , we can't write an ACL type paper where we say , " OK , we 've done this 
D:  and now we 're whatever percentage better than everybody else " . You know . 

D:  maybe even  That 's maybe the time to introduce the  the new formalism that you guys have cooked up . 

D:  Look at the web page and let 's talk about it maybe tomorrow afternoon ? 

D:  don't make any plans for spring break next year . 

D:  We 're gonna do an int EDU internal workshop in Sicily . 
D:  I 've already got the funding . 

D:  So we could  we could say this is what  what 's sort of state of the art today . Nuh ? 
D:  And say , this is bad . Nuh ? 
D:  And then we can say , uh well what we do is this . 

A:  And how much to get into the cognitive neural part ? 

B:  That 's the only  That 's the question mark . 

D:  Yeah , and you can  you can just point to the  to the literature , 

A:  Like , NLP cognitive neural . 

A:  The fact that the methods here are all compatible with or designed to be compatible with whatever , neurological  neuro neuro - biol su stuff . 

A:  Like introducing the formalism might be not really possible in detail , 
A:  but you can use an example of it . 

D:  So this will be sort of documenting what we think , and documenting what we have in terms of the Bayes - net stuff . 

D:  The SUDO - square <writing on whiteboard> is , <three-syllable laugh> " Situation " , " User " , " Discourse " , right ? " Ontology " . 

D:  You know , these are our , whatever , belief - net decision nodes , 
D:  and they all contribute to these  <tapping on white board> things down here . 

A:  Wait , wait , what 's the middle thing ? 

D:  Well , in the moment it 's a Bayes - net . 
D:  And it has sort of fifty not - yet - specified interfaces . 
D:  OK . Eh  I have taken care that we actually can build little interfaces , <squeaking from writing on the white board> to other modules that will tell us whether the user likes these things and , n the  or these things , 

D:  Think of  back at the EVA vector , 
D:  and Johno coming up with the idea that if the person discussed the  discussed the admission fee , in  eh previously , that might be a good indication that , " how do I get to the castle ? " , actually he wants to enter . 

D:  So what would be kind of cool is that if we encounter concepts that are castle , tower , bank , hotel , we run it through the ontology , 
D:  and the ontology tells us it has um , admission , opening times , 

D:  and then search dynamically through the uh , discourse history for  occurrences of these things in a given window of utterances . 

D:  And furthermore , I mean we can idealize that , you know , people don't change topics , 

D:  But , even th for that , there is a student of ours who 's doing a dialogue act um , recognition module . 

D:  but we  what we actually decided last week , is to , and this is , again , for your benefit  is to um , pretend we have observed and parsed an utterance such as " where is the Powder - Tower " , 

D:  So that  And I will  I will then  <cough> come up with the ontology side uh , bits and pieces , 

D:  and then we can sort of fiddle with these things to see what it actually produces , in terms of output . 

B:  I don't see unde how we would be able to distinguish between the two intentions just from the g utterance , though . 
B:  I mean , uh bef or , before we don't  before we cranked it through the Bayes - net . 

D:  So we want to sort of come up with what gets uh , input , and how inter in case of a " where is " question . 
D:  So what  what would the outcome of  of your parser look like ? 
D:  And , what other discourse information from the discourse history could we hope to get , squeeze out of that utterance ? 
D:  So define the  the input into the Bayes - net <mouth> based on what the utterance , " where is X " , gives us . 

D:  So w if we have a construction  node , " where is X " , it 's gonna both get the po posterior probability that  it 's Info - on up , 
D:  Info - on is True - up , and that Go - there is True - up , as well . 
D:  Which would be exactly analogous to what I 'm proposing is , this makes  uh makes something here true , 

D:  and this makes this True - up , and this makes this True - up as well . 

E:  I kinda like it better without that extra level of indirection too . 

A:  Wait , so do , or do not take other kinds of constructions into account ? 

D:  Well , if you  if you can , oh definitely do , 

D:  If i if  if it 's not at all triggered by our thing , then it 's irrelevant , 

D:  Think  Uh , well this is just a mental exercise . 

D:  focus on this question , how would you design  that ? 

A:  So it 's like you infer the speaker intent , 
A:  and then infer a plan , a larger plan from that , for which you have the additional information , 

D:  so that we actually end up with um , um , nodes for the discourse and ontology 
D:  so that we can put them into our Bayes - net , 

D:  and we can run our better JavaBayes , and have it produce some output . 

D:  So th this might be a nice opening paragraph for the paper as saying , " you know people look at kinds of  <clear throat> at ambiguities " , 

D:  A , uh these things are never really ambiguous in discourse , B , 

D:  but normal statements that seem completely unambiguous , such as " where is the blah - blah " , actually are terribly complex , and completely ambiguous . 

D:  Also we 're getting a  a person who just got fired uh , from her job . 
D:  Uh a person from Oakland who is interested in maybe continuing the wizard bit once Fey leaves in August . 

D:  Remember this , we can completely change the set - up any time we want . 

D:  No , we 're approaching twenty now . 

D:  Look at the results we 've gotten so far for the first , whatever , fifty some subjects ? 

D:  But , until Fey is leaving , we surely will hit the  some of the higher numbers . 

D:  We have uh , eh found someone here who 's hand st hand transcribing the first twelve . 

D:  And so , @ @ whatever that is , it 's the generic default intention . That it would find out . 

E:  What would one possibly put in such a paper ? 

D:  and specify um , what  what we think the  the output uh , observe , out  i input nodes for our Bayes - nets for the sub sub - D , for the discourse bit , should be . 

