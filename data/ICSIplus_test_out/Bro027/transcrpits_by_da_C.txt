C:  or  
C:  Eight , eight ? 
C:  Three . 
C:  Yea - actually , the left edge of the first filter is at sixty - four . 
C:  So  
C:  Mmm . 
C:  Yeah . 
C:  It  
C:  This is the filter bank in the frequency domain that starts at sixty - four . 
C:  Yeah . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Um , I 've been playing with , first , the , um , VAD . 
C:  Um , <clears throat> so it 's exactly the same approach , 
C:  but the features that the VAD neural network use are , uh , MFCC after noise compensation . 
C:  Oh , I think I have the results . 
C:  Before it was just P L 
C:  So . 
C:  Yeah , 
C:  noisy  noisy features . 
C:  Um  
C:  This is what we get after  This  
C:  So , actually , we , yeah , here the features are noise compensated 
C:  and there is also the LDA filter . 
C:  Um , and then it 's a pretty small neural network which use , um , <clears throat> nine frames of  of six features from C - zero to C - fives , plus the first derivatives . 
C:  And it has one hundred hidden units . 
C:  Yeah . 
C:  Mm - hmm . 
C:  So it 's twelve times nine . 
C:  Hidden 
C:  and 
C:  two outputs . 
C:  Mm - hmm . 
C:  It should be OK . 
C:  So the previous syst 
C:  It 's based on the system that has a fifty - three point sixty - six percent improvement . 
C:  It 's the same system . 
C:  The only thing that changed is the n a p eh  a es the estimation of the silence probabilities . 
C:  Which now is based on , uh , cleaned features . 
C:  Yeah . 
C:  Um  
C:  So it 's  it 's not bad , 
C:  but the problem is still that the latency is too large . 
C:  Because  
C:  um  
C:  the  the latency of the VAD is two hundred and twenty milliseconds . 
C:  And , uh , the VAD is used uh , i for on - line normalization , 
C:  and it 's used before the delta computation . 
C:  So if you add these components it goes t to a hundred and seventy , 
C:  right ? 
C:  With two an two hundred and seventy . 
C:  If  
C:  Yeah , 
C:  if you add the c delta comp delta computation 
C:  which is done afterwards . 
C:  Um  
C:  The two - twenty is one hundred milliseconds for the um  
C:  No , it 's forty milliseconds for t for the , uh , uh , cleaning of the speech . 
C:  Um  
C:  then there is , um , the neural network which use nine frames . 
C:  So it adds forty milliseconds . 
C:  Um , 
C:  after that , um , you have the um , filtering of the silence probabilities . 
C:  Which is a million filter 
C:  it , 
C:  and it creates a one hundred milliseconds delay . 
C:  So , um  
C:  Yeah , 
C:  and there is the delta at the input 
C:  which is , 
C:  um  
C:  So it 's  @ @  

C:  Mmm . 
C:  Forty  
C:  This forty plus twenty , plus one hundred . 
C:  Uh  
C:  Yeah , there are twenty that comes from  
C:  There is ten that comes from the LDA filters also . 
C:  Right ? 
C:  Uh , so it 's two hundred and ten , 
C:  yeah . 
C:  Plus the frame , 
C:  so it 's two - twenty . 
C:  Yeah , I think it 's  it 's five frames , 
C:  but . 
C:  So . 
C:  Forty cleaning . 
C:  Yeah . 
C:  Twenty for the delta . 
C:  Yeah . 
C:  Yeah . 
C:  For the frame I guess . 
C:  I computed two - twenty  
C:  Yeah , well , it 's  
C:  I guess it 's for the fr  the  
C:  So this is the features that are used by our network 
C:  and then afterwards , you have to compute the delta on the , uh , main feature stream , 
C:  which is um , delta and double - deltas , 
C:  which is fifty milliseconds . 
C:  The VAD use , uh , LDA filtered features also . 
C:  Mm - hmm . 
C:  No . 
C:  There is , um , just downsampling , upsampling , and the LDA . 
C:  It 's fifty . 
C:  But well , we could probably put the delta , um , <mouth> before on - line normalization . 
C:  It should not that make a big difference , 
C:  because  
C:  Yeah . 
C:  Yeah , 
C:  but , nnn  
C:  Mm - hmm . 
C:  Cuz i 
C:  Yeah , 
C:  cuz the time constant of the on - line normalization is pretty long compared to the delta window , 
C:  so . 
C:  It should not make  
C:  Mm - hmm . 
C:  Yeah , 
C:  yeah . 

C:  Hmm . 
C:  Well , but I think the main thing , maybe , is the cleaning of the speech , which takes forty milliseconds or so . 
C:  And  
C:  and  but  the LDA is , well , pretty short right now . 
C:  Yeah . 
C:  No . 
C:  I just  
C:  This is the first try . 
C:  I mean , I  maybe the LDA 's not very useful then . 
C:  Mmm . 
C:  Well , in the proposal , um , the input of the VAD network were just three frames , I think . 
C:  Uh , static features . 

C:  Mm - hmm . 
C:  Yeah . 
C:  Mm - hmm . 
C:  Yeah . 
C:  The best was fifty - four point five . 
C:  And our system was forty - nine , 
C:  but with the neural network . 
C:  It would 
C:  Mm - hmm . 
C:  Yeah , even for a well - matched case it 's sixty percent error rate reduction , 
C:  which is  
C:  Um , 
C:  Yeah . 
C:  So actually , this is in between <2 sniffs> what we had with the previous VAD and what Sunil did with an IDL VAD . 
C:  Which gave sixty - two percent improvement , right ? 
C:  So  
C:  Yeah . 
C:  So , if you use , like , an IDL VAD , uh , for dropping the frames , 
C:  the best that we can get  i That means that we estimate the silence probability on the clean version of the utterances . 
C:  Then you can go up to sixty - two percent error rate reduction , globally . 
C:  Mmm  
C:  Yeah . 
C:  Yeah . 
C:  If you add a g good v very good VAD , that works as well as a VAD working on clean speech , 
C:  then you wou you would go  
C:  Mm - hmm . 
C:  Yeah . 
C:  Mm - hmm . 
C:  Uh , yeah , the next thing is , I started to play  
C:  Well , I don't want to worry too much about the delay , 
C:  no . 
C:  Maybe it 's better to wait 
C:  for the decision 
C:  from the committee . 
C:  Uh , but I started to play with the , um , <vocal squeak> <mouth> uh , tandem neural network . 
C:  Mmm 
C:  I just did the configuration that 's very similar to what we did for the February proposal . 
C:  And  
C:  Um . 
C:  So . There is a f a first feature stream that use uh straight MFCC features . 
C:  Well , these features actually . 
C:  And the other stream is the output of a neural network , using as input , also , these , um , cleaned MFCC . 
C:  Um  
C:  I don't have the comp 
C:  Mmm ? 
C:  So there is just this feature stream ,  the fifteen MFCC plus delta and double - delta . 
C:  Um , so it 's  makes forty - five features  that are used as input to the HTK . 
C:  And then , there is  there are more inputs that comes from the tandem MLP . 
C:  Yeah . 
C:  Um  
C:  So , um , 
C:  uh , yeah . 
C:  Right now it seems that  i I just tested on SpeechDat - Car while the experiment are running on your  on TI - digits . 
C:  Well , it improves on the well - matched and the mismatched conditions , 
C:  but it get worse on the highly mismatched . 
C:  Um , 
C:  Compared to these numbers , yeah . 
C:  Um , 
C:  like , on the well - match and medium mismatch , the gain is around five percent relative , 
C:  but it goes down a lot more , like fifteen percent on the HM case . 

C:  The  
C:  i 
C:  I have , 
C:  um  
C:  From the networks , it 's twenty - eight . 
C:  So  
C:  So , d i It 's forty - five . 
C:  Yeah . 
C:  Yeah . 
C:  Mm - hmm . 
C:  There 's a KLT after the neural network , as  as before . 
C:  Yeah . 
C:  I don't know . 
C:  Uh . 
C:  It 's  i i i It 's because it 's what we did for the first proposal . 
C:  We tested , uh , trying to go down 
C:  and 
C:  Yeah . 
C:  So  
C:  Um . 
C:  I wanted to do something very similar to the proposal as a first  first try . 
C:  But we have to  for sure , we have to go down , 
C:  because the limit is now sixty features . 
C:  So , 
C:  uh , 
C:  we have to find a way to decrease the number of features . 
C:  Um  
C:  Mmm . 
C:  But it 's worse . 
C:  Yeah , I  
C:  Uh - huh . 
C:  Mm - hmm . 
C:  Um , 
C:  I don't know . 
C:  I 'm surprised , 
C:  because I expected the neural net to help more when there is more mismatch , as it was the case for the  
C:  Yeah , it 's the same training set , 
C:  so it 's TIMIT with the TI - digits ' , uh , noises , uh , added . 
C:  Um  
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Yeah . 
C:  Mm - hmm . 
C:  Yeah . 
C:  I g I guess . 
C:  Um . 
C:  The reason I did it this ways is that in February , it  we  we tested different things like that , 
C:  so , having two KLT , having just a KLT for a network , or having a global KLT . 
C:  And  
C:  Well  
C:  Yeah . 
C:  And , uh , th 
C:  Yeah . 
C:  The differences between these configurations were not huge , 
C:  but  it was marginally better with this configuration . 
C:  Um . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Yeah . 
C:  Of the straight features , yeah . 
C:  So . 
C:  But n th the , um , 
C:  tandem features are u used as they are . 
C:  So , 
C:  yeah , maybe we can add some context from these features also as  Dan did in  in his last work . 
C:  Mm - hmm . 
C:  Um , not  no relation . 
C:  The f the forty - eight hundred bits is for transmission of some features . 
C:  And generally , i it  s allows you to transmit like , fifteen , uh , cepstrum . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Yeah , I see . 
C:  It 's , uh , ranging from zero to clean ? 
C:  Yeah . 
C:  From zero to clean . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  It 's TIMIT with noise . 
C:  So , yeah , it 's rather a small  
C:  Um , 
C:  Uh , it 's , uh , five hundred hidden units . 
C:  And  
C:  Yeah . 
C:  Yeah . 
C:  Yeah . @ @ ? 

C:  Yeah . 
C:  Um  
C:  They  k uh  
C:  Mmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Yeah . 
C:  Mm - hmm . 
C:  You mean the  the most noisy occurrences on SpeechDat - Car might be a lot more noisy than  
C:  Maybe . 
C:  Yeah , 
C:  yeah . 
C:  Mm - hmm . 
C:  Yeah . 
C:  Mm - hmm . 
C:  Well , if the initial range of SNR is different , we  the problem was already there before . 
C:  And  
C:  Because  
C:  Mmm  
C:  Hmm . 
C:  Yeah , uh  
C:  Yeah . 
C:  So  
C:  Clean training , yeah . 
C:  Yeah , we 'll  so we 'll see . 
C:  Uh . 
C:  Maybe . 
C:  Mm - hmm . 
C:  Yeah . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Yep . 
C:  Yeah , actually <clears throat> to s eh , what I observed in the HM case is that the number of deletion dramatically increases . 
C:  It  it doubles . 
C:  When I added the num the neural network it doubles the number of deletions . 
C:  Yeah , so I don't you know <laugh> how to interpret that , 
C:  but , mmm  
C:  t 
C:  They p stayed the same , 
C:  they  maybe they are a little bit uh , lower . 
C:  They are a little bit better . 
C:  Yeah . But  
C:  Mm - hmm . 
C:  No , it doesn't . 
C:  No . 
C:  Clean training and  
C:  It 's clean training  
C:  Well , close microphone training and distant microphone , um , high speed , I think . 
C:  Well  
C:  The most noisy cases are the distant microphone for testing . 
C:  Separating . 
C:  Yeah . 
C:  But  
C:  Yeah . 
C:  I mean , but without the neural network it 's  well , it 's better . 
C:  It 's just when we add the neural networks . 
C:  The feature are the same except that  
C:  Yeah . 
C:  Mm - hmm . 
C:  Uh , there is a car noise . 
C:  So there are f just four noises . 
C:  Um , 
C:  uh , " Car " , I think , 
C:  " Babble " , 
C:  " Subway " , right ? 
C:  and  
C:  and  " Street " isn't  
C:  " Train station " , yeah . 
C:  So  it 's mostly  Well , " Car " is stationary , 
C:  " Babble " , it 's a stationary background plus some voices , 
C:  some speech over it . 
C:  And the other two are rather stationary also . 
C:  Mm - hmm . 
C:  It was  b a little bit worse . 
C:  Than just the features , 
C:  yeah . 
C:  Mm - hmm . 
C:  It was helping , uh , if the features are b were bad , 
C:  I mean . 
C:  Just plain P L Ps or M F 
C:  C Cs . 
C:  as soon as we added LDA on - line normalization , and <clears throat> all these things , then  
C:  Yeah , 
C:  mm - hmm . 
C:  Mm - hmm . 
C:  Yeah , 
C:  mm - hmm . 
C:  Y yeah , oh . 
C:  Yeah . 
C:  I d I  I think it 's  it was one or two percent . 
C:  That 's not that bad , 
C:  but it was l like two percent relative worse on SpeechDat - Car . 
C:  I have to  to check that . 
C:  Well , I have  I will . 
C:  Mm - hmm . 
C:  Hmm . 
C:  Yeah , it was  it was slightly worse . 
C:  Um , 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  No , actually , I think  i 
C:  Well . 
C:  What do we do with the ANN is  is something like that 
C:  except that it 's not linear . 
C:  But it 's  it 's like a nonlinear discriminant analysis . 
C:  But . 
C:  Yeah . 
C:  It 's  
C:  Yeah . 
C:  Uh . 
C:  Mm - hmm . 
C:  Yeah , I know . 
C:  That  that  Yeah . 
C:  Well , in the proposal , they were transformed u using PCA , 
C:  but  
C:  Yeah , it might be that LDA could be better . 
C:  It 's the transformation they 're estimating on  
C:  Well , they are trained on the same data as the final HMM are . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Yeah . 
C:  Mm - hmm . 
C:  Yeah , 
C:  eventually we could  could just 
C:  Then you don't have to worry about the thresholds and  
C:  but just  
C:  Mm - hmm . 
C:  Yeah . 
C:  Mm - hmm . 
C:  Yeah , yeah . 
C:  Right . 
C:  Hmm - hmm ! 
C:  Um  
C:  Uh , still not . 
C:  Yeah . 
C:  Th 
C:  Mm - hmm . 
C:  Yeah . 
C:  Yeah , so  
C:  maybe , yeah , when  
C:  That 's what  
C:  Yeah . 
C:  But  
C:  So I might maybe look at , 
C:  is it due to the fact that um , the probability of the silence at the output of the network , is , 
C:  uh , 
C:  too  too high 
C:  or  
C:  If it 's the case , then multiplying it again by  i by something ? 
C:  Mm - hmm . 
C:  Oh - eee - hhh . 
C:  Uh , yeah . 
C:  Mm - hmm . 
C:  Yeah . 
C:  Yeah , like the entropy of the  the output , 
C:  or  
C:  It  it seems that the VAD network doesn't  Well , it doesn't drop , uh , too many frames 
C:  because the dele the number of deletion is reasonable . 
C:  But it 's just when we add the tandem , the final MLP , and then  
C:  u 
C:  But  
C:  Yeah . 
C:  Right . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Yeah . 
C:  So , m 
C:  Yeah . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Yeah , and the other thing  
C:  Well , there are other issues maybe for the tandem , 
C:  like , uh , well , do we want to , w uh n Do we want to work on the targets ? 
C:  Or , like , instead of using phonemes , using more context dependent units ? 
C:  Well , I 'm  
C:  Yeah . 
C:  I 'm thinking , also , a w about Dan 's work where he  he trained <outbreath> a network , not on phoneme targets but on the HMM state targets . 
C:  And  
C:  it was giving s slightly better results . 
C:  Yeah . 
C:  Yeah . 
C:  Uh  
C:  Mmm . 
C:  I was just thinking maybe about , like , generalized diphones , 
C:  and  come up with a  a reasonable , not too large , set of context dependent units , 
C:  and  
C:  and  
C:  Yeah . 
C:  And then anyway we would have to reduce this with the KLT . 
C:  So . 
C:  But  
C:  I don't know . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Mm - hmm . 
C:  Oh , it would be  
C:  Yeah . 
C:  Around five percent better , I guess . 
C:  If  if  i 
C:  Yeah . 
C:  If you extrapolate the SpeechDat - Car well - matched and medium - mismatch , it 's around , yeah , maybe five . 
C:  Sixty - two , 
C:  yeah . 
C:  Well , it 's around five percent , because it 's  s 
C:  Right ? 
C:  If everything is five percent . 
C:  Mm - hmm . 
C:  I d I d I just have the SpeechDat - Car right now , 
C:  so  
C:  It 's running  
C:  it shou we should have the results today during the afternoon , 
C:  but  
C:  Well . 
C:  I don't know . 
C:  The meeting in  is the five and six of December . 
C:  So  
C:  Mm - hmm . 
C:  Yeah , 
C:  um  
C:  so the evaluation should be on a week before 
C:  or  
