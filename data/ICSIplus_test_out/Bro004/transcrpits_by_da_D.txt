D:  Yeah . 
D:  Channel  channel three , 
D:  yeah . 
D:  OK . 
D:  Mike four . 
D:  Uh so .  uh  We  So  As I was already said , we  we mainly focused on uh four kind of features . 
D:  The PLP , the PLP with JRASTA , the MSG , and the MFCC from the baseline Aurora . 
D:  Uh , and we focused for the  the test part on the English and the Italian . 
D:  Um . We 've trained uh several neural networks on  
D:  so  on the TI - digits English  and on the Italian data 
D:  and also on the broad uh  English uh French and uh Spanish databases . 
D:  Mmm , so there 's our result tables here , for the tandem approach , 
D:  and um , actually what we  we @ @ observed is that if the network is trained on the task data it works pretty well . 
D:  Yeah , 
D:  so if the network is trained on the task data um  tandem works pretty well . 
D:  And uh actually we have uh , 
D:  results are similar Only on , 
D:  yeah . 
D:  Just that task . 
D:  But actually we didn't train network on  uh both types of data 
D:  I mean  uh  phonetically ba 
D:  phonetically balanced uh data and task data . 
D:  We only did either task  task data or  uh broad  data . 
D:  Um  Yeah . 
D:  So , 
D:  Mmm . Yeah . 
D:  If we use the same language ? 
D:  Mm - hmm . 
D:  But  Yeah but I did not uh do that . 
D:  We  No , we did four  four kind of  of testing , actually . 
D:  The first testing is  with task data  
D:  So , with nets trained on task data . 
D:  So for Italian on the Italian speech @ @ . 
D:  The second test is trained on a single language um with broad database , 
D:  but the same language as the t task data . 
D:  But for Italian we choose Spanish which  we assume is close to Italian . 
D:  The third test is by using , um the three language database 
D:  and the fourth is 
D:  This includes  
D:  Yeah . 
D:  But  not digits . 
D:  I mean it 's  
D:  Yeah 
D:  And the fourth test is uh  excluding from these three languages the language  that is  the task language . 
D:  Yeah . 
D:  Uh , yeah . 
D:  So um  for uh TI - digits for ins 
D:  example  uh when we go from TI - digits training to  TIMIT training  uh we lose  uh around ten percent , 
D:  uh . The error rate increase u of  of  of ten percent , relative . 
D:  So this is not so bad . 
D:  And then when we jump to the multilingual data it 's uh it become worse 
D:  and , well Around uh , let 's say ,  twenty perc twenty percent further . 
D:  So . 
D:  Yeah . 
D:  Twenty to  to thirty percent further . 
D:  Yeah . 
D:  Yeah . 
D:  Yeah . Yeah . 
D:  But the first step is al already removing the task s specific from  from  
D:  So . 
D:  And we lose  
D:  Yeah . 
D:  Uh  So , basically when it 's trained on the  the multilingual broad data  um or number  
D:  so , 
D:  the  the  ratio of 
D:  our error rates uh with the  baseline error rate is around  uh one point one . 
D:  So . 
D:  No no no . 
D:  Uh same language we are at uh  for at English at O point eight . 
D:  So it improves ,  compared to the baseline . 
D:  But  
D:  So . 
D:  Le - let me . 
D:  Tas - task data 
D:  we are u 
D:  Yeah . 
D:  Mmm . 
D:  Hmm . 
D:  Oh yeah , the f Yeah , OK . 
D:  Yeah . 
D:  Yeah . 
D:  Sure . 
D:  Mmm . 
D:  Mm - hmm . 
D:  Mm - hmm . 
D:  Mm - hmm . 
D:  Yeah , it 's around one point one . 
D:  Yeah . 
D:  Ye 
D:  Uh , more actually . 
D:  If I  Yeah . 
D:  What would you say ? 
D:  Around one point four 
D:  yeah . 
D:  If we exclude English ,  um  there is  not much difference with the  data 
D:  with English . 
D:  So . Yeah . 
D:  Uh . 
D:  Hmm . 
D:  Yeah . 
D:  The only difference it 's  is that it 's multilingual  
D:  Um 
D:  Yeah . 
D:  Yeah sure . 
D:  Uh yeah . 
D:  A part of it , 
D:  yeah . 
D:  Um  It 's two times , 
D:  actually ? 
D:  Yeah . 
D:  Um . The English data   No , the multilingual databases are two times the  broad English  data . 
D:  We just wanted to keep this , w well , not too huge . 
D:  So . 
D:  I think so . 
D:  Do you  
D:  Uh , Yeah . 
D:  Yeah . 
D:  Mm - hmm . 
D:  Yeah . 
D:  Hmm ? 

D:  Mmm . 
D:  Uh , let me check . 
D:  Uh . 
D:  So . This was for the PLP , 
D:  Um . The  
D:  Yeah . 
D:  For the PLP with JRASTA the   the  we  
D:  This is quite the same  tendency ,  with a slight increase of the error rate ,  uh if we go to  to TIMIT . 
D:  And then it 's  it gets worse with the multilingual . 
D:  Um . Yeah . 
D:  There  there is a difference actually with  b between PLP and JRASTA 
D:  is that  JRASTA  seems to  perform better with the highly mismatched  condition  but slightly  slightly worse  for the well matched condition . 
D:  Mmm . 
D:  Yeah , yeah . 
D:  OK . 
D:  Uh , no , no . 
D:  Training on a single language , you mean , 
D:  and testing on the other one ? 
D:  Uh , no . 
D:  So the only  task that 's similar to this is the training on two languages , 
D:  and  that  
D:  Uh , No . 
D:  Either thi this is test with  uh the same language  but from the broad data , 
D:  or it 's test with  uh different languages 
D:  also from the broad data , 
D:  excluding the  
D:  So , it 's  it 's three or  three and four . 
D:  Uh . No . 
D:  You mean  training digits  on one language and using the net  to recognize on the other ? 
D:  No . 
D:  Uh ,  No , I don't think so . 
D:  So . 
D:  So you have uh basically two  uh parts . 
D:  The upper part is for TI - digits 
D:  and it 's divided in three  rows  of four  four rows each . 
D:  And the first four rows is well - matched , 
D:  then the s the second group of four rows is mismatched , 
D:  and  finally highly mismatched . 
D:  And then the lower part is for Italian 
D:  and it 's the same   the same thing . 
D:  So . It 's  it 's the HTK results , I mean . 
D:  So it 's  HTK training testings  with different kind of features 
D:  and what appears in the  uh left column is  the networks that are used for doing this . 
D:  So . Uh Yeah . 
D:  It - It was part of these results . 
D:  Mmm . 
D:  Mmm . 
D:  You mean the HTK Aurora baseline ? 
D:  It 's uh the one hundred number . 
D:  It 's , well , all these numbers are the ratio  with respect to the baseline . 
D:  Yeah , this is  a word error rate ratio . 
D:  Yeah . 
D:  So , seventy point two means that  we reduced the error rate uh by thirty  thirty percent . 
D:  So . 
D:  Hmm . 
D:  Yeah . 
D:  To TIMIT . 
D:  Mmm . 
D:  Then you have  uh MF ,  MS and ME 
D:  which are for French , Spanish and English . 
D:  And , 
D:  yeah . 
D:  Actually I   I uh forgot to say that  the multilingual net are trained  on  uh  features without the s derivatives 
D:  uh but with  increased frame numbers . 
D:  Mmm . 
D:  And we can  we can see on the first line of the table that it  it   it 's slightly  slightly worse when we don't use 
D:  delta but it 's not   not that much . 
D:  So . Multi - French , Multi - Spanish , and Multi - English . 
D:  Yeah . 
D:  Yeah . 
D:  Yeah . 
D:  Still poor . 
D:  Yeah . Yeah . 
D:  No these are the s s s same noises , 
D:  yeah . 
D:  At least  at least for the first   for the well - matched , 
D:  yeah . 
D:  Mmm . 
D:  Yeah . 
D:  Yeah , so for the Italian the results are <outbreath> uh  stranger 
D:  um  Mmm . 
D:  So what appears is that perhaps Spanish is  not very close to Italian 
D:  because uh , well ,  when using the  the network trained only on Spanish it 's   the error rate is  almost uh twice  the baseline error rate . 
D:  Mmm . <inbreath> Uh . 
D:  Yeah . 
D:  There  there is  another difference , is that the noise  the noises are different . 
D:  Well , For  for the Italian part I mean 
D:  the  uh  the um  networks are trained with noise from  Aurora  TI - digits , 
D:  mmm . 
D:  Yeah . 
D:  And perhaps the noise are  quite different from the noises  in the speech that Italian . 
D:  And  
D:  No . 
D:  Yeah . 
D:  Yeah . 
D:  It 's  
D:  no , 
D:  the third part , 
D:  so it 's uh  highly mismatched . 
D:  So . Training and  test noise are different . 
D:  Yeah . 
D:  Yeah . Yeah . 
D:  Yeah .  But it 's not a clean case . 
D:  It 's  a noisy case 
D:  but  uh training and test noises are the same . 
D:  So  Yeah . 
D:  Yeah . 
D:  So it 's always noisy basically , 
D:  and ,  well , the  
D:  Mmm . 
D:  Uh , no we don't plan to fill the holes 
D:  but  actually there is something important , 
D:  is that  um we made a lot of assumption concerning the on - line normalization 
D:  and we just noticed  uh recently that  uh the  approach that we were using  was not  uh  leading to very good results  when we  used the straight features to HTK . 
D:  Um   Mmm . 
D:  So basically d  if you look at the  at the left of the table ,  the first uh row ,  with eighty - six , one hundred , and forty - three and seventy - five , 
D:  these are the results we obtained for Italian  uh with  straight  mmm , PLP features  using on - line normalization . 
D:  Mmm . And the , mmm  what 's  in the table , just  at the left of the PLP twelve  on - line normalization column , 
D:  so , the numbers seventy - nine , fifty - four and  uh forty - two  are the results obtained by uh Pratibha with  uh his on - line normalization  uh her on - line normalization approach . 
D:  So . 
D:  Just  uh 
D:  Yeah . 
D:  So these are the results of  OGI with  on - line normalization 
D:  and straight features to HTK . 
D:  And the previous result , eighty - six and so on ,  are with our  features straight to HTK . 
D:  So  what we see that  is  there is that um  uh the way we were doing this was not correct , 
D:  but  still  the networks  are very good . 
D:  When we use the networks  our number are better that  uh Pratibha results . 
D:  Yeah . 
D:  There were diff there were different things 
D:  and  basically ,  the first thing is the mmm ,  alpha uh  value . 
D:  So , the recursion  uh  part . 
D:  um ,  I used point five percent ,  which was the default value in the   in the programs here . 
D:  And Pratibha used five percent . 
D:  So it adapts more  quickly 
D:  Um , but , 
D:  yeah . 
D:  I assume that this was not important because  uh previous results from  from Dan and  show that basically  the  both  both values g give the same  same  uh results . 
D:  It was true on uh  TI - digits but it 's not true on Italian . 
D:  Uh , second thing is the initialization of the  stuff . 
D:  Actually ,  uh what we were doing is to start the recursion from the beginning of the  utterance . 
D:  And using initial values that are the global mean and variances  measured across the whole database . 
D:  And Pratibha did something different is 
D:  that he  uh she initialed the um values of the mean and variance  by computing  this on the  twenty - five first frames of each utterance . 
D:  Mmm . There were other minor differences , 
D:  the fact that  she used fifteen dissities instead s instead of thirteen , 
D:  and that she used C - zero instead of log energy . 
D:  Uh , but the main differences concerns the recursion . 
D:  So .  Uh , I changed the code 
D:  uh and now we have a baseline that 's similar to the OGI baseline . 
D:  We  It  it 's slightly  uh different 
D:  because  I don't exactly initialize the same way she does . 
D:  Actually I start ,  mmm , I don't wait to a fifteen  twenty - five  twenty - five frames  before computing a mean and the variance  to e to  to start the recursion . 
D:  I  I use the on - line scheme 
D:  and only start the re recursion after the twenty - five   twenty - fifth frame . 
D:  But , well it 's similar . 
D:  So  uh I retrained  the networks with  these  
D:  well , the  the  the networks are retaining with these new  features . 
D:  And , 
D:  yeah . 
D:  So basically what I expect is that  these numbers will a little bit go down 
D:  but  perhaps not  not so much 
D:  because  I think the neural networks learn perhaps  to  
D:  even if the features are not  normalized . It  it will learn how to normalize 
D:  and  
D:  Yeah . 
D:  Yeah <inbreath> I 
D:  No , 
D:  I  we plan to start this 
D:  uh so , act actually we have discussed uh  @ @ um , these  
D:  what we could do  more as a  as a research 
D:  and   and  we were thinking perhaps that  uh  the way we use the tandem is not  
D:  Uh , well , there is basically perhaps a flaw in the  in the  the stuff 
D:  because  we  trained the networks  
D:  If we trained the networks on the  on  a language and a t or a specific  task , 
D:  um , what we ask is  to the network  is to put the bound the decision boundaries somewhere in the space . 
D:  And uh  mmm and ask the network to put one ,  at one side of the  for  for a particular phoneme at one side of the boundary  decision boundary 
D:  and one for another phoneme at the other side . 
D:  And  so there is kind of reduction of the information there that 's not correct 
D:  because if we change task  and if the phonemes are not in the same context in the new task ,  obviously the  decision boundaries are not   should not be at the same  place . 
D:  But the way the feature gives  The  the way the network gives the features is that it reduce completely the   it removes completely the information   a lot of information from the  the features  by uh  uh  placing the decision boundaries at  optimal places for  one kind of  data 
D:  but  this is not the case for another kind of data . 
D:  So  
D:  Yeah . 
D:  So uh what we were thinking about is perhaps  um one way  to solve this problem is increase the number of  outputs of the neural networks . 
D:  Doing something like , um  um phonemes within context and , 
D:  well , basically context dependent phonemes . 
D:  Yeah but , we know that  
D:  Ye - yeah but here it 's something different . 
D:  We want to have features 
D:  uh well ,  um . 
D:  Mm - hmm . 
D:  Yeah . 
D:  Yeah , but  mmm , I mean ,  the  
D:  the way we  we do it now is that we have a neural network 
D:  and  basically  the net network is trained almost to give binary decisions . 
D:  And  uh  binary decisions about phonemes . 
D:  Nnn  Uh It 's  
D:  Yeah . 
D:  Yeah . 
D:  Yeah , sure 
D:  but 
D:  uh  So basically it 's almost binary decisions 
D:  and  um the idea of using more  classes is  to  get something that 's  less binary decisions . 
D:  But  yeah , but  
D:  Yeah , but if  
D:  Mmm . 
D:  Mm - hmm . 
D:  Mmm . 
D:  Yeah , but I think  
D:  Yeah , perhaps you 're right , 
D:  but you have more classes 
D:  so  you  you have more information in your features . 
D:  So , <inbreath> Um  You have more information in the  uh 
D:  posteriors vector 
D:  um 
D:  which means that  
D:  But still the information is relevant 
D:  because it 's  it 's information that helps to discriminate , 
D:  if it 's possible to be able to discriminate  among the phonemes in context . 
D:  But the  
D:  Mmm . 
D:  Mmm . 
D:  Mmm . 
D:  Mmm . 
D:  Mm - hmm . 
D:  Mm - hmm . 
D:  N 
D:  Yeah . 
D:  Yeah . 
D:  Uh so there is this combination , 
D:  yeah . 
D:  Working on combination obviously . 
D:  Um , I will start work on multi - band . 
D:  And  we  plan to work also on the idea of using both  features  and net outputs . 
D:  Um . And  we think that  with this approach perhaps  we could reduce the number of outputs of the neural network . 
D:  Um , So , get simpler networks , 
D:  because we still have the features . 
D:  So we have um  come up with um  different kind of  broad phonetic categories . 
D:  And we have  Basically we have three  types of broad phonetic classes . 
D:  Well , something using place of articulation 
D:  which  which leads to  nine , I think ,  broad classes . 
D:  Uh , another which is based on manner , 
D:  which is  is also something like nine classes . 
D:  And then ,  something that combine both , 
D:  and we have  twenty f  twenty - five ? 
D:  Twenty - seven broad classes . 
D:  So like , uh , 
D:  oh , I don't know , 
D:  like back vowels , front vowels . 
D:  Um For the moments we do not  don't have nets , 
D:  I mean ,  It 's just  Were we just changing  the labels to retrain nets  with fewer out outputs . 
D:  And then  
D:  Mm - hmm . 
D:  It - It 's the single net , 
D:  yeah . 
D:  It 's one net with  um  twenty - seven outputs 
D:  if we have twenty - seven classes , 
D:  yeah . 
D:  So it 's  Well , it 's basically a standard net with fewer  classes . 
D:  Yeah , 
D:  but I think  
D:  Yeah . 
D:  B b including the features , yeah . 
D:  I don't think this  will work  alone . 
D:  I think it will get worse 
D:  because Well , I believe the effect that  of  of too reducing too much the information is  basically  basically what happens 
D:  and  
D:  but  
D:  Yeah , 
D:  because  there is perhaps one important thing that the net  brings , 
D:  and OGI show showed that , is  the distinction between  sp speech and silence 
D:  Because these nets are trained on well - controlled condition . 
D:  I mean the labels are obtained on clean speech , and we add noise after . 
D:  So this is one thing 
D:  And 
D:  But perhaps , something intermediary using also  some broad classes could  could bring so much more information . 
D:  Uh . 
D:  Yeah . 
D:  Yeah . 
D:  Mm - hmm . 
D:  There will probably be , 
D:  yeah , 
D:  one single KL to transform everything 
D:  or <inbreath>  uh , 
D:  per 
D:  This is  still something  that 
D:  yeah , 
D:  we  don't know  
D:  Yeah . 
D:  Yeah . 
D:  Mmm . 
D:  Uh , yeah . 
D:  Mm - hmm . 
D:  Mm - hmm . 
D:  Uh , no . I don't think so . 
D:  Yeah , I have one . 
D:  Mm - hmm . 
D:  Mm - hmm . 
D:  Yeah . 
D:  Mmm . 
D:  Mmm . 
D:  Mmm . 
D:  The features , 
D:  yeah . 
D:  Yeah . 
D:  Yeah , I don't know . 
D:  I think   I think it 's the C - zero  using C - zero instead of log energy . 
D:  Yeah , it 's this . 
D:  It should be that , 
D:  yeah . 
D:  Because  
D:  Yeah . 
D:  i 
D:  Yeah . 
D:  Yeah . 
D:  Yeah , you can basically remove the  the frames from the feature  feature files . 
D:  I t 
D:  Mm - hmm . 
D:  Yeah . 
D:  I think we 're alright , 
D:  um ,  not much problems with that . 
D:  It 's OK . 
D:  Well this table took uh  more than five days to get back . 
D:  But  
D:  Yeah . 
D:  Mmm , no . 
D:  You were using Gin  perhaps , 
D:  yeah ? 
D:  No . 
D:  Hmm .  Mm - hmm . 
D:  Mmm . 
D:  Yeah . 
D:  Yeah . 
D:  For HTK ? 
D:  Uh Training is longer . 
D:  Yeah . 
D:  Mmm . 
D:  Mmm . 
D:  Yeah . 
