E:  Sure . 
E:  Yep . 
E:  That 's right . 
E:  Yeah . 
E:  Yeah , that 's usually what we do . 
E:  We just sorta go around and people say what 's going on , what 's the latest uh  
E:  Yeah . 
E:  That would be great . 
E:  Yeah . 
E:  It seemed like there were still some issues , 
E:  right ? 
E:  that they were trying to decide ? 
E:  Like the voice activity detector , 
E:  Right . 
E:  Right . 
E:  Right . 
E:  Right . 
E:  Mm - hmm . 
E:  Right . 
E:  Yeah . 
E:  Right . 
E:  Right . 
E:  Mm - hmm . 
E:  Yeah . 
E:  Mm - hmm . 
E:  Right . 
E:  Right . 
E:  Mm - hmm . 
E:  Ah . 
E:  Hmm . 
E:  Right . 
E:  But wait a minute , 
E:  I thought the endpointing really only helped in the noisy cases . 
E:  Oh , but you still have that with the MFCC . 
E:  OK . 
E:  Yeah . 
E:  Right . 
E:  Yeah . 
E:  Yeah . 
E:  Yeah . 
E:  Hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Hmm . 
E:  So what about the issue of um the weights on the  for the different systems , 
E:  the well - matched , and medium - mismatched and  
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  And they 're the  staying the same ? 
E:  Mm - hmm . 
E:  Right . 
E:  Yeah . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Hmm . 
E:  You haven't tried that yet ? 
E:  Hmm . 
E:  Hmm ! 
E:  Oh ! 
E:  I see . 
E:  Right . 
E:  Well you 're  you 're basically y 
E:  Yeah . 
E:  So you 're making all your training data more uniform . 
E:  Hmm . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Hmm . 
E:  Hmm ! 
E:  What is that ? 
E:  The TRAPS sound familiar , I  but I don't  
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Hmm . 
E:  But you have many of those vectors per phoneme , 
E:  right ? 
E:  Uh - huh . 
E:  Hmm . 
E:  Hmm . 
E:  Hmm . 
E:  How wide are the uh frequency bands ? 
E:  Well how do you  how do you uh convert this uh energy over time in a particular frequency band into a vector of numbers ? 
E:  Yeah but what 's the number ? 
E:  Is it just the  
E:  it 's just the amount of energy in that band from f in that time interval . 
E:  OK . 
E:  Mm - hmm . 
E:  Yeah . 
E:  Yeah . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Hmm . 
E:  Mm - hmm . 
E:  Hmm . 
E:  So it 's really w 
E:  It 's sort of like saying that what 's happening at one kilohertz depends on what 's happening around it . 
E:  It 's sort of relative to it . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Hey Stephane . 
E:  Yep . 
E:  Sure . 
E:  Go ahead . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Mmm . 
E:  Mmm . 
E:  Yeah . 
E:  Mm - hmm . 
E:  Yeah . 
E:  Mm - hmm . 
E:  But you probably won't have anything before the next time we have to evaluate , 
E:  right ? 
E:  Yeah . 
E:  Ah . 
E:  Latency and things . 
E:  Mm - hmm . 
E:  Yeah . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Wow . 
E:  Is he getting married or something ? 
E:  Right . 
E:  Well , I mean , I 've known other friends who  they  they go to Ind - they go back home to India for a month , they come back married , 
E:  you know , huh . 
E:  So he finally had some incentive to finish , 
E:  huh ? 
E:  Oh . 
E:  When he knew you were happy , 
E:  huh ? 
E:  Hmm . 
E:  Hmm . 
E:  So have the um  
E:  when is the next uh evaluation ? 
E:  June or something ? 
E:  No , for uh Aurora ? 
E:  Hmm . 
E:  Oh , OK . 
E:  Are people supposed to rerun their systems , 
E:  or  ? 
E:  Hmm . 
E:  Wow . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Do you know what the new baseline is ? 
E:  Oh , I guess if you don't have  
E:  Using your uh voice activity detector ? 
E:  Similar , yeah . 
E:  Mm - hmm . 
E:  Yeah . 
E:  Yeah . 
E:  Oh , OK . 
E:  Hmm . 
E:  Hmm . 
E:  Wow . 
E:  Where 's uh Guenter going ? 
E:  Mm - hmm . 
E:  Hmm ! 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Hmm ! 
E:  Mm - hmm . 
E:  Wow ! 
E:  Hmm . 
E:  Hmm . 
E:  How 's your documentation or whatever 
E:  it w what was it you guys were working on last week ? 
E:  Hmm . 
E:  So have you been running some new experiments ? 
E:  I  I thought I saw some jobs of yours running on some of the machine  
E:  Really ? ! 
E:  That has no effect ? 
E:  Eh  Is this in the baseline ? 
E:  or in uh  
E:  in  
E:  uh - huh , uh - huh . 
E:  In some what ? 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  You 're compressing the range , 
E:  right ? 
E:  of that  
E:  Mm - hmm . 
E:  Oh . 
E:  Yeah . 
E:  Uh - huh . 
E:  Right . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Insertions . 
E:  Oh . 
E:  Uh - huh . 
E:  You need like a  some kind of a  
E:  You need to have a genetic algorithm , that basically tries random permutations of these things . 
E:  Yeah . 
E:  Mm - hmm . 
E:  Yeah . 
E:  You know actually , I don't know that this wouldn't be all that bad . 
E:  I mean you  you compute the features once , 
E:  right ? 
E:  And then these exponents are just applied to that  
E:  So . 
E:  And is this something that you would adjust for training ? 
E:  or only recognition ? 
E:  You would do it on both . 
E:  So you 'd actually  
E:  Mm - hmm . 
E:  So for each  uh set of exponents that you would try , it would require a training and a recognition ? 
E:  Oh . 
E:  So if you  
E:  Instead of altering the feature vectors themselves , you  you modify the  the  the Gaussians in the models . 
E:  Uh - huh . 
E:  But why  if you 're  if you 're multi if you 're altering the model , why w in the test data , why would you have to muck with the uh cepstral coefficients ? 
E:  No . But you 're running your data through that same model . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  All of the  all of the mean and variances that correspond to C - one , you put them to zero . 
E:  Yeah . 
E:  But what are you multiplying ? 
E:  Cuz those are means , 
E:  right ? 
E:  I mean you 're  
E:  I think you  
E:  Yeah , I think you 'd have to modify the standard deviation or something , so that you make it <inbreath> wider or narrower . 
E:  Oop . 
E:  Sorry . 
E:  So . 
E:  So by making th the standard deviation narrower ,  uh your scores get worse for  
E:  unless it 's exactly right on the mean . 
E:  Right ? 
E:  I mean there 's  you 're  you 're allowing for less variance . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Couldn't you just do that to the test data and not do anything with your training data ? 
E:  Uh - huh . 
E:  Mm - hmm . 
E:  Could you also if you wanted to  
E:  if you wanted to try an experiment uh by  leaving out say , C - one , couldn't you , in your test data , uh modify the  all of the C - one values to be um way outside of the normal range of the Gaussian for C - one that was trained in the model ? 
E:  So that effectively , the C - one never really contributes to the score ? 
E:  Do you know what I 'm say 
E:  Yeah , 
E:  someth 
E:  Mm - hmm . 
E:  But what if you set if to the mean of the model , then ? 
E:  And it was a cons you set all C - ones coming in through your test data , you  you change whatever value that was there to the mean that your model had . 
E:  Yeah . 
E:  Oh , that 's true , 
E:  right , 
E:  yeah , because you  you have  
E:  Yeah . 
E:  Mm - hmm . Mm - hmm . 


E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  You 're talking about the standard deviation ? 
E:  Yeah . 
E:  Mm - hmm . 
E:  But  don't you have to do something to the mean , also ? 
E:  But if your  
E:  If your um original data for C - one had a mean of two . 
E:  And now you 're  you 're  you 're changing that by squaring it . 
E:  Now your mean of your C - one original data has   is four . 
E:  But your model still has a mean of two . 
E:  So even though you 've expended the range , your mean doesn't match anymore . 
E:  Do you see what I mean ? 
E:  Mm - hmm . 
E:  Yeah . 
E:  Right . 
E:  Yeah , so y 
E:  It 's predictable , yeah . 
E:  Yeah . 
E:  But as a simple thing , you could just  just muck with the variance . 
E:  to get uh this  uh this  the effect I think that you 're talking about , 
E:  right ? 
E:  Could increase the variance to decrease the importance . 
E:  Yeah , because if you had a huge variance , you 're dividing by a large number ,  you get a very small contribution . 
E:  Yeah . 
E:  Hmm . 
E:  Yeah , you know actually , this reminds me of something that happened uh when I was at BBN . 
E:  We were playing with putting um pitch into the Mandarin recognizer . 
E:  And this particular pitch algorithm um when it didn't think there was any voicing , was spitting out zeros . 
E:  So we were getting  uh when we did clustering , we were getting groups uh of features 
E:  yeah , with  with a mean of zero and basically zero variance . 
E:  So , when ener  when anytime any one of those vectors came in that had a zero in it , we got a great score . 
E:  I mean it was just , <mike click> you know , incredibly <mike click> high score , and so that was throwing everything off . 
E:  So <laugh> if you have very small variance you get really good scores when you get something that matches . 
E:  So . <inbreath> So that 's a way , yeah , yeah  
E:  That 's a way to increase the  yeah , n 
E:  That 's interesting . 
E:  So in fact , that would be  
E:  That doesn't require any retraining . 
E:  So that means it 's just 
E:  recognitions . 
E:  Yeah . 
E:  You  you have a step where you you modify the models , make a d copy of your models with whatever variance modifications you make , and rerun recognition . 
E:  And then do a whole bunch of those . 
E:  That could be set up fairly easily I think , 
E:  and you have a whole bunch of 
E:  you know  
E:  That 's an interesting idea , actually . 
E:  For testing the  
E:  Yeah . 
E:  Huh ! 
E:  That 's right . 
E:  In fact , and  and they 're just t right now they 're installing uh  increasing the memory on that uh  the Linux box . 
E:  Yeah . 
E:  Absinthe . 
E:  Absinthe . 
E:  We 've got five processors on that . 
E:  And two gigs of memory . 
E:  Yeah . 
E:  Exactly . 
E:  Yeah . 
E:  See how many cycles we used ? 
E:  Yeah . 
E:  Uh - huh . 
E:  Oh , my gosh ! 
E:  So he had to make it look like  
E:  Yeah . 
E:  How  
E:  Idle time . 
E:  Yeah . 
E:  Have you ever seen those little 
E:  um  
E:  It 's  it 's this thing that 's the shape of a bird and it has a red ball and its beak dips into the water ? 
E:  So <laugh> if you could hook that up so it hit the keyboard  
E:  That 's an interesting experiment . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Ugh ! 
E:  Yeah . 
E:  Yeah . 
E:  Yeah . 
E:  And there wasn't ? 
E:  Huh ! 
E:  Well that 's  that 's a really i 
E:  That wouldn't be too difficult to try . 
E:  Maybe I could set that up . 
E:  And we 'll just  
E:  Yeah , so the first set of uh variance weighting vectors would be just you know one  modifying one and leaving the others the same . 
E:  And  and do that for each one . 
E:  That would be one set of experiment  
E:  Wh - yeah , when the data matches that , then you get really  
E:  Yeah . 
E:  Right . 
E:  But there could just naturally be low variance . 
E:  Because I  Like , I 've noticed in the higher cepstral coefficients , the numbers seem to get smaller , 
E:  right ? 
E:  So d 
E:  I mean , just naturally . 
E:  Yeah . 
E:  Exactly . 
E:  And so it seems like they 're already sort of compressed . 
E:  The range  of values . 
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Mm - hmm . 
E:  Hmm . 
E:  Mm - hmm . 
E:  Hmm . 

E:  Is the um  
E:  if we mail to " Aurora - inhouse " , does that go up to you guys also ? 
E:  OK . 
E:  So i What is it  
E:  Yeah we sh 
E:  Do we have a mailing list that includes uh the OGI people ? 
E:  Oh ! 
E:  Maybe we should set that up . 
E:  That would make it much easier . 
E:  So maybe just call it " Aurora " or something that would  
E:  Mm - hmm . 
E:  Mm - hmm . 
E:  OK . 
E:  Maybe we can set that up . 
E:  Mm - hmm . 
E:  Hmm ! 
E:  Mm - hmm . 
E:  Hmm . 
E:  Hmm . 
E:  Mm - hmm . 
E:  Wow ! 

E:  Hmm . 
E:  Yeah . 
E:  Wow ! 
E:  Hmm . 
E:  Does anybody have anything else ? 
E:  to  
E:  Shall we read some digits ? 
E:  So . 
E:  Hynek , I don't know if you 've ever done this . 
E:  The way that it works is each person goes around in turn ,  and uh you say the transcript number and then you read the digits , the  the strings of numbers as individual digits . 
E:  So you don't say " eight hundred and fifty " , you say " eight five oh " , and so forth . 
E:  Um . 
E:  Sure . 
