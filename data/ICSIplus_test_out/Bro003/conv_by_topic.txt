D:  Channel one . 
G:  Test . 
B:  <whistling> 
E:  Hello . 
D:  Channel three . 
G:  Test . 
A:  Uh - oh . 
G:  <mike clicks> 
C:  <laugh> 
A:  <microphone noise> 
F:  So you think we 're going now , yes ? 
B:  <whistling> 
F:  OK , good . 
F:  Alright 
F:  Going again 
F:  Uh  
F:  So we 're gonna go around as before , and uh do  do our digits . 
F:  <snap> 
F:  Uh 


F:  transcript one three one one dash one three three zero .  
F:  three two three  
F:  four seven six five  
F:  five three one six two four one  
F:  six seven  
F:  seven  
F:  eight  
F:  nine zero nine four zero zero three  
F:  zero one five eight  
F:  one seven three five three  
F:  two six eight zero  
F:  three six two four three zero seven  
F:  four  
F:  five zero six nine four  
F:  seven four  
F:  eight five seven  
F:  nine six one five  
F:  O seven eight O two  
F:  zero nine six zero four zero zero  
F:  one  
F:  two  
D:  I am reading transcript one three nine one one four one zero  
D:  six  
D:  seven  
D:  eight  
D:  nine O six nine eight  
D:  zero one three one nine  
D:  one six  
D:  two three seven  
D:  three  
D:  four  
D:  five  
D:  six  
D:  eight four seven  
D:  nine two zero seven five  
D:  O three six nine six  
D:  zero nine three one zero zero three  
D:  one  
D:  two O  
D:  three zero nine seven  
D:  five two six seven nine eight three  
D:  six seven zero six  
A:  OK I 've got transcript one two seven one one two nine zero .  
A:  one seven one O  
A:  two eight one two zero seven one  
A:  three  
A:  four  
A:  five zero nine six zero eight zero  
A:  seven three  
A:  eight six eight one three  
A:  nine three four zero three nine four  
A:  O four two  
A:  zero  
A:  one  
A:  two zero three eight two three two  
A:  four two eight one six  
A:  five three five eight three O eight  
A:  six five nine one zero seven six  
A:  seven six six two  
A:  eight seven  
A:  nine  
A:  O O five seven eight O four  
A:  one four one  
B:  Transcript number one two nine one dash one three one zero .  
B:  two three nine zero two  
B:  three eight one five two  
B:  four seven four six seven  
B:  five  
B:  six  
B:  seven zero  
B:  nine four  
B:  O one O O  
B:  zero seven two six zero  
B:  one six zero four six four five  
B:  two  
B:  three  
B:  four zero seven two  
B:  six four four zero  
B:  seven four one eight eight  
B:  eight eight five six three  
B:  nine eight nine  
B:  O  
B:  zero zero three nine  
B:  one  
E:  Transcript number one one seven one dash one one nine zero .  
E:  eight three four three five  
E:  nine four three nine  
E:  O three  
E:  zero nine six nine one  
E:  one  
E:  two O O  
E:  three  
E:  five five six O eight  
E:  six three zero four zero one one  
E:  seven eight eight two  
E:  eight  
E:  nine eight five five  
E:  O  
E:  zero zero  
E:  two one four  
E:  three six seven O two  
E:  four seven nine  
E:  five nine zero five seven three five  
E:  six seven six nine four six five  
E:  seven  
C:  OK , <clears throat> this is Barry Chen and I am reading transcript 
C:  one three five one dash one three seven zero , um  
C:  four  
C:  five O seven eight nine two eight  
C:  seven one four eight nine  
C:  eight five nine three six  
C:  nine seven one five nine  
C:  O four seven three O - one O O  
C:  zero  
C:  one  
C:  two zero six two  
C:  four three four six seven  
C:  five four four  
C:  six  
C:  seven eight nine nine  
C:  eight  
C:  nine  
C:  O O one  
C:  one two four seven three four nine  
C:  two five  
C:  three five zero zero eight  
C:  four eight  
G:  Transcript <clicks> one three seven one one three nine O .  
G:  five  
G:  six  
G:  seven O O five  
G:  nine two six three three  
G:  O five five  
G:  zero four nine  
G:  one six  
G:  two  
G:  three O O two one nine two  
G:  four  
G:  six six  
G:  seven two O eight five O three  
G:  eight six  
G:  nine  
G:  O eight eight seven four  
G:  zero  
G:  one zero eight  
G:  three two four five  
G:  four five  
G:  five six nine two three  


F:  Uh  
F:  Yeah , you don't actually n need to say the name . 
F:  That 'll probably be bleeped out . 
C:  OK . 
F:  So . That 's if these are anonymized , but 
C:  <laugh> 
F:  <laugh> 
C:  Oh .  OK . 
F:  Yeah  
C:  <laugh> 
F:  uh  I mean  not that there 's anything defamatory about uh  eight five seven or 
C:  <laugh> 
F:  <laugh> or anything , but 
B:  <laugh> 
A:  <laugh> 
G:  <laugh> 
E:  <laugh> 
B:  <clears throat> 
F:  Uh , anyway . 
C:  OK . 
F:  Uh  so here 's what I have for  I  I was just jotting down things I think th w that we should do today . 
F:  Uh  This is what I have for an agenda so far 
F:  Um , 
F:  We should talk a little bit about the plans for the uh  
F:  the field trip next week . 
F:  Uh  a number of us are doing a field trip to 
F:  uh 
F:  Uh  OGI 
F:  And uh  
F:  mostly uh 
F:  First though about the logistics for it . 
F:  Then maybe later on in the meeting we should talk about what we actually 
F:  you know , might accomplish . 
C:  <laugh> 
F:  Uh  
C:  OK . 
F:  Uh , in and  kind of go around  see what people have been doing  
F:  talk about that ,  a r progress report . Um , 
F:  Essentially . 
F:  Um  
F:  And then uh  
F:  Another topic I had was that uh  uh  
F:  Uh  
F:  Dave here had uh said uh " Give me something to do . " 
F:  And I  I have  I have uh  failed so far in doing that . 
F:  And so maybe we can discuss that a little bit . 
F:  If we find some holes in some things that  that  
F:  someone could use some help with , he 's  he 's volunteering to help . 
A:  I 've got to move a bunch of furniture .  
F:  <laugh> 
F:  OK , always count on a <laugh> 
F:  serious comment from that corner . 
F:  So , um , 
F:  uh , and uh , then uh , 
F:  talk a little bit about  about disks and resource  
F:  resource issues that  that 's starting to get worked out . 
F:  And then , 
F:  anything else anybody has that 
F:  isn't in that list ? 
F:  Uh  
D:  I was just wondering , does this mean the battery 's dying and I should change it ? 
F:  Uh 
F:  I think that means the battery 's O K .  d  do you 
A:  Let me see . 
D:  Oh OK , so th 
A:  Yeah , that 's good . 
F:  Yeah . 
A:  You 're alright ? 
D:  Cuz it 's full . 
F:  Yeah . 
F:  Yeah . 
D:  Alright . 
F:  Yeah . It looks 
F:  full of electrons . 
D:  <laugh> 
F:  <laugh> 
F:  OK . 
F:  Plenty of electrons left there . 


F:  OK , so , um , uh . OK , 
F:  so , uh , 
F:  I wanted to start this with this mundane thing . Um  
F:  Uh  I  I  it was  it was 
F:  kind of my bright idea to have us take a plane that leaves at seven twenty in the morning . 
A:  <laugh> 
F:  <inbreath> Um . 
C:  Oh , yeah , that 's right . 
F:  Uh <laugh> this is uh  
F:  The reason I did it uh was because otherwise for those of us who have to come back the same day it is really not much of a  
F:  of a visit . 
F:  Uh  <inbreath> 
F:  So um the issue is how  how  how would we ever accomplish that ? 
A:  <laugh> 
F:  Uh  what  what  what part of town do you live in ? 
C:  Um , I live in , um , the corner of campus . The , um , southeast corner . 
F:  OK . 
F:  OK , so 
F:  would it be easier  
F:  those of you who are not , 
F:  you know , used to this area , it can be very tricky to get to the airport at  
F:  at uh , you know , six thirty . 
F:  <inbreath> Um . So . 
F:  Would it be easier for you if you came here and I drove you ? 
F:  Yeah ? 
G:  Yeah , perhaps , yeah . 
F:  Yeah , yeah , OK . <laugh> 
E:  Yeah . 
G:  Mm - hmm . 
C:  Yeah . Sure . 
G:  <laugh> 
F:  OK , so if  if everybody can get here at six . 
E:  At six . 
F:  Yeah , I 'm afraid 
F:  we need to do that to get there on time . Yeah , so . 
C:  Six , OK . 
F:  <laugh> 
F:  <inbreath> Oh boy . Anyway , so . 
A:  Will that  be enough time ? 
F:  Yeah . 
F:  Yeah , so I 'll just pull up in front at six and just be out front . 
F:  And , uh , and yeah , that 'll be plenty of time . It 'll take  it  it  it won't be bad traffic that time of day and  
F:  and uh 
A:  I guess once you get past the bridge  that that would be the worst . 
F:  Going to Oakland . 
B:  Yeah , Oakland . 
C:  Oakland . 
A:  Yeah . 
F:  Bridge 
A:  Once you get past the turnoff to the  Bay Bridge . 
F:  oh , the turnoff to the bridge 
A:  Yeah . 
F:  Won't even do that . I mean , just go down Martin Luther King . 
B:  Yeah . 
A:  Yeah . OK . Mm - hmm . 
F:   And then Martin Luther King to nine - eighty to eight - eighty , and it 's  it 'd take us , 
A:  Yeah . 
F:  tops uh thirty minutes to get there . 
A:  Oh , I  
F:  So that leaves us fifty minutes before the 
F:  plane  it 'll just  yeah . 
F:  So 
F:  Great , OK so that 'll 
F:  <inbreath> It 's  I mean , it 's still not going to be really easy but  well 
F:  Particularly for  for uh  
F:  for Barry and me , we 're not  we 're not staying overnight 
F:  so we don't need to bring anything particularly except for 
F:  <inbreath> uh  
F:  a pad of paper <laugh> and  
F:  <laugh> 
C:  <laugh> 
F:  So , and , uh you , two 
C:  OK . 
F:  have to bring a little bit 
F:  but uh  
F:  you know , don't  don't bring a footlocker and we 'll be OK 
F:  <inbreath> So . <laugh> 
C:  <laugh> 
C:  s 
F:  W you 're staying overnight . I figured you wouldn't need a great big suitcase , yeah . That 's sort of  <laugh> 
C:  So just  
G:  Oh yeah . Yeah . 
G:  <laugh> 
F:  one night . So . Anyway . 
C:  So , s six AM , in front . 
F:  OK . 
F:  Six AM in front . 
C:  OK . 
F:  Uh , I 'll be here . 
F:   Uh  I 'll  I 'll  I 'll  
F:  I 'll give you my phone number , 
F:  <laugh> If I 'm not here for a few m after a few minutes then  
C:  <laugh> Wake you up . <laugh> 
F:  <laugh> 
F:  Nah , I 'll be fine . I just , uh  
F:  it  for me it just means getting up a half an hour earlier 
F:  than I usually do . 
F:  Not  not  not a lot , so 
C:  OK . 
C:  Wednesday .  
F:  <inbreath> 




F:  OK , that was the real real important stuff . Um , 
F:  I  I  I figured maybe wait on the 
F:  potential goals for the meeting 
F:  uh  until 
F:  we talk about wh what 's been going on . 
F:  So , 
F:  uh , what 's been going on ? Why don't we start  start over here . 
G:  Um . 
G:  <mouth> 
G:  Well , preparation of the French test data actually .  
F:  OK . 
G:  So , 
G:  <inbreath> 
G:  it means that um , 
G:  <inbreath> 
G:  well , it is , uh , a digit French database of microphone speech , 
G:  downsampled to eight kilohertz and 
G:  I 've added noise to one part , 
G:  with the  actually the 
G:  Aurora - two noises .  
G:  And , @ @ 
G:  so this is a training part . 
G:  And then  the remaining part , I use for testing and  
G:  with other kind of noises . So we can  
G:  So this is almost ready . I 'm preparing the  the HTK baseline for this task . 
G:  And , yeah . 
F:  OK 
F:  Uh , 
F:  So the HTK base lines  
F:  so this is using mel cepstra and so on , or  ? 
G:  Yeah . 
F:  Yeah . 
F:  OK . 


F:  And again , I guess the p the plan is , uh , 
F:  to uh  
F:  then given this  
F:  What 's the plan again ? <laugh> 
G:  The plan with  these data ? 
F:  With  So  So  Does i Just remind me of what  what you were going to do 
F:  with the  what  what  what  what 's  y 
F:  You just described what you 've been doing . 
G:  Yeah . 
F:  So if you could remind me of what you 're going to be doing . 
F:  Oh , this is  yeah , yeah . 
G:  Uh , yeah . 
C:  @ @ Tell him about the cube . 
G:  Well . 
G:  The cube ? 
G:  I should tell him about the cube ?  Yeah . 
E:  <laugh> 
C:  Yeah . 
F:  Oh ! Cube . Yeah . 
G:  Uh we  actually we want to , mmm , 
E:  Fill in the cube .  
G:  Uh , <laugh> uh , 
E:  <laugh> 
G:  analyze three dimensions ,  
G:  the feature dimension , the  training data dimension , and the test data dimension . 
G:  Um . 
G:  <mouth> Well , what we want to do is 
G:  first we have number for each  uh task . 
G:  So we have the um , 
G:  TI - digit task , the Italian task , the French task  and the Finnish task .  
F:  Yeah ? 
G:  So we have numbers with  uh  systems  
G:  I mean  I mean 
G:  neural networks trained on the task data .  
G:  And then to have systems with neural networks trained on , 
G:  <mouth> uh , 
G:  data from the same language , if possible , with , 
G:  well , using a more generic database , which is phonetically  phonetically balanced , and . 
G:  Um . 
G:  Yeah . So . 
F:  OK , so anyway , th the basic plan is to , uh , test this cube . 
F:  @ @ 
G:  Yeah . <laugh> 
F:  Yes . <laugh> To fill i fill it in , yeah . <laugh> 
E:  To fill in the cube .  
G:  <laugh> Uh . 
F:  <inbreath> OK . 
G:  Yeah , and perhaps , um   
G:  We were thinking that perhaps the cross - language issue is not , uh , so big of a issue . Well , w w we  perhaps we should not focus too much on that cross - language stuff . 
G:  I mean , uh , training  training a net on a language and testing a for another language . 
F:  Uh - huh . 
G:  Mmm . 
F:  But that 's  


F:  So - so we had talked  I guess we had talked at one point about 
F:  maybe , the language ID corpus ? 
F:  Is that a possibility for that ? 
G:  Ye - uh   
G:  Yeah , but , uh these corpus , w w there is a CallHome and a CallFriend also , 
G:  The CallFriend is for language ind identification . Well , 
G:  anyway , these corpus are all telephone speech . So , um . 
G:  <inbreath> 
G:  This could be a   a problem for  
G:  Why ? Because uh , uh , 
G:  the  the SpeechDat databases are not telephone speech . 
G:  They are downsampled to eight kilohertz but  
G:  but 
G:  they are not <inbreath> uh with telephone bandwidth . 
F:  Yeah . 
F:  That 's really funny isn't it ? 
F:  I mean cuz th this whole thing is for  developing new standards for the telephone . 
C:  Telephone .  <laugh> 
F:  Yeah . 
G:  Yeah , but the  the idea is to compute the feature before  the  
G:  before sending them to the  
G:  Well ,  you don't  do not send speech , you send features , computed on th the   the device , or  Well . 
F:  Mm - hmm . 
F:  Yeah , I know , but the reason  
F:  Oh I see , so your point is that it 's  it 's  
G:  So you  
F:  it 's uh  
F:  the features are computed locally , and so they aren't necessarily telephone bandwidth , uh 
G:  Yeah . 
G:  Yeah . 
F:  or telephone 
A:  Did you  happen to find out anything about the OGI multilingual database ? 
F:  distortions . 
G:  Yeah , it 's  
F:  Yeah , that 's wh that 's wh that 's what I meant . 
F:  I said  @ @ , there 's  there 's  there 's an OGI language ID , not the  not the , uh  
F:  the CallFriend is a  is a , uh , LDC w thing , right ? 
G:  Yea - 
G:  Yeah , there are also two other databases .  
G:  One they call the multi - language database , 
G:  and another one is a twenty - two language , something like that . 
G:  But it 's also telephone speech . 
A:  Oh , they are ? 
A:  OK . 
G:  Uh . 
G:  Well , 
G:  nnn . 
F:  But I 'm not sure  
G:  So  
F:  I mean , we ' r e e The bandwidth shouldn't be such an issue right ? Because 
F:  e e this is downsampled and  and filtered , right ? 
F:  So it 's just the fact that it 's not telephone . 
F:  And there are so many other differences between these different databases . I mean some of this stuff 's recorded in the car , and some of it 's  I mean there 's  there 's many different acoustic differences . So I 'm not sure 
G:  Yeah . 
F:  if  . 
F:  I mean , unless we 're going to include a bunch of car recordings in the  in the training database , 
F:  I 'm not sure if it 's  
F:  completely rules it out 
F:  if our  if we  if our major goal is to have phonetic context 
G:  Mmm . 
F:  and you figure that there 's gonna be a mismatch in acoustic conditions 
F:  does it make it much worse f to 
F:  sort of add another mismatch , if you will . 
F:  Uh , i i I  I guess the question is how important is it to  for us to get multiple languages 
F:  uh , in there . 
G:  Yeah , but  
G:  Mm - hmm . <inbreath> 
G:  Um . 
G:  Yeah . 
G:  Well , actually , for the moment if we w do not want to use these phone databases , we  we already have uh  English , Spanish and French 
B:  <cough> 
G:  uh , with microphone speech . 
F:  Mm - hmm . 
F:  Yeah . 
G:  So . 
F:  So that 's what you 're thinking of using is sort of the multi the equivalent of the multiple ? 
G:  Well . 
G:  Yeah , for the multilingual part we were thinking of using these three databases . 
F:  And for the difference in phonetic context  that you  ? 
G:  Well , this  
G:  Uh , actually , these three databases are um generic databases . 
F:  Provide that .  
E:  Yeah . 
G:  So w 
G:  f for  for uh Italian , 
G:  which is close to Spanish , French and , i i uh , TI - digits we have both 
G:  uh , digits  training data and also  more general training data . So . 
G:  Mmm . 
F:  Well , we also have this Broadcast News that we were talking about taking off the disk , which is  <laugh> is microphone data for  for English . 
G:  Yeah . 
G:  Yeah , perhaps  yeah , there is also TIMIT . We could use TIMIT . 
F:  Yeah . 
F:  Right . 
G:  Um . 
F:  Yeah , so there 's plenty of stuff around . 


G:  Perhaps the most important is to have neural networks trained on the target languages . But , uh , with a general database  general databases . 
F:  <squeaks> 
G:  u 
G:  So that th Well , 
G:  the  the guy who has to develop an application with one language can use the net trained o on that language , 
G:  or a generic net , but not trained on a  
F:  Uh , depen it depen it depends how you mean " using the net " . So , if you 're talking about for producing these discriminative features  that we 're talking about  you can't do that . 
G:  <clears throat> 
G:  Mmm . 
F:  Because  because 
F:  the  what they 're asking for is  is a feature set . 
G:  <inbreath> 
F:  Right ? 
F:  And so , 
F:  uh , we 're the ones who have been weird by  by doing this training . 
G:  Yeah . 
F:  But if we say , " No , you have to have a different feature set for each language , " I think this is ver gonna be very bad . 
C:  Oh . 
F:  So  Oh yeah . Yeah . I mean , 
G:  You think so . Mmm . 
E:  Oh . 
C:  That 's  
F:  in principle , I mean conceptually , it 's sort of like they want a re @ @  
G:  Mmm . 
F:  well , they want a replacement for mel cepstra . 
F:  So , we say " OK , this is the year two thousand , we 've got something much better than mel cepstra . It 's , you know , gobbledy - gook . " OK ? And so 
C:  <laugh> 
F:  <inbreath> we give them these gobbledy - gook features but these gobbledy - gook features are supposed to be good for any language . 
G:  Hmm . 
F:  Cuz you don't know who 's gonna call , and you know , I mean so it 's  it 's  it 's , uh , uh  how do you know what language it is ? 
F:  Somebody picks up the phone . 
F:  So thi this is their image . 
G:  Well , I  chh   
F:  Someone picks up the phone , right ? 
F:  And  and he  he picks up the ph 
G:  Yeah , but the  the application is  there is a target language for the application . So , if a  
F:  Yeah . y y y  
F:  Well . 
G:  Well . 
F:  But , no but , y you  you pick up the phone , 
G:  Yeah ? 
F:  you talk on the phone , and it sends features out . 
F:  OK , so the phone doesn't know 
F:  what a  what  what your language is . 
G:  Yeah , if  Yeah . 
G:  If it 's th in the phone , but  well , it  that  that could be th at the server 's side , and , well . 
F:  But that 's the image that they have . 
F:  It could be , but that 's the image they have , right ? 
G:  Mmm , yeah . 
F:  So that 's  that 's  I mean , one could argue all over the place about how things really will be in ten years . 
F:  But the particular image that the cellular industry has right now is that it 's distributed speech recognition , 
F:  where the , uh , uh , probabilistic part , and  and s semantics and so forth are all on the servers , 
F:  and you compute features of the  uh , on the phone . 
F:  So that 's  that 's what we 're involved in . We might  might or might not agree that that 's the way it will be in ten years , but 
F:  that 's  that 's  that 's what they 're asking for . 
F:  So  so I think 
F:  that  th th it is an important issue whether it works cross - language . 
F:  Now , it 's the OGI , uh , folks ' perspective right now that probably that 's not the biggest deal . 
F:  And that the biggest deal is the , um 
F:  envir acoustic - environment mismatch . 
F:  And they may very well be right , but I  I was hoping we could just do a test and determine if that was true . 
F:  If that 's true , we don't need to worry so much . Maybe  maybe we have a couple languages in the training set and that gives us enough breadth 
F:  uh , uh , that  that  that the rest doesn't matter . 
F:  Um , the other thing is , uh , 
F:  this notion of training to uh  
F:  which I  I guess they 're starting to look at up there ,  
F:  training to something more like articulatory features . 
F:  Uh , and if you have something that 's just good for distinguishing different articulatory features that should just be good across , you know , a wide range of languages . 
G:  Mmm . 
G:  Yeah . Yeah . 
F:  Uh , but  
F:  Yeah , so I don't th I know  unfortunately I don't  I see what you 're comi where you 're coming from , I think , but I don't think we can ignore it . 
G:  So we  we really have to do test with a real cross - language . I mean , tr for instance training on English and testing on Italian , or  
G:  Or we can train  
G:  or else , uh , can we train a net on , uh , a range of languages and  
C:  Test on an unseen . 
G:  which can include the test  the test @ @ the target language , or  
F:  Yeah , so , um , 
F:  there 's  there 's , uh  This is complex . So , ultimately , 
F:  uh , as I was saying , I think it doesn't fit within their image that you switch nets based on language . 
G:  <mouth> Yeah . 
F:  Now , 
F:  can you include , uh , the  the target language ? 
F:  Um , 
F:  from a purist 's standpoint it 'd be nice not to 
F:  because then you can say when  
F:  because surely someone is going to say at some point , " OK , so you put in the German and the Finnish . Uh , now , what do you do , uh , when somebody has Portuguese ? " you know ? 
G:  Mmm . 
F:  Um , and  
F:  Uh , however , 
F:  you aren't  it isn't actually a constraint 
F:  in this evaluation . 
F:  So I would say 
C:  <laugh> 
F:  if it looks like there 's a big difference to put it in , 
F:  then we 'd make note of it , and then we probably put in the other , because we have so many other problems in trying to get things to work well here that  
G:  Mmm ? 
F:  that , you know , it 's not so bad as long as we  we note it and say , " Look , we did do this " . 
A:  And so , ideally , what you 'd wanna do is you 'd wanna 
F:  Uh . 
A:  run it with and without the target language and the training set for a wide range of languages . 
F:  Yeah . 
G:  Yeah , perhaps . Yeah . 
G:  Yeah . 
A:  And that way you can say , " Well , " 
F:  Yeah . 
A:  you know , 
A:  " we 're gonna build it for what we think are  the most common ones " , but if 
A:  that  somebody uses it with a different language , 
A:  you know , " here 's what 's you 're l here 's what 's likely to happen . " 
F:  Yeah , cuz the truth is , is that it 's  it 's not like there are  I mean , al although there are thousands of languages , 
F:  uh , from uh , uh , the point of view of cellular companies , there aren't . There 's  <laugh> you know , there 's fifty or something , you know ? So , 
A:  Right . 
A:  <laugh> 
F:  uh , an and they aren't  
F:  you know , 
F:  with the exception of Finnish , which I guess it 's pretty different from most  most things . 
G:  <laugh> 
F:  uh , it 's  it 's , uh  most of them are 
F:  like at least some of the others . 
F:  And so , our guess that Spanish is like Italian , and  
F:  and so on . 
F:  I guess Finnish is a  is  is a little bit like Hungarian , supposedly , right ? 
F:  Or is  I think  well , I kn oh , well I know that 
A:  I don't know anything about Finnish . 
F:  H uh , H I mean , I 'm not a linguist , but I guess Hungarian and Finnish and one of the  one of the languages from the former Soviet Union 
F:  are in this sort of same family . 
F:  But they 're just these , you know , 
A:  Hmm . 
F:  uh  countries that are pretty far apart from one another , have  
A:  Hmm . 
G:  Mmm .  
F:  I guess , 
F:  people rode in on horses and 
A:  <laugh> 
F:  brought their  
F:  OK . 


G:  The  Yeah . 
F:  Your turn . 
C:  Oh , my turn . Oh , OK . Um , 
F:  <laugh> 
C:  Let 's see , I  I spent the last week , uh , looking over Stephane 's shoulder . 
G:  <mike noise> 
F:  <laugh> 
C:  And  <laugh> and understanding some of the data . 
G:  <laugh> 
C:  I re - installed , um , 
C:  um , HTK , the free version , 
C:  so , um , everybody 's now using three point O , which is the same version that , uh , OGI is using . 
F:  Oh , good . 
C:  Yeah . So , without  without any licensing big deals , or anything like that . 
G:  <laugh> 
C:  And , um , so we 've been talking about this  this , uh , cube thing , 
C:  and it 's beginning more and more looking like the , uh , the Borge cube thing . 
C:  It 's really gargantuan . 
A:  <laugh> 
C:  Um , <mouth> but I 
G:  <laugh> 
C:  I 'm  
F:  So are  are you going to be assimilated ? 
A:  <laugh> 
C:  Am I  <laugh> 
F:  <laugh> 
A:  <laugh> Resistance is futile . <laugh> 
C:  Exactly . <laugh> 
C:  Um , yeah , so I I 've been looking at , uh , uh , TIMIT stuff .  
G:  <clears throat> 
C:  Um , the  the stuff that we 've been working on with TIMIT , trying to get a , 
C:  um  a labels file so we can , uh , train up a  train up a net on TIMIT and test , um , the difference between this net trained on TIMIT and a net trained on digits alone . 
F:  Mm - hmm . 
C:  Um , and seeing if  if it hurts or helps . 
F:  And again , when y just to clarify , when you 're talking about training up a net , you 're talking about training up a net for a tandem 
C:  Anyway . 
C:  Yeah , yeah . Um . 
F:  approach ? And  and the inputs are PLP and delta and that sort of thing , or  ?  
C:  Mm - hmm . 
C:  Well , the inputs are one dimension of the cube , which , um , we 've talked about it being , uh , PLP , um , M F C Cs , um , J - JRASTA , JRASTA - LDA   
G:  Hmm .  
F:  Yeah , but your initial things you 're making one choice there , right ? Which is PLP , or something ? Yeah . 
C:  Yeah , right . 
C:  Um , I  I haven't  I haven't decided on  on the initial thing . Probably  probably something like PLP . 
F:  Yeah . 
C:  Yeah . 
G:  Hmm . 
F:  Um , so  so you take PLP and you  you , uh , do it  uh , you  you , uh , use HTK with it with the transformed features using a neural net that 's trained . 
F:  And the training could either be from Digits itself or from TIMIT . And that 's the  and , and th and then the testing would be these other things which  which  which might be foreign language . 
C:  Right . 
C:  Right . 
F:  I see . 
C:  Right . 
F:  I  I  I get in the picture about the cube . OK . 
C:  Yeah . 
C:  Maybe  OK .  
G:  <laugh> 
F:  OK .  
C:  Uh - huh . 
F:  Um , I mean , those listening to this will not have a picture either , so , um , I guess I 'm  I 'm not any worse off . 
C:  <inbreath>  
F:  But <laugh> but at some point  somebody should just show me the cube . It sounds s I  I get  I think I get the general idea of it , yeah . 
C:  <laugh> 
C:  Yeah , yeah , b 
G:  <mike noise> 


A:  So , when you said that you were getting the labels for TIMIT ,  
C:  May - 
C:  Mm - hmm . 
A:  um , are y what do you mean by that ? 
C:  Oh , I 'm just  I 'm just , uh , transforming them from the , um , the standard TIMIT transcriptions into  into a nice long huge P - file to do training . 
A:  Mmm . 
A:  Were the digits , um , hand - labeled for phones ? 
C:  Um , the  the digits   
A:  Or were they  those labels automatically derived ? 
C:  Oh yeah , those were  those were automatically derived by  by Dan 
A:  Mmm . 
C:  using , um , embedded  embedded training and alignment . 
F:  Ah , but which Dan ? 
C:  Uh , Ellis . Right ? 
F:  OK . 
F:  OK . 
G:  <laugh> 
F:  <laugh> 
C:  Yeah . 
C:  So . 
C:  <laugh> 
A:  I was just wondering because that test you 're t I  I think you 're doing this test because you want to determine whether or not , 
C:  Uh - huh . 
A:  uh , having s general speech performs as well as having specific  speech . 
C:  That 's right . 
F:  Well , especially when you go over the different languages again , because you 'd  the different languages have different words for the different digits , so it 's  
A:  Mm - hmm . And I was  yeah , so I was just wondering if the fact that TIMIT  you 're using the hand - labeled stuff from TIMIT might be  confuse the results that you get . 
F:  I  I think it would , but  but on the other hand it might be better . 
F:  <laugh> 
A:  Right , but if it 's better , it may be better because  it was hand - labeled . 
F:  Oh yeah , but still @ @ probably use it . 
A:  Yeah . 
A:  OK . 
F:  I mean , you know , I  I  I guess I 'm sounding cavalier , but I mean , I think 
F:  the point is you have , uh , a bunch of labels and  and they 're han hand uh  hand - marked . 
F:  Uh , I guess , actually , TIMIT was not entirely hand - marked . It was automatically first , and then hand  hand - corrected . 
F:  But  but , um , uh , it  it , um , it might be a better source . 
A:  Oh , OK . 
A:  <cough> 
F:  So , i it 's  you 're right . It would be another interesting scientific question to ask , " Is it because it 's a broad source or because it was , you know , carefully ? " uh . 
A:  Mm - hmm . 
F:  And that 's something you could ask , but given limited time , I think the main thing is if it 's a better thing for going across languages on this training tandem system , then it 's probably  
A:  Yeah . 
G:  <clears throat> 
A:  Right . 


A:  What about the differences in the phone sets ? 
C:  Uh , between languages ? 
A:  No , between TIMIT and the  the digits . 
C:  Oh , um , right . 
C:  Well , there 's a mapping from the sixty - one phonemes in TIMIT to  
E:  Sixty - one .  
C:  to fifty - six , the ICSI fifty - six .  
A:  Oh , OK . I see . 
C:  And then the digits phonemes , um , there 's about twenty twenty - two or twenty - four of them ? Is that right ? 
A:  Out of that fifty - six ? Oh , OK . 
G:  Yep . 
C:  Out of that fifty - six . 
C:  Yeah . 
C:  So , it 's  it 's definitely broader , yeah . 
G:  But , actually , the issue of phoneti phon uh phone phoneme mappings 
G:  will arise when we will do severa use several languages because 
E:  Yeah . 
G:  you  Well , 
G:  some phonemes are not , uh , in every languages , and  
G:  So we plan to develop a subset of the phonemes , 
G:  uh , that includes , uh , all the phonemes of our training languages , and 
A:  Mm - hmm . 
F:  Mm - hmm . 
G:  use a network with kind of one hundred outputs or something like that . 
F:  You mean a superset , sort of . 
G:  Uh , yeah , superset , yeah . 
F:  Yeah . <laugh> Yeah . 
E:  Yeah . 
E:  I th I looks the SAMPA SAMPA phone .  
G:  Mmm . 
G:  Yeah . 
E:  SAMPA phone ? For English  uh American English , and the  the  the language who have more phone are the English . 
A:  Mmm . 
E:  Of the  these language . 
E:  But n for example , in Spain , the Spanish have several phone that d doesn't appear in the E English and we thought to complete . 
E:  But for that , it needs  we must r h do a lot of work <laugh> because we need to generate new tran transcription for the database that we have . 
F:  Mm - hmm . 
F:  Mm - hmm . 
B:  Other than the language , is there a reason not to use the TIMIT phone set ? 
B:  Cuz it 's larger ? 
B:  As opposed to the ICSI  phone set ? 
C:  Oh , you mean why map the sixty - one to the fifty - six ? 
B:  Yeah . 
C:  I don't know . I have  
F:  Um , I forget if that happened starting with you , or was it  o or if it was Eric , afterwards who did that . But I think , 
F:  basically , there were several of the phones that were just hardly ever there . 
A:  Yeah , and I think some of them , they were making distinctions between 
A:  silence at the end and silence at the beginning , 
B:  Oh .   
C:  <laugh> 
A:  when really they 're  both silence . 
A:  I th I think it was things like that that got it mapped down to fifty - six . 
B:  OK . 
F:  Yeah , especially in a system like ours , which is a discriminative system . You know , you 're really asking this net to learn . @ @ <laugh> It 's  it 's kind of hard . So . 
B:  Yeah . 
A:  Yeah . 
A:  There 's not much difference , really . 
A:  And  the ones that are gone , I think are  
A:  I think there was  they also in TIMIT had like a glottal stop , which 
B:  Mm - hmm . 
A:  was basically a short period of silence , and 
A:  so . 
B:  Well , we have that now , too , right ? 
A:  I don't know . 
B:  Yeah . 
F:  i It 's actually pretty common that a lot of the recognition systems people use have things like  like , say thirty - nine , 
A:  So .  
F:  phone symbols , right ? Uh , and then they get the variety by  by bringing in the context , the phonetic context . Uh . 
F:  So we actually have an unusually large number in  in what we tend to use here . 
F:  Um . 


F:  So , a a actually  maybe  now you 've got me sort of intrigued . What  there 's  
F:  Can you describe what  what 's on the cube ? I mean  <laugh> 
C:  Yeah , w I th I think that 's a good idea to  to talk about the whole cube and maybe we could sections in the cube for people to work on . 
E:  <laugh> 
F:  Yeah , yeah . 
E:  Yeah . 
E:  Yeah . 
F:  Yeah . 
F:  Yeah . 
C:  Um , 
C:  OK . 
F:  OK , so even  even though the meeting recorder doesn't  doesn't , uh  and since you 're not running a video camera we won't get this , but if you use a board it 'll help us anyway . 
C:  Uh , do you wanna do it ? 
A:  <laugh> 
C:  OK . 
F:  Uh , point out one of the limitations of this <laugh> medium , but you 've got the wireless on , right ? Yeah , so you can walk around . 
C:  OK .  
C:  Yeah , I have the wireless . 
F:  <laugh> 
C:  OK . 
C:  Can y can you walk around too ? No . <laugh> 
C:  OK , well , um , s 
F:  Uh , he can't , actually , but  <laugh> He 's tethered . 
A:  <laugh> 
C:  basically , the  the cube will have three dimensions . 
C:  The first dimension is the  the features that we 're going to use . 
C:  And the second dimension , um , is the training corpus .  
C:  And that 's the training on the discriminant neural net . 
C:  Um  and  <tapping sounds, writing on whiteboard> 
C:  the last dimension 
F:  Yeah and again  Yeah . So the  the training for HTK is always  that 's always set up for the individual test , right ? That there 's some training data and some test data . So that 's different than this . Yeah . 
C:  happens to be  
C:  Right , right . This is  this is for  
C:  for ANN only . 
C:  And , yeah , the training for the HTK models is always , uh , fixed for whatever language you 're testing on . 
F:  Right . 
F:  <clunk noise on table> 
C:  And then , there 's the testing corpus . 
C:  <tapping sounds, writing on whiteboard> 
C:  <swallow> <mouth> <inbreath> 


C:  So , then I think it 's probably instructive to go and  and  and show you the features that we were talking about . 
C:  Um , so , let 's see . Help me out with  With what ? 
G:  @ @ PLP .  
G:  PLP . 
C:  PLP ? OK . 
C:  <tapping sounds, writing on whiteboard> 
G:  MSG .  
C:  MSG .  
C:  <tapping sounds, writing on whiteboard> 
G:  Uh , JRASTA .  
C:  JRASTA . <tapping sounds, writing on whiteboard> 
G:  And JRASTA - LDA .  
C:  JRASTA - LDA . 
C:  <tapping sounds, writing on whiteboard> 
G:  Um , 
G:  multi - band .  
C:  Multi - band . 
C:  <tapping sounds, writing on whiteboard> 
G:  So there would be multi - band before , um  before our network , I mean . 
C:  Yeah , just the multi - band features , right ? 
G:  And  
G:  Yeah . 
C:  Yeah . 
F:  Uh - huh . Ah . Ah . 
G:  So , something like , uh , s TCT within bands and  Well . 
G:  And then multi - band after networks . Meaning that we would have , uh , neural networks , uh , discriminant neural networks for each band . 
G:  Uh , yeah .  
G:  And using the  the outputs of these networks or the linear outputs or something like that . 
G:  Uh , yeah . 
A:  What about mel cepstrum ? 
C:  Oh , um  
A:  Or is that  you don't include that because it 's part of the base or something ? 
F:  Well , y you do have a baseline 
E:  Yeah databases . Yeah . 
F:  system that 's m that 's mel cepstra , right ? So . 
E:  Mm - hmm . 
G:  But , uh , well , not for the  the ANN . I mean  
F:  OK . 
G:  So , yeah , we could  we could add  MFCC also . 
C:  We could add  <laugh> 
F:  Probably should . I mean at least  at least conceptually , you know , it doesn't meant you actually have to do it , but conceptually it makes sense as a  as a base line . 
G:  Yeah . 
E:  Yeah . 
A:  It 'd be an interesting test just to have  
A:  just to do MFCC with the neural net and everything else the same . Compare that with just M - MFCC without the  the net . 
E:  Without the   Yeah . 
G:  Yeah . 
E:  Mm - hmm . 
C:  I think  I think Dan did some of that . 
A:  Oh . 
C:  Um , in his previous Aurora experiments . And with the net it 's  it 's wonderful . Without the net it 's just baseline . 
A:  <laugh> 
A:  <laugh> 
F:  Um , I think OGI folks have been doing that , too . D Because I think that for a bunch of their experiments they used , uh , mel cepstra , actually . 
C:  Yeah . 
C:  Yeah . 
F:  Um , 
F:  of course that 's there and this is here and so on . OK ? 


C:  OK . 
B:  <clears throat> 
C:  Um , for the training corpus  corpus , um , we have , 
C:  um , the  the d  digits 
C:  <tapping sounds, writing on whiteboard> 
C:  from the various languages . 
C:  Um , English  Spanish  um , French   
C:  What else do we have ? <breath> 
G:  And the  Finnish . 
G:  <laugh> 
C:  Finnish . 
E:  And Italian . Uh , no , Italian no . Italian no . I Italian yes . Italian ? 
A:  Where did th where did that come from ? Digits ? Oh . 
C:  Oh . 
C:  Italian . 
F:  Italian . 
A:  Is that  Was that distributed with Aurora , or  ?  Where did that  ? 
C:  One L or two L 's ? 
F:  The newer one . Yeah . 
G:  So English , uh , Finnish and Italian are Aurora . 
G:  And Spanish and French is something that we can use in addition to Aurora . 
G:  Uh , well . 
F:  Yeah , so Carmen brought the Spanish , and Stephane brought the French . 
C:  OK . 
C:  And , um , <mouth> <inbreath> oh yeah , and  
F:  Is it French French or Belgian French ? There 's a  
G:  <laugh> It 's , uh , French French . <laugh> 
E:  <laugh> 
F:  <laugh> 
C:  <laugh> 
B:  <laugh> 
C:  French French .  
E:  Like Mexican Spain and Spain . <laugh> 
F:  Yeah . <laugh> 
B:  <laugh> 
B:  Or Swiss . Swiss - German . 
E:  I think that is more important , Mexican Spain . Because more people  Yeah . <laugh> 
F:  Yeah . Yeah , probably so . Yeah . 
F:  Yeah , Herve always insists that Belgian is  i is absolutely pure French , has nothing to do with  but he says those  those  those Parisians talk funny . 
F:  <laugh> 
G:  Yeah , yeah , yeah . <laugh> 
G:  They have an accent . <laugh> 
F:  Yeah they  they do , yeah . 
F:  Yeah .  
F:  But then he likes Belgian fries too , so .  OK . 
C:  And then we have , uh , 
C:  um , 
C:  broader  
C:  broader corpus , 
C:  um , like TIMIT . <outbreath> 
C:  TIMIT so far , right ? 
E:  And Spanish too . 
C:  Spanish  
C:  Oh , Spanish stories ? 
E:  Albayzin is the name . 
A:  What about TI - digits ? 
C:  Um , TI - digits  uh all these Aurora f d data p 
A:  Uh - huh . 
C:  data is from  is derived from TI - digits . 
A:  Oh . Oh OK . 
C:  Um , basically , they  they corrupted it with , uh , different kinds of noises at different SNR levels . 
A:  Ah . 
A:  I see . 
C:  Yeah . 
F:  y And I think Stephane was saying there 's  there 's some broader s material in the French also ? 
G:  Yeah , we cou we could use  Yeah . 
C:  <mouth> OK . 
G:  The French data .  
E:  Spanish stories ? No . 
C:  No . Sp - Not Spanish stories ? 
E:  No . 
F:  Spanish  
E:  No . 
C:  Spanish something . <laugh> OK . 
E:  Albayz - 
E:  Yeah . 
F:  <laugh> 
C:  <laugh> 
B:  Did the Aurora people actually corrupt it themselves , or just specify the signal and the signal - t 0 - noise ratio ? 
C:  They  they corrupted it , um , themselves , but they also included the  the noise files for us , right ? 
B:  OK . 
C:  Or  so we can go ahead and corrupt other things . 
G:  Yeah . 
F:  <inbreath> 
F:  I 'm just curious , Carmen  I mean , I couldn't tell if you were joking or  
F:  i Is it  is it Mexican Spanish , or is it  
E:  No no no no . No no no no . Spanish from Spain . 
F:  Oh , no , no . It 's  it 's Spanish from Spain , Spanish . Yeah , OK . 
B:  <laugh> 
C:  <laugh> From Spain . 
E:  <mike noises> 
F:  Alright . 
F:  Spanish from Spain . Yeah , we 're really covered there now . OK . <laugh> 
C:  <laugh> 
E:  <laugh> 
F:  And the French from France . 
C:  OK . 
G:  Yeah , the  No , the French is f yeah , from , uh , Paris , also . 
C:  <snaps top on or off of dry marker> 
F:  <laugh> 
G:  OK . 
C:  Oh , from Paris , OK . 
G:  <laugh> 
F:  Yeah . 
C:  And TIMIT 's from  lots of different places . <laugh> Yeah .  Yeah . 
F:  From TI . From  i It 's from Texas . So may maybe it 's  <laugh> 
B:  From the deep South .  
F:  So - s so it 's not really from the US either . Is that  ? OK . 
C:  OK . And , um , with within the training corporas um , we 're , uh , thinking about , um , training with noise . 
C:  So , incorporating the same kinds of noises that , um , Aurora is in incorporating in their , um  in their training corpus . 
C:  Um , I don't think we we 're given the , uh  the unseen noise conditions , though , right ? 
F:  I think what they were saying was that , um , for this next test there 's gonna be some of the cases where they have the same type of noise as you were given before hand and some cases where you 're not . 
C:  Like  
C:  Mm - hmm . 
G:  Mm - hmm . 
C:  OK . 
F:  So , presumably , that 'll be part of the topic of analysis of the  the test results , is how well you do when it 's matching noise and how well you do where it 's not . 
C:  Right . 
F:  I think that 's right . 
C:  So , I guess we can't train on  on the  the unseen noise conditions . 
F:  Well , not if it 's not seen , yeah . <laugh> 
C:  Right . If  Not if it 's unseen . Yeah . 
F:  OK . 
F:  I mean , i i i i it does seem to me that a lot of times when you train with something that 's at least a little bit noisy it can  it can help you out in other kinds of noise even if it 's not matching just because 
F:  there 's some more variance that you 've built into things . 
F:  But , but , uh , uh , exactly how well it will work will 
G:  Mm - hmm . 
F:  depend on how near it is to what you had ahead of time . So . 
G:  Mm - hmm . 


F:  OK , so that 's your training corpus , and then your testing corpus  ?  
C:  Um , the testing corporas are , um , just , um , the same ones as Aurora testing . 
C:  <tapping sounds, writing on whiteboard> 
C:  And , that includes , um , the English 
E:  Finnish . 
C:  Spa - um , Italian . 
C:  Finnish . 
C:  Uh , we ' r we 're gonna get German , right ? 
C:  Ge -  
F:  Well , so , yeah , the final test , on a guess , is supposed to be German and Danish , right ? 
C:  At the final test will have German . 
G:  Uh , yeah . 
C:  Right . 
C:  <snaps top on or off of dry marker> 
G:  The s yeah , the Spanish , perhaps , 
C:  Spanish . Oh yeah , we can  we can test on s Spanish . 
G:  we will have . Yeah . 
G:  But the  the Aurora Spanish , I mean . 
C:  Oh yeah . Mm - hmm . 
F:  Oh , there 's a  there 's Spanish testing in the Aurora ? 
G:  Uh , not yet , but , uh , yeah , 
E:  Yeah , it 's preparing . 
G:  uh , e pre they are preparing it , and , well , according to Hynek it will be  we will have this at the end of November , or  
E:  They are preparing . 
G:  Um . 
F:  OK , so , uh , something like seven things in each , uh  each column . 
G:  Yeah  
F:  So that 's , uh , three hundred and forty - three , uh , <laugh> different systems that are going to be developed . 
C:  <snaps top on or off of dry marker> <laugh> 
G:  <laugh> 
F:  There 's three of you . 
G:  <laugh> 
F:  Uh , so that 's  hundred and  <laugh> hundred and fourteen each .  What ? <laugh> 
C:  Yeah . One hundred each , about .  <laugh> 
B:  <laugh> 
D:  What a what about noise conditions ? 
G:  <laugh> 
D:  w 
D:  Don't we need to put in the column for noise conditions ? 
F:  Are you just trying to be difficult ? <laugh> 
C:  <laugh> 
D:  No , I just don't understand . <laugh> 
G:  <laugh> 
C:  Well , th uh , when  when I put these testings on there , I 'm assumi 
F:  I 'm just kidding . Yeah . 
C:  There - there 's three  three tests . Um , type - A , type - B , and type - C .  
C:  And they 're all  they 're all gonna be test tested , um , with one training of the HTK system . 
C:  Um , there 's a script that tests all three different types of noise conditions . Test - A is like a matched noise . Test - B is a  is a slightly mismatched . And test - C is a , um , mismatched channel . 
C:  <snaps top on or off of dry marker> 
D:  And do we do all our  training on clean data ? 
C:  Um , no , no , we 're  we 're gonna be , um , training on the noise files that we do have . 
E:  Also , we can clean that . 
G:  No . 


F:  So , um  
F:  Yeah , so I guess the question is how long does it take to do a  a training ? I mean , it 's not totally 
F:  crazy t I mean , these are  a lot of these are built - in things and we know  we have programs that compute PLP , we have MSG , we have JRA -  you know , a lot of these things 
F:  will just kind of happen , won't take uh a huge amount of development , it 's just trying it out . 
B:  <clears throat> 
F:  So , we actually can do quite a few experiments . 
F:  But how  how long does it take , do we think , for one of these 
C:  Mm - hmm . 
F:    
F:  trainings ? 
C:  That 's a good question . <laugh> 
A:  What about combinations of things ? 
F:  Oh yeah , that 's right . I mean , cuz , so , for instance , I think the major advantage of MSG  Yeah , good point . 
C:  Oh ! 
C:  <laugh> 
C:  Och !  
C:  <laugh> 
F:  A major advantage of MSG , I see , th that we 've seen in the past is combined with PLP . 
E:  Yeah . 
F:  Um . 
F:  <laugh> 
C:  Now , this is turning into a four - dimensional cube ? <laugh> 
B:  Or you just add it to the features . 
E:  No . Here . 
A:  Well , you just select multiple things on the one dimension . 
C:  Just  
C:  Oh , yeah . OK . 
F:  Yeah , so , I mean ,  you don't wanna , uh  
F:  <page turning> 
F:  Let 's see , seven choose two would  <laugh>  be , uh , twenty - one different combinations . Um . 
F:  Probably  
B:  It 's not a complete set of combinations , though , right ? 
F:  What ? 
B:  It 's not a complete set of combinations , though , right ? 
C:  No . 
F:  Yeah , I hope not . 
F:  Yeah , there 's  
F:  <laugh> 
C:  That would be  
F:  Uh , yeah , so PLP and MSG I think we definitely wanna try cuz we 've had a lot of good experience with putting those together . 
E:  Mm - hmm . 
F:  Um . 
F:  <breath> 
F:  Yeah . 
A:  When you do that , you 're increasing the size of the inputs to the net . Do you have to reduce the hidden layer , or something ? 
F:  Well , so  I mean , so i it doesn't increase the number of trainings . 
A:  No , no , I 'm  I 'm just wondering about number of parameters in the net . Do you have to worry about keeping that the same , or  ?  
F:  But  
F:  Uh , I don't think so . 
B:  There 's a computation limit , though , isn't there ? 
F:  Yeah , I mean , it 's just more compu 
F:  Excuse me ? 
B:  Isn't there like a limit  on the computation load , or d latency , or something like that for Aurora task ? 
F:  Oh yeah , we haven't talked about any of that at all , have we ?  
C:  No . 
F:  Yeah , so , there 's not really a limit . 
F:  What it is is that there 's  there 's , uh  it 's just penalty , you know ? 
F:  That  that if you 're using , uh , a megabyte , then they 'll say that 's very nice , but , of course , it will never go on a cheap cell phone . 
F:  <laugh> 
F:  Um . 
B:  OK . 
F:  And , u uh , I think the computation isn't so much of a problem . 
F:  I think it 's more the memory . 
F:  Uh , and , expensive cell phones , exa expensive hand - helds , and so forth , are gonna have lots of memory . 
F:  So it 's just that , uh , these people see the  the cheap cell phones as being still the biggest market , so . 
B:  Mm - hmm . 
F:  Um . 
F:  But , yeah , I was just realizing that , actually , it doesn't explode out , 
F:  um  
F:  It 's not really two to the seventh . 
F:  But it 's  but  but  i i it doesn't really explode out the number of trainings cuz these were all trained individually . 
F:  Right ? 
F:  So , uh , if you have all of these nets trained some place , then , uh , you can 
F:  combine 
F:  their outputs and do the KL transformation and so forth and  
C:  Mm - hmm . 
F:  and , uh  
F:  So , what it  it blows out is the number of uh testings . 
F:  And , you know  and the number of times you do that last part . But that last part , I think , is so  has gotta be pretty quick , so . 
F:  Uh . 
F:  Right ? I mean , it 's just 
F:  running the data through  
C:  Oh . 
F:  Well , you gotta do the KL transformation , but  
A:  But wh what about a net that 's trained on multiple languages , though ? 
G:  Eight  y 
A:  Is that just separate nets for each language then combined , or is that actually one net trained on ? 
E:  Necessary to put in .  
G:  Uh , probably one net . Well . 
F:  Good question . 
G:  Uh . 
F:  One would think one net , 
G:  So . 
F:  but we 've  I don't think we 've tested that . Right ? 
G:  So , in the broader training corpus we can  
G:  we can use , uh , the three , or , a combination of  of two  two languages . 
E:  Database three . 
A:  In one net . 
G:  Yeah . 
A:  Mm - hmm . 
F:  Yeah , so , I guess the first thing is if w if we know how much a  how long a  a training takes ,  
F:  if we can train up all these  these combinations , 
F:  uh , then we can start working on testing of them individually , and in combination . Right ? 
F:  Because the putting them in combination , I think , is not as much computationally as the r training of the nets in the first place . 
C:  Mm - hmm . 
F:  Right ? 
G:  Yeah . 
F:  So y you do have to compute the KL transformation . 
F:  Uh , which is a little bit , but it 's not too much . 
G:  It 's not too much , no . 
F:  Yeah . 
F:  So it 's  
G:  But  Yeah . 
G:  But there is the testing also , which implies training , uh , the HTK models and , well , 
E:  The  the model  the HTK model . 
F:  Uh , right . 
G:  it 's  yeah . But it 's  it 's  it 's not so long . It @ @  Yeah . 
F:  Right . So if you do have lots of combinations , it 's  
F:  How long does it take for an , uh , HTK training ? 
G:  It 's around six hours , I think . 
E:  It depends on the  
G:  For training and testing , yeah . 
E:  More than six hours . For the Italian , yes . 
G:  More . 
E:  Maybe one day . <laugh> 
G:  One day ? 
F:  For HTK ? Really ? 
E:  Yeah . 
E:  Well . 
F:  Running on what ? 
E:  Uh , M  MFCC . 
F:  No , I 'm sorry , ru running on what machine ? 
E:  Uh , Ravioli . 
F:  Uh , I don't know what Ravioli is . Is it  is it an Ultra - five , or is it a  ?  
E:  mmm 
E:  Um . 
C:  I don't know .  
A:  I don't know . 
E:  Who is that ? I don't know . I don't know .  
B:  <laugh> I don't know what a Ravioli is . 
C:  <laugh> 
B:  <laugh> 
C:  I don't know .  
C:  <laugh> 
B:  We can check really quickly , I guess . 
G:  Yeah , I I think it 's - it 's - it 's not so long because , well , the TI - digits test data is about , uh 
G:  how many hours ? 
G:  Uh , th uh , thirty hours of speech , I think , something like that . 
F:  It 's a few hours . 
B:  Hmm . 
F:  Yeah . 
F:  <inbreath> 
G:  And it p Well . 
F:  Right , so , I mean , clearly , there  there 's no way we can even begin to do an any significant amount here unless we use multiple machines . 
G:  It 's six hours . 
F:  Right ? So  so  w we  I mean there 's plenty of machines here and they 're n they 're often not in  in a great  great deal of use . So , I mean , I think it 's  
F:  it 's key that  that the  that you look at , uh , 
F:  you know , what machines are fast , what machines are used a lot   
F:  Uh , are we still using P - make ? Is that  ? 
C:  Oh , I don't know how w how we would P - make this , though . 
C:  Um . 
F:  Well , you have a  
F:  I mean , once you get the basic thing set up , you have just all the  uh , a all these combinations , right ? 
C:  Yeah . 
C:  Mm - hmm . 
F:  Um . 
F:  It 's  it 's  let 's say it 's six hours or eight hours , or something for the training of HTK . How long is it for training of  of , uh , the neural net ? 
C:  The neural net ? 
C:  Um . 
G:  I would say two days . 
A:  Depends on the corpuses , right ? 
E:  It depends . 
G:  Yeah . 
B:  It s also depends on the net . 
E:  Depends on the corpus . 
C:  Yeah . 
B:  How big is the net ? 
E:  For Albayzin I trained on neural network , uh , was , um , one day also . 
F:  Uh , but on what machine ? 
C:  On a SPERT board . 
E:  Uh . 
E:  I  I think the neural net SPERT . Yes . 
C:  Y you did a  you did it on a SPERT board . 
F:  OK , again , we do have a bunch of SPERT boards . 
C:  Yeah . 
F:  And I think there  there  there 's  I think you folks are probably go the ones using them right now . 
C:  <breath-laugh> 
A:  Is it faster to do it on the SPERT , or  ?  
F:  Uh , don't know . Used to be . 
C:  It 's  it 's still a little faster on the SPERTs. Yeah , yeah . 
A:  Is it ? 
C:  Ad - Adam  Adam did some testing . Or either Adam or  or Dan did some testing and they found that the SPERT board 's still  still faster . And 
A:  Mm - hmm . 
F:  Mm - hmm . 
C:  the benefits is that , you know , you run out of SPERT and then you can do other things on your  your computer , and you don't  
A:  Mm - hmm . 
F:  Yeah . 
F:  So you could be  we have quite a few SPERT boards . You could set up , uh , 
F:  you know , ten different jobs , or something , to run on SPERT  different SPERT boards 
F:  and  and have ten other jobs running on different computers . 
F:  So , it 's got to take that sort of thing , or  or we 're not going to get through any significant number of these . 
C:  Yeah . 
F:  Um . 
F:  So this is  Yeah , I mean , I kind of like this because 
C:  <laugh> 
F:  what it  
F:  No  uh , no , what I like about it is we  we  we do have a problem that we have very limited time . 
C:  OK . 
F:  You know , so , with very limited time , we actually have 
F:  really quite a  quite a bit of computational resource available if you , you know , get a look across the institute and how little things are being used . 
F:  And uh , 
F:  on the other hand , almost anything 
F:  that really i you know , is  is new , 
F:  where we 're saying , 
E:  Yeah . 
F:  " Well , let 's look at , like we were talking before about , uh , uh , 
F:  voiced - unvoiced - silence detection features and all those sort  " 
F:  that 's  I think it 's a great thing to go to . 
F:  But if it 's new , then we have this development and  and  and learning process t to  to go through 
F:  on top of  just the  the  all the  all the work . 
F:  So , I  I  I don't see how we 'd do it . 
F:  So what I like about this is you basically have listed all the things that we already know how to do . 
F:  <laugh> 
F:  And  and all the kinds of data that we , at this point , already have . <laugh> 
E:  Yeah . 
F:  And , uh , you 're just saying let 's look at the outer product of all of these things and see if we can calculate them . 
C:  <laugh> 
F:  a a Am I  am I interpreting this correctly ? Is this sort of what  what you 're thinking of doing in the short term ? 
G:  Mmm . 
F:  OK . 
G:  Yeah . 
F:  So  so then I think it 's just the  the missing piece is that you need to , uh , you know  you know , talk to  talk to , uh , Chuck , talk to , uh , Adam , 
F:  uh , sort out about , uh , what 's the best way to really , 
F:  you know , attack this as a  as a  as a mass problem in terms of using many machines . 
F:  Uh , and uh , then , you know , set it up in terms of scripts and so forth , and  
F:  uh , in  in kind o some kind of structured way . Uh . 
F:  Um , and , you know , when we go to , uh , OGI next week , 
F:  uh , we can then present to them , you know , what it is that we 're doing . 
F:  And , uh , we can pull things out of this list that we think they are doing sufficiently , that , you know , we 're not  we won't be contributing that much . 
C:  Mmm . 
C:  Mm - hmm . 
F:  Um . 
F:  And , uh  
F:  Then , uh , like , we 're there . 
F:  Yeah ? 
B:  How big are the nets you 're using ? 
C:  Um , for the  
C:  for nets trained on digits ,  
C:  um , we have been using , uh , four hundred order hidden units .  
C:  And , um , for the broader class nets we 're  we 're going to increase that because the , um , the digits nets only correspond to about twenty phonemes . 
B:  Uh - huh . 
C:  So . 
F:  Broader class ? 
C:  Um , the broader  broader training corpus nets like TIMIT . 
C:  Um , w we 're gonna  
F:  Oh , it 's not actually broader class , it 's actually finer class , but you mean  
C:  Right . 
F:  y You mean <laugh> 
C:  Right . 
B:  <laugh> 
C:  Yeah . 
F:  more classes . <laugh> 
C:  More classes . Right , right . More classes . That 's what I mean . Mm - hmm . 
F:  Yeah . Yeah . Yeah . 
C:  And . 
C:  Yeah . 


F:  Carmen , did you  do you have something else to add ? We  you haven't talked too much , and  
E:  <laugh> D I begin to work with the Italian database 
E:  to  nnn , to  with the f front - end and with the HTK program 
E:  and the @ @ . 
E:  And I trained eh , with the Spanish two neural network with PLP and with LogRASTA PLP . 
E:  I don't know exactly what is better if  if LogRASTA or JRASTA . 
F:  Well , um , 
F:  JRASTA 
F:  has the potential to do better , but it doesn't always . 
F:  It 's  i i JRASTA is more complicated . 
F:  <clears throat> 
F:  It 's  it 's , uh  instead of doing RASTA with a log , 
F:  you 're doing RASTA with a log - like function 
F:  that varies depending on a J parameter , 
F:  uh , which is supposed to be sensitive to the amount of noise there is . 
F:  So , it 's sort of like the right transformation to do the filtering in , is dependent on how much noise there is . 
E:  Hm - hmm . 
F:  And so in JRASTA you attempt to do that . 
F:  It 's a little complicated because once you do that , you end up in some funny domain and you end up having to do a transformation afterwards , which requires some tables . 
F:  And , uh , so it 's  it 's  it 's a little messier , uh , there 's more ways that it can go wrong , uh , but if  if  if you 're careful with it , it can do better . So , it 's  So . 
E:  Hm - hmm . 
E:  It 's a bit  I 'll do better . 
E:  Um , and I think to  to  
E:  to recognize the Italian digits with the neural netw Spanish neural network , 
E:  and also to train another neural network with the Spanish digits , the database of Spanish digits . 
E:  And I working that .  
F:  Yeah . 


E:  But prepa to prepare the  the database are difficult . 
E:  Was for me , 
E:  n it was a difficult work last week 
E:  with the labels because the  the program with the label obtained that I have , the Albayzin , is different w to the label to train the neural network . 
E:  And  <mouth> that is another work that we must to do , to  to change . 
F:  I  I didn't understand . 
E:  Uh , for example Albayzin database was labeled automatically with HTK . 
E:  It 's not hand  it 's not labels by hand . Labels .  I 'm sorry , I 'm sorry . The labels . I 'm sorry . 
F:  Oh , " l labeled " . I 'm sorry , I have a p I had a problem with <laugh> the pronunciation . <laugh-breath> 
E:  The labels . 
F:  Yeah , OK . So , OK , so let 's start over . So , TI TIMI TIMIT 's hand - labeled , and  and you 're saying about the Spanish ? 
E:  Oh , also that  Yes . 
B:  <cough> 
E:  The Spanish labels ? That was in different format , that the format for the em  the program to train the neural network . 
F:  Oh , I see . 
E:  I necessary to convert . 
E:  And someti well  
A:  So you 're just having a problem converting the labels . 
E:  It 's  it 's  Yeah . 
E:  Yeah , but n yes , because they have one program , Feacalc , but no , l LabeCut , but don't  doesn't , eh , include the HTK format to convert . 
F:  Mm - hmm . 
E:  And , 
B:  Hmm . 
E:  I don't know what . I ask  e even I ask to Dan Ellis what I can do that , and h they  he say me that h he does doesn't any  any s any form to  to do that . 
E:  And at the end , I think that with LabeCut I can transfer to ASCII format , and HTK is an ASCII format . 
E:  And I m do another , uh , one program to put 
E:  ASCII format of HTK to ase ay ac ASCII format to 
F:  Mm - hmm . 
E:  Exceed and they used LabCut to  <laugh> to pass . 
E:  <laugh> 
F:  OK , yeah . 
E:  Actually that was complicated , but well , I know how we can did that  do that . 
F:  So you 
F:  Sure . So it 's just usual kind of uh  sometimes say housekeeping , right ? To get these  get these things sorted out . 
E:  Yeah . 


F:  So it seems like there 's  there 's some peculiarities of the , uh  of each of these dimensions that are getting sorted out . 
F:  And then , um , if  if you work on getting the , uh , assembly lines together , and then the  the pieces sort of get ready to go into the assembly line and gradually can start , you know , start turning the crank , more or less . 
F:  And , uh , 
F:  uh , we have a lot more computational capability here than they do at OGI , so I think that i if  
F:  What 's  what 's great about this is it sets it up in a very systematic way , so that , uh , 
F:  once these  all of these , you know , mundane but real problems get sorted out , we can just start turning the crank and  and 
E:  Mm - hmm . 
F:  push all of us through , and then finally figure out what 's best . 
C:  Yeah . 


C:  Um , I  I was thinking two things . Uh , the first thing was , um  we  we actually had thought of this as sort of like , um  not  not in stages ,  
C:  but more along the  the time axis .  
C:  Just kind of like one stream at a time , je - je - je - je - je  check out the results and  and go that way . 
F:  Mm - hmm . 
F:  Oh , yeah , yeah , sure . 
F:  No , I 'm just saying , I 'm just thinking of it like loops , right ? And so , y y y if you had three nested loops , that you have a choice for this , a choice for this , and a choice for that , right ? And you 're going through them all . That  that 's what I meant . 
C:  Uh - huh . 
C:  Yeah . Mm - hmm . 
C:  Right , right . 
F:  And , uh , the thing is that once you get a better handle on how much you can realistically do , 
F:  uh , um , <mouth> 
F:  concurrently on different machines , different SPERTs , and so forth , uh , and you see how long it takes on what machine and so forth , 
F:  you can stand back from it and say , " OK , if we look at all these combinations we 're talking about , and combinations of combinations , and so forth , " you 'll probably find you can't do it all . 
C:  Mm - hmm . 
C:  OK . 
F:  OK , so then at that point , 
C:  <laugh> 
F:  uh , we should sort out which ones do we throw away . 
F:  Which of the combinations across  you know , what are the most likely ones , and  
C:  Mm - hmm . 
F:  And , uh , I still think we could do a lot of them . I mean , it wouldn't surprise me if we could do a hundred of them or something . 
F:  But , probably when you include all the combinations , you 're actually talking about a thousand of them or something , and that 's probably more than we can do . 
C:  <laugh> 
F:  Uh , but a hundred is a lot . 
F:  And  and , uh , um  
C:  OK . 
F:  Yeah . 


C:  Yeah , and the  the second thing was about scratch space .  
C:  And I think you sent an email about , um , e scratch space for  for people 
C:  to work on . And I know that , uh , Stephane 's 
C:  working from an NT machine , so his  his home directory exists somewhere else . 
F:  His  his stuff is somewhere else , yeah . 
F:  Yeah , I mean , my point I  I want to  
F:  Yeah , thanks for bring it back to that . My  th I want to clarify my point about that  that  that Chuck repeated in his note . 
F:  Um . 
F:  We 're  over the next year or two , we 're gonna be upgrading the networks in this place , but right now they 're still all te pretty much all ten megabit lines . 
C:  Mm - hmm . 
F:  And we have reached the  this  the machines are getting faster and faster . 
F:  So , it actually has reached the point where it 's a significant drag on the time for something to move the data from one place to another . 
C:  Mm - hmm . 
F:  So , you  you don't w especially in something with repetitive computation where you 're going over it multiple times , 
F:  you do  don't want to have the  the data that you 're working on distant from where it 's being  where the computation 's being done if you can help it . 
C:  Mm - hmm . 
F:  Uh . 
F:  Now , we are getting more disk for the central file server , which , since it 's not a computational server , would seem to be a contradiction to what I just said . 
F:  But the idea is that , uh , suppose you 're working with , uh , this big bunch of multi multilingual databases . 
F:  Um , you put them all in the central ser at the cen central file server . 
C:  Mm - hmm . 
F:  Then , when you 're working with something and accessing it many times , you copy the piece of it that you 're working with over to some place that 's close to where the computation is and then do all the work there . 
F:  And then that way you  you won't have the  the network  you won't be clogging the network for yourself and others . 
C:  Mmm . 
F:  That 's the idea . 
F:  So , uh , it 's gonna take us  
F:  It may be too late for this , uh , p precise crunch we 're in now , but , uh , 
F:  we 're , uh  It 's gonna take us a couple weeks at least to get the , uh , uh , the amount of disk we 're gonna be getting . We 're actually gonna get , uh , 
F:  I think four more , uh , thirty - six gigabyte drives 
F:  and , uh , put them on another  another disk rack . We ran out of space on the disk rack that we had , so we 're getting another disk rack and <laugh> 
F:  four more drives to share between , uh  
F:  primarily between this project and the Meetings  Meetings Project . 
F:  Um . 
F:  <mouth> 
F:  But , uh , 
F:  we 've put another  I guess there 's another eighteen gigabytes that 's  that 's in there now to help us with the immediate crunch . 
F:  But , uh , are you saying  
F:  So I don't know where  you 're  Stephane , where you 're doing your computations . If  i so , you 're on an NT machine , so you 're using some external machine to  
G:  Yeah , it , uh  
G:  Well , to  
G:  It 's Nutmeg and Mustard , I think , the  
F:  Do you know these yet ? 
G:  I don't know what kind . 
A:  Nuh - uh .  
F:  Yeah , OK . 
C:  <laugh> 
F:  Uh , are these  are these , uh , computational servers , or something ? I 'm  I 've been kind of out of it . 
G:  Yeah , I think , yeah . 
G:  I think so . 
G:  Mmm . 
F:  Unfortunately , these days my idea of running comput of computa doing computation is running a spread sheet . So . 
F:  <laugh> 
C:  <perhaps drink and swallow> 
C:  <laugh> 
G:  Mmm . 
F:  Uh , haven't been  haven't been doing much computing personally , so . 
C:  <laugh> 
F:  Um . 
F:  Yeah , so those are computational servers . So I guess the other question is what disk there i space there is there on the computational servers . 
A:  Right . 
A:  Yeah , I 'm not sure what 's available on  is it  you said Nutmeg and what was the other one ? 
G:  Mustard .  
A:  Mustard . 
A:  OK . 
F:  Yeah , Well , you 're the  you 're the disk czar now . So 
B:  Huh . 
E:  <cough> 
A:  Right , right . Well , I 'll check on that . 
F:  <laugh> 
F:  Yeah . 
F:  Yeah , so basically , uh , Chuck will be the one who will be sorting out what disk needs to be where , and so on , and I 'll be the one who says , " OK , spend the money . " 
E:  <mike noise> 
F:  So . <laugh> 
F:  Which , I mean , n these days , uh , 
F:  if you 're talking about scratch space , it doesn't increase the , uh , need for backup , 
F:  and , uh , I think it 's not that big a d and the  the disks themselves are not that expensive . Right now it 's  
A:  What you can do , when you 're on that machine , is , uh , just go to the slash - scratch directory , and do a DF minus K , and it 'll tell you if there 's space available . 
G:  Yeah . 
A:  Uh , and if there is then , uh   
F:  But wasn't it , uh  I think Dave was saying that he preferred that people didn't 
F:  put stuff in slash - scratch . It 's more putting in d s XA or XB or , right ? 
A:  Well , there 's different  there , um , there 's  Right . 
A:  So there 's the slash - X - whatever disks , and then there 's slash - scratch . 
A:  And both of those two kinds are not backed up . 
A:  And if it 's called " slash - scratch " , it means it 's probably an internal disk to the machine . 
A:  Um . 
A:  And so that 's the kind of thing where , like if  
A:  um , 
A:  OK , if you don't have an NT , but you have a  a  a Unix workstation , 
A:  and they attach an external disk ,  it 'll be called " slash - X - something " uh , if it 's not backed up and it 'll be " slash - D - something " if it is backed up . 
A:  And if it 's inside the machine on the desk , it 's called " slash - scratch " . 
A:  But the problem is , if you ever get a new machine , they take your machine away . It 's easy to unhook the external disks , put them back on the new machine , but then your slash - scratch is gone . 
A:  So , you don't wanna put anything in slash - scratch that you wanna keep around for a long period of time . 
A:  But if it 's a copy of , say , some data that 's on a server , 
A:  you can put it on slash - scratch because , um , 
A:  first of all it 's not backed up , and second it doesn't matter if that machine disappears and you get a new machine because you just recopy it to slash - scratch . 
A:  So tha that 's why I was saying you could check slash - scratch on those  on  on , um , Mustard and  and Nutmeg to see if  if there 's space that you could use there . 
F:  I see . 
A:  You could also use slash - X - whatever disks on Mustard and Nutmeg . 
G:  Yeah , yeah . 
A:  Um . 
A:  Yeah , and we do have  I mean , yeah , so  so you  yeah , it 's better to have things local 
A:  if you 're gonna run over them lots of times so you don't 
F:  Right , so es so especially if you 're  right , if you 're  if you 're taking some piece of the training corpus , 
A:  have to go to the network . 
F:  which usually resides in where Chuck is putting it all on the  on the , uh , file server , 
F:  uh , then , yeah , it 's fine if it 's not backed up because if it g g gets wiped out or something , y I mean it is backed up on the other disk . So , yeah , OK . 
A:  Mm - hmm . 
A:  Yeah , so , <clear throat> one of the things that I need to  I 've started looking at  
A:  Uh , is this the appropriate time to talk about the disk space stuff ? 
F:  Sure . 
A:  I 've started looking at , um , disk space . Dan  David , um , put a new , 
A:  um , drive onto Abbott , 
A:  that 's an X disk , which means it 's not backed up . So , 
A:  um , I 've been going through and copying 
A:  data that is , you know , some kind of corpus stuff usually , that  that we 've got on a CD - ROM or something , 
A:  onto that new disk to free up space  on other disks . 
A:  And , um , so far , um , I 've copied a couple of Carmen 's , 
A:  um , 
A:  databases over there . 
A:  We haven't deleted them off of the slash - DC disk that they 're on right now in Abbott , 
A:  um , 
A:  uh , but we  
A:  I would like to go through  
A:  sit down with you about some of these other ones and see if we can move them onto , um , 
A:  this new disk also . 
G:  Yeah , OK . 
A:  There 's  there 's a lot more space there , and it 'll free up more space for doing the experiments and things . 
A:  So , anything that  that you don't need backed up , we can put on this new disk . 
A:  Um , but if it 's experiments and you 're creating files and things that you 're gonna need , you probably wanna have those on a disk that 's backed up , just in case something  
A:  goes wrong . So . 
A:  Um  
A:  So far I 've  I 've copied a couple of things , but I haven't deleted anything off of the old disk to make room yet . 
A:  Um , and I haven't looked at the  any of the Aurora stuff , except for the Spanish . 
A:  So I  I guess I 'll need to get together with you and see what data we can move onto the new disk . 
G:  Yeah , OK . 


F:  Um , yeah , I  I just  an another question occurred to me is  is what were you folks planning to do about normalization ? 
F:  Uh . 
G:  Um . 
G:  Well , we were thinking about using this systematically for all the experiments . 
G:  Um . 
F:  This being  ?  
G:  <mouth> So , but  
G:  Uh . 
G:  So that this could be another dimension , but we think perhaps we can use the  the best , uh , um , uh , normalization scheme as OGI is using , so , with 
G:  parameters that they use there , or  and  
F:  Yeah , I think that 's a good idea . I mean it 's i i we  we seem to have enough dimensions as it is . So probably if we <laugh> 
G:  u <squeak> u 
C:  <laugh> 
G:  Yeah , yeah , yeah . <laugh> 
F:  sort of take their  probably the on - line  line normalization because then it   
F:  it 's  if we do anything else , we 're gonna end up having to do on - line normalization too , so we may as well just do on - line normalization . 
G:  Mm - hmm . 
F:  So . 
F:  Um . 
F:  So that it 's plausible for the final thing . 
F:  Good . 


F:  Um . 
F:  So , I guess , yeah , th the other topic  
F:  I  maybe we 're already there , or almost there , is goals for the  for next week 's meeting . 
F:  Uh . 
F:  i i 
F:  i it seems to me that 
F:  we wanna do is flush out what you put on the board here . 
F:  Uh . 
F:  You know , maybe , have it be somewhat visual , 
F:  a little bit . <laugh> Uh , so w we can say what we 're doing , yeah . 
C:  OK . 
C:  Like a s like a slide ? 
C:  OK . 
F:  And , um , also , if you have  sorted out , um , 
F:  this information about how long i roughly how long it takes to do on what and , 
F:  you know , what we can  how many of these trainings , uh , uh , and testings and so forth that we can realistically do , 
F:  uh , then one of the big goals of going there next week would be to  to actually settle on which of them we 're gonna do . 
F:  And , uh , when we come back we can charge in and do it . 
F:  Um . 
F:  Anything else that  I 
F:  a a Actually  
F:  started out this  this field trip started off with  with , uh , Stephane talking to Hynek , so you may have  you may have had other goals , 
F:  uh , for going up , and any anything else you can think of would be  we should think about  accomplishing ? 
F:  I mean , I 'm just saying this because  maybe there 's things we need to do in preparation . 
G:  Oh , I think basically , this is  this is , uh , yeah . 
F:  OK . 
F:  OK . 
F:  Uh . 
F:  Alright . 


F:  And uh  and the other  the  the last topic I had here 
F:  was , um , uh d 
F:  Dave 's fine offer to  
F:  to , uh , do something  <laugh> on this . I mean he 's doing  
F:  <laugh>  he 's working on other things , <laugh> but to  to do something on this project . 
C:  <laugh> 
F:  So the question is , " Where  where could we , uh , uh , most use Dave 's help ? " 
E:  <clears throat> 
F:  <breath-laugh> 


G:  Um , yeah , I was thinking perhaps if , um , 
G:  additionally to all these experiments , which is not really research , 
G:  well I mean it 's , uh , running programs and , um , 
F:  Yeah . 
F:  <breath-laugh> 
G:  <mouth> trying to have a closer look at the  perhaps the , um , 
G:  <mouth> speech , uh , noise detection or , 
G:  uh , voiced - sound - unvoiced - sound detection and  
G:  Which could be important in  i for noise  noise  
A:  I think that would be a  I think that 's a big  big deal . 
A:  Because the  you know , the thing that Sunil was talking about , 
A:  uh , with the labels , 
A:  uh , 
A:  labeling the database when it got to the noisy stuff ? 
A:  The  
A:  That  that really throws things off . 
A:  You know , having the noise all of a sudden , your  your , um , speech detector , I mean the  the , um  
A:  What was it ? What was happening with his thing ? He was running through these models very quickly . He was getting lots of , uh , 
F:  Mmm . 
A:  uh insertions , is what it was , in his recognitions . 
F:  <inbreath> 
F:  The only problem  I mean , maybe that 's the right thing  the only problem I have with it is 
F:  exactly the same reason why 
F:  you thought it 'd be a good thing to do . 
F:  Um , I  I think that  
F:  Let 's fall back to that . But I think the first responsibility is sort of to figure out if there 's something  that , uh , an  an additional  
F:  <laugh> Uh , that 's a good thing you  remove the mike . Go ahead , good . 
F:  Uh , <laugh> uh . 
B:  <laugh> 
F:  What an additional clever person could help with when we 're really in a crunch for time . 
F:  Right ? Cuz Dave 's gonna be around for a long time , right ? He 's  he 's gonna be here for years . 
E:  Yeah . 
G:  Yeah . 
F:  And so , um ,  over years , if he 's  if he 's interested in , you know , voiced - unvoiced - silence , he could do a lot . But if there  if in fact there 's something else  
A:  <laugh> 
B:  <laugh> 
F:  that he could be doing , that would help us when we 're  we 're sort of uh strapped for time  
F:  We have  we  we 've , you know , only ,  uh , another  another month or two  to  you know , with the holidays in the middle of it , um , to  to get a lot done .  
F:  If we can think of something  some piece of this that 's going to be  
F:  The very fact that it is sort of just work , and i and it 's running programs and so forth , 
F:  is exactly why  it 's possible that it  some piece of could be handed to someone to do , because it 's not   
F:  Uh , yeah , so that  that 's the question . And we don't have to solve it right this s second , but if we could think of some  
F:  some piece that 's  that 's well defined , that he could help with , he 's expressing a will willingness to do that . 
A:  What about training up a , um , 
F:  Uh . 
A:  a multilingual net ? 
G:  Yeah . <breathe into mike> 
E:  Yes , maybe to , mmm , put together the  the label  the labels between TIMIT and Spanish or something like that . 
G:  Yeah , so defining the superset , and , uh , joining the data and  
E:  Yes . 
E:  Yeah . 
G:  Mmm . 
G:  Yeah . 
F:  <inbreath> 
F:  Uh . 
F:  Yeah , that 's something that needs to be done in any event . 
E:  Yeah . 
F:  So what we were just saying is that  that , um  
F:  I was arguing for ,  if possible , coming up with something that  that really was development and wasn't research because we  we 're  we have a time crunch . 
F:  And so , uh , if there 's something that would  would save some time that someone else could do on some other piece , then we should think of that first . 
F:  See the thing with voiced - unvoiced - silence is I really think that  that it 's  to do  to do a  a  a  a poor job is  is pretty quick , uh , or , you know , a so - so job . 
F:  You can  you can  you can throw in a couple fea we know what  what kinds of features help with it .  
E:  Hmm . 
F:  You can throw something in . 
F:  You can do pretty well . But I remember , in fact , when you were working on that , and you worked on for few months , as I recall , and you got to , say ninety - three percent , and 
A:  Mm - hmm . 
F:  getting to ninety - four  <laugh> really really hard . <laugh> 
A:  Another year . 
A:  <laugh> 
C:  <laugh> 
F:  Yeah , yeah . So , um  
B:  <cough> 
F:  And 
F:  th th the other tricky thing is , since we are , 
F:  uh , 
F:  even though we 're not  we don't have a strict prohibition on memory size , and  and computational complexity , 
F:  uh , clearly there 's some limitation to it . So if we have to  
F:  if we say we have to have a pitch detector , say , if we  if we 're trying to 
F:  incorporate pitch information , or at least some kind of 
F:  harmonic  harmonicity , or something , 
F:  this is another whole thing , 
F:  take a while to develop . 
F:  Anyway , it 's a very very interesting topic . 
F:  I mean , one  I think one of the  
F:  a lot of people would say , and I think Dan would also , uh , that one of the things wrong with current speech recognition is that we  we really do throw away 
F:  all the harmonicity information . 
F:  Uh , we try to get spectral envelopes . 
F:  Reason for doing that is that most of the information about the phonetic identity is in the spectral envelopes are not in the harmonic detail . 
F:  But the harmonic detail does tell you something . 
F:  Like the fact that there is harmonic detail is  is real important . 
F:  <inbreath> So . 
F:  Um . 


F:  So , uh . 
F:  So I think  Yeah . So  wh that  so the  
F:  the other suggestion that just came up was , well what about having him  work on the , uh ,  multilingual super f superset  kind of thing . 
F:  Uh , coming up with that and then , you know , training it  training a net on that , say , um , from  from , uh  from TIMIT or something . 
F:  Is that  or uh , for multiple databases . What  what would you  
F:  what would you think it would  
F:  wh what would this task consist of ? 
G:  Yeah , it would consist in , uh , well , um , creating the  the superset , and , uh , modifying the lab labels for matching the superset . 
G:  Uh . 
F:  Uh , creating a superset from looking at the multiple languages , and then creating i m changing labels on TIMIT ? 
G:  Well , creating the mappings , actually . Yeah . 
F:  Or on  or on multiple language  <hiccup> multiple languages ? 
E:  No . 
G:  Yeah , yeah , with the @ @ three languages , and  
E:  The multiple language . Maybe for the other language because TIMIT have more phone . 
F:  Yeah .  
F:  Uh . 
A:  So you 'd have to create a mapping from each language to the superset . 
E:  Yeah . Mm - hmm . 
G:  From each language to the superset , yeah . Mmm . 
E:  Yeah . 
C:  There 's , um  
F:  <mike noise> 
C:  Carmen was talking about this SAMPA thing ,  and it 's , um , 
C:  <mouth> it 's an effort by linguists to come up with , um , 
C:  a machine readable IPA , um , sort of thing , right ? 
C:  And , um , they  they have a web site that Stephane was showing us that has , um  
F:  Yeah . 
C:  has all the English phonemes and their SAMPA correspondent , um , phoneme , and then , um , they have Spanish , they have German , they have all  all sorts of languages , um , mapping  mapping to the SAMPA phonemes , which  
E:  Yeah , the tr the transcription , though , for Albayzin is n the transcription are of SAMPA the same , uh , how you say , symbol that SAMPA appear . 
B:  SAMPA ? 
B:  What does " SAMPA " mean ? 
F:  Mm - hmm . 
F:  Hmm . 
E:  But I don't know if TIMIT o how is TIMIT . 
B:  So , I mean  
F:  What  
B:  I 'm sorry .  
F:  Go ahead . 
B:  I was gonna say , does that mean IPA is not really international ? <laugh> 
C:  No , it 's  it 's saying  y 
A:  It uses special diacritics and stuff , which you can't do with ASCII characters . 
E:  Yeah . 
C:  can't print on ASCII . 
B:  Oh , I see .  
A:  So the SAMPA 's just mapping those . 
B:  Got it . 
F:  What , uh  <two vocal squeaks> Has OGI done anything about this issue ? 
F:  Do they have  
F:  Do they have any kind of superset that they already have ? 
G:  I don't think so . Well , 
G:  they  they  they 're going actually the  the other way , defining uh , phoneme clusters , apparently . Well . 
F:  Aha . 
F:  That 's right . 
F:  Uh , and that 's an interesting  way to go too . 
A:  So they just throw the speech from all different languages together , then cluster it into sixty or fifty or whatever clusters ? 
G:  <inbreath> 
G:  I think they 've not done it , uh , doing , uh , multiple language yet , but 
G:  what they did is to training , uh , English nets with all the phonemes , and then training it in English nets with , uh , kind of seventeen , I think it was  seventeen , uh , broad classes . 
A:  Automatically derived  Mm - hmm . 
G:  Yeah . 
A:  Automatically derived broad classes , or  ?  Uh - huh . 
G:  Yeah , I think so . 
G:  Uh , and , yeah . 
G:  And the result was that apparently , when testing on cross - language it was better . I think so . 
G:  But Hynek didn't add  didn't have all the results when he showed me that , so , well . But  
F:  So that does make an interesting question , though . Is there 's some way that we should tie into that with this . Um . 
B:  <mike noise> 
B:  <mike noise> 
F:  Right ? I mean , if  if in fact that is a better thing to do ,  should we leverage that , rather than doing ,  um , our own . 
F:  Right ? 
F:  <inbreath> So , if i if  if they s I mean , we have   
F:  i we have the  the trainings with our own categories . 
F:  And now we 're saying , " Well , how do we handle cross - language ? " 
F:  And one way is to come up with a superset ,  
F:  but they are als they 're trying coming up with clustered ,  
F:  and do we think there 's something wrong with that ? 
G:  I think that there 's something wrong or  Well , because  
F:  OK . What w 
G:  Well , for the moment we are testing on digits , and e i perhaps u using broad phoneme classes , it 's  it 's OK for um , uh classifying the digits , but 
G:  as soon as you will have more words , 
G:  well , words can differ with only a single phoneme , and  
G:  which could be the same , uh , class . Well . 
F:  I see . 
G:  So . 
F:  Right . Although , you are not using this for the  You 're using this for the feature generation , though , not the  
G:  So , I 'm afraid  
G:  Yeah , but you will ask the net to put one for th th the phoneme class and  
F:  s  
F:  Yeah . 
G:  So . 
F:  Yeah . 
A:  So you 're saying that there may not be enough information coming out of the net to help you discriminate the words ? 
G:  Well . 
G:  Yeah , yeah . Mmm . 
B:  Fact , most confusions are within the phone  phone classes , right ? 
B:  I think , uh , Larry was saying like obstruents are only confused with other obstruents , et cetera , et cetera .  
F:  Yeah . 
A:  Hmm . 
F:  Yeah . Yeah . 
G:  Yeah , this is another p yeah , another point . 
F:  Yeah . 
C:  So  so , maybe we could look at articulatory type stuff , right ? 
F:  But that 's what I thought they were gonna  Did they not do that , or  ? 
G:  I don't think so . Well , 
F:  So  
G:  they were talking about , perhaps , but they d I d w Yeah . 
F:  They 're talking about it , but that 's sort of a question whether they did because that 's  that 's the other route to go . Instead of this , you know  
C:  Mm - hmm . 
C:  Superclass . 
F:  Instead of the  the  the  the superclass thing , which is to take  
F:  So suppose y you don't really mark arti To really mark articulatory features , you really wanna look at the acoustics and  and see where everything is , and we 're not gonna do that . 
F:  So ,  uh , the second class way of doing it is  to look at the , uh , phones that are labeled and translate them into acoustic  uh , uh  articulatory , uh , uh , features . 
B:  <laugh> 
F:  So it won't really be right . You won't really have these overlapping  things and so forth , but  
A:  So the targets of the net  are these  ?  
F:  <mouth> Articulatory feature . 
A:  Articulatory features . 
F:  Right . 
A:  But that implies that you can have more than one on at a time ? 
F:  That 's right . 
A:  Ah . OK . 
F:  You either do that or you have multiple nets . 
A:  I see . 
F:  Um . 
F:  And , um 
F:  I don't know if our software  
F:  this  if the qu versions of the Quicknet that we 're using allows for that . 
F:  Do you know ? 
C:  Allows for  ?  
F:  Multiple targets being one ? 
C:  Oh , um , we have gotten soft targets to  to work . 
F:  OK . So that  that 'll work , yeah . 
C:  Yeah . 
F:  OK . 
F:  So , um , that 's another thing that could be done  is that we could  we could , uh , just translate  instead of translating to a superset ,  
B:  Um . 
F:  just translate to articulatory features , some set of articulatory features and train with that . 
F:  Now the fact  even though it 's a smaller number ,  it 's still fine because you have the  the , uh , combinations . 
F:  So , in fact , it has every , you know  it had  has  has every distinction in it that you would have the other way . 
G:  Yeah . 
F:  <inbreath> But it should go across languages better . 
A:  We could do an interesting cheating experiment with that too . 
A:  We could  
A:  I don't know , if you had 
A:  uh 
A:  the phone labels , you could replace them by their articulatory features and then feed in 
A:  a vector with 
A:  those 
A:  uh , things turned on based on what 
A:  they 're supposed to be for each phone to see if it  if you get a big win . Do you know what I 'm saying ? 
F:  No . 
A:  So , um , I mean , if your net is gonna be outputting , 
A:  uh , a vector of  basically of  well , it 's gonna have probabilities , but let 's say that they were ones and zeros , 
A:  then y and you know for each , 
A:  um , I don't know if you know this for your testing data , but if you know for your test data , 
A:  you know , what the string of phones is and  and you have them aligned , then you can just  
A:  instead of going through the net , just create the vector for each phone and feed that in 
A:  to see 
A:  if that data helps . Eh , eh , what made me think about this is , I was talking with Hynek and he said that there was a guy at A T - andT who spent 
A:  eighteen months working on a single feature . 
A:  And because they had done some cheating experiments  
F:  <inbreath> 
F:  This was the guy that we were just talking a that we saw on campus . So , this was Larry Saul who did this  did this . He used sonorants . 
A:  Oh , OK . 
A:  Right , OK , right . 
F:  Was what he was doing . 
A:  And they  they had done a cheating experiment or something , right ? and determined that  
F:  Yeah . 
F:  He  he di he didn't mention that part . But . 
A:  Well , Hynek said that  that , I guess before they had him work on this , they had done some experiment where if they could get that one feature right , 
F:  I see . 
F:  OK .  
A:  it dramatically improved 
A:  the result . So I was thinking , you know  it made me think about this , that if  
A:  it 'd be an interesting experiment just to see , you know , if you did get all of those right . 
F:  Should be . Because if you get all of them in there , that defines all of the phones . So that 's  that 's equivalent to saying that you 've got  
A:  <laugh> 
A:  Right . 
F:  <laugh> 
F:  got all the phones right . So , if that doesn't help , there 's   
A:  Yeah . 
F:  Although , yeah , it would be  make an interesting cheating experiment because we are using it in this funny way , where we 're converting it into features . 
A:  Yeah . 
A:  And then you also don't know what error they 've got on the HTK side . 
F:  <laugh> 
A:  You know ? 
F:  Yeah . 
A:  It sort of gives you your  the best you could hope for , kind of . 
C:  Mmm . 
C:  Mmm , I see . 
B:  The soft training of the nets still requires the vector to sum to one , though , right ? 
C:  To sum up to one . 
B:  So you can't really feed it , like , two articulatory features that are on at the same time 
B:  with ones 
B:  cuz it 'll kind of normalize them down to one half or something like that , for instance . 
G:  But perhaps you have the choice of the  final nonl uh , nonlinearity , yeah . 
C:  Right . 
C:  Nonlinearity ? 
C:  Um , it 's sig No , it 's actually sigmoid - X 
G:  Is it always softmax or  ?  
G:  Yeah . 
C:  for the  
G:  So if you choose sigmoid it 's o it 's OK ? 
C:  You , um  I think  I think apparently , the , uh  
F:  Did we just run out of disk , or  ? 
B:  Why don't you just choose linear ? 
C:  What 's that ? 
B:  Right ? Linear outputs ? 
C:  Linear outputs ? 
B:  Isn't that what you 'll want ? 
C:  Um . 
B:  If you 're gonna do a KL Transform on it . 
C:  Right , right . 
C:  Right , but during the training , 
C:  we would train on sigmoid - X and then 
B:  Oh , you  Yeah ? 
C:  at the end just chop off the 
B:  Hmm . 
C:  final nonlinearity . 
F:  So , we 're  we 're  we 're off the air , or  ? 
H:  Um , about to be  
F:  About to be off the air . 


