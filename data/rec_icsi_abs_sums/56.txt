None

D:  Yeah . So there was this conference call this morning , 
D:  and the only topic on the agenda was just to discuss 
D:  a and to come at  uh , to get a decision about this latency problem . 

D:  Uh , yeah . There were like two hours of  discussions , 
D:  and then suddenly , <breath> uh , people were tired , I guess , 
D:  and they decided on <mike noise> a number , 
D:  two hundred and twenty , 
D:  included e including everything . 
D:  So , currently d uh , we have system that has two hundred and thirty . 
B:  we have to reduce it by ten milliseconds somehow . 
D:  That 's not a problem , I  I guess . 
B:  W It 's  it 's p d primary  primarily determined by the VAD at this point , 
B:  S so we can make the VAD a little shorter . 
B:  Yeah . We probably should do that pretty soon so that we don't get used to it being a certain way . 
D:  Uh , yeah . So , the second thing is the system that we have currently . 
D:  Oh , yes . We have , like , a system that gives sixty - two percent improvement , 
D:  but <mouth> if you want to stick to the  <breath> this latency  
D:  Well , it has a latency of two thirty , 
D:  but <breath> if you want also to stick to the number <breath> of features that  limit it to sixty , <breath> then we go a little bit down 
D:  but it 's still sixty - one percent . 

B:  Um , while we 're still on Aurora stuff  maybe you can talk a little about the status with the , uh , <breath> Wall Street Journal <breath> things for it . 
A:  So I 've , um , downloaded , uh , a couple of things from Mississippi State . 
A:  They wrote some scripts that sort of make it easy to run <breath> the system on the Wall Street Journal , uh , data . 
A:  Um , so I haven't run the scripts yet . 
A:  Uh , I 'm waiting  there was one problem with part of it 
A:  and I wrote a note to Joe asking him about it . 
A:  So I 'm waiting to hear from him . 
A:  they 're  I I 'm still waiting for them to  release the , um , <mouth> multi - CPU version of their scripts , 
A:  cuz right now their script only handles processing on a single CPU , 
A:  which will take a really long time to run . 
A:  So , as soon as they get that , then I 'll  I 'll grab those too 
B:  Yeah . Cuz we have to get started , 
A:  Yeah . I 'll go ahead and try to run it though with just the single CPU one , 
A:  and  I  they  they , <breath> um , released like a smaller data set that you can use that only takes like sixteen hours to train and stuff . 
A:  So I can  I can run it on that just to make sure that the  <breath> the thing works and everything . 
B:  So it could be  I mean , Chuck and I had actually talked about this a couple times , and  and  over some lunches , I think , <breath> that , um , <mouth> one thing that we might wanna do  
B:  The - there 's this question about , you know , what do you wanna scale ? 
B:  Suppose y you can't adjust <breath> these word insertion penalties and so forth , 
B:  so you have to do everything at the level of the features . 
B:  And , uh , one thing I had suggested at an earlier time was maybe some sort of scaling , 
B:  some sort of root or  or something of the , um , <mouth> uh , features . 
B:  it occurred to me later , 
B:  because what you really want to do is scale the , uh , @ @  the range of the likelihoods rather than  
B:  But ,  I mean , I guess we still haven't had a  <breath> a ruling back on this . 
B:  And we may end up being in a situation where we just you know really can't change the <breath> word insertion penalty . 
B:  But the other thing we could do <breath> is  also we could  
B:  I mean , this  this may not help us , <breath> uh , in the evaluation 
B:  but it might help us in our understanding at least . 
B:  We might , <breath> just run it with different insper insertion penalties , 
B:  and show that , uh , " well , OK , not changing it , <breath> playing the rules the way you wanted , we did this . But in fact if we did that , it made a   a big difference . " 

C:  So Michael Kleinschmidt , who 's a PHD student from Germany , <breath> showed up this week . 
C:  He 'll be here for about six months . 
C:  And he 's done some work using <breath> an auditory model  of , um , <breath> human hearing , 
C:  and  using that f uh , to generate speech recognition features . 
C:  And  he did <breath> work back in Germany <breath> with , um , a toy recognition system <breath> using , um , isolated <breath> digit recognition <breath> as the task . 
C:  he w he 's coming here to u u use it on a <breath> uh , a real speech recognition system . 
C:  Th - this is  because it 's , um  there are these different parameters for the shape of these <breath> basis functions , <breath> um  <breath> there are a lot of different possible basis functions . 
C:  And so he  <breath> he actually does <breath> an optimization procedure to choose an  <breath> an optimal set of basis functions out of all the possible ones . 
C:  is ,  <breath> um , <mouth> he starts with  he has a set of M of them . 
C:  I mean , he t he tries , um , <mouth> using  just M minus one of them . 
C:  So there are M possible subsets of this <breath> length - M vector . 
C:  He tries classifying , using each of the M <breath> possible sub - vectors . 
C:  Whichever sub - vector , <breath> um , works the  the best , I guess , he says  <breath> the  the fe feature that didn't use was the most useless feature , 
C:  so we 'll throw it out 
C:  and we 're gonna randomly select another feature  from the set of possible basis functions . 
B:  So I th I think it 's  it 's  I think it 's kinda neat stuff . 
B:  the thing that I wanted to  to add to it also was to have us use this in a multi - stream way . 
B:  so  so that , um , <mouth> when you come up with these different things , <breath> and these different functions , <breath> you don't necessarily just put them all into one huge vector , 
B:  but perhaps <clears throat> you <breath> have some of them in one stream and some of them in another stream , and so forth . 
E:  Well , that sort of segues into  what  what I 'm doing . 
E:  Um , <breath> so , uh , the big picture is k um , <mouth> come up with a set of , <breath> uh , intermediate categories , 
E:  then build intermediate category classifiers , then do recognition , 
E:  Um , so right now I 'm in  in the phase where <breath> I 'm looking at  at , um , deciding on a initial set of intermediate categories . 
E:  And <breath> I 'm looking <breath> for data data - driven  methods that can help me find , <breath> um , a set of intermediate categories <breath> of speech that , uh , will help me to discriminate  later down the line . 
E:  And one of the ideas , <breath> um , that was to take a  take a neural net  
E:  train  train an ordinary neural net <breath> to  <breath> uh , to learn the posterior probabilities of phones . 
E:  Um , <mouth> the other one  was , <breath> um , to , <breath> uh , come up with a  a  a model   um , a graphical model , <breath> that treats  the intermediate categories <breath> as hidden  hidden variables , latent variables , that we don't know anything about , 
E:  but that through , <breath> um , s statistical training and the EM algorithm , <breath> um , at the end of the day , <breath> we have , um  we have learned something about these  these latent , um  latent variables 
E:  which happen to correspond to <breath> intermediate categories . 

B:  Ho - how much memory d ? H how many  ? 
D:  I d I d uh , I  I don't kn remember exactly , 
B:  Yeah . I 'd like to  see that , 
B:  cuz maybe I could think a little bit about it , 
B:  cuz we <mouth> maybe we could make it a little smaller 
B:  Uh , I 'd like to see how far off we are . 
B:  But I guess it 's still within their rules to have  have it on the , uh , t uh , server side . 

D:  Uh , yeah . There were like two hours of  discussions , 
D:  and then suddenly , <breath> uh , people were tired , I guess , 
D:  and they decided on <mike noise> a number , 
D:  two hundred and twenty , 
D:  included e including everything . 
D:  So , currently d uh , we have system that has two hundred and thirty . 
B:  we have to reduce it by ten milliseconds somehow . 
D:  That 's not a problem , I  I guess . 
B:  W It 's  it 's p d primary  primarily determined by the VAD at this point , 
B:  S so we can make the VAD a little shorter . 
B:  Yeah . We probably should do that pretty soon so that we don't get used to it being a certain way . 
D:  Uh , yeah . So , the second thing is the system that we have currently . 
D:  Oh , yes . We have , like , a system that gives sixty - two percent improvement , 
D:  but <mouth> if you want to stick to the  <breath> this latency  
D:  Well , it has a latency of two thirty , 
D:  but <breath> if you want also to stick to the number <breath> of features that  limit it to sixty , <breath> then we go a little bit down 
D:  but it 's still sixty - one percent . 

D:  Uh , and if we drop the tandem network , then we have fifty - seven percent . 
B:  Uh , but th the two th two thirty includes the tandem network ? 
B:  And i is the tandem network , uh , small enough that it will fit on the terminal size 
D:  Uh , no , I don't think so . 
D:  It 's still  in terms of computation , if we use , like , their way of computing the  the maps  the  the MIPs , <breath> I think it fits , 
D:  but it 's , uh , m mainly a problem of memory . 
D:  and I don't know how much  this can be discussed or not , 
D:  because it 's  it could be in ROM , 
D:  so it 's maybe not that expensive . 
B:  Ho - how much memory d ? H how many  ? 
D:  I d I d uh , I  I don't kn remember exactly , 
B:  Yeah . I 'd like to  see that , 
B:  cuz maybe I could think a little bit about it , 
B:  cuz we <mouth> maybe we could make it a little smaller 
B:  Uh , I 'd like to see how far off we are . 
B:  But I guess it 's still within their rules to have  have it on the , uh , t uh , server side . 

D:  Yeah . The last thing is that I think we are getting close to human performance . 
D:  Well , that 's something I would like to investigate further , 
D:  I did , like , um  I did , uh , listen to the m most noisy utterances of the SpeechDat - Car Italian 
D:  and tried to transcribe them . 
B:  So this is a particular human . 
B:  This is  this i this is Stephane . 
D:  that 's the  the flaw of the experiment . 
D:  Um , but what happens also is that if I listen to the , um  <noise> a re - synthesized version of the speech 
D:  and  I re - synthesized this using a white noise that 's filtered by a LPC , uh , filter  
D:  while our system is currently at seven percent . 
D:  but still , uh , <breath-laugh> what happens is  is that , <mouth> uh , the digit error rate on this is around one percent , 
D:  well , you can argue , that , uh  that this is not speech , 
D:  so the ear is not trained to recognize this . 
D:  But s actually it sound like  whispering , 
B:  There 's two problems there . 
B:  I mean  I mean , so  so the first is <breath> that by doing LPC - twelve with synthesized speech w like you 're saying , uh , it 's  <breath> i i you 're  you 're adding other degradation . 
B:  So it 's not just the noise 
B:  but you 're adding in fact some degradation 
B:  because it 's only an approximation . 
B:  and the second thing is  which is m maybe more interesting  is that , um ,  <breath> if you do it with whispered speech , you get this number . 
B:  What if you had  done analysis  re - synthesis and taken the pitch as well ? 
B:  So now you put the pitch in . 
B:  What would the percentage be then ? 
B:  See , that 's the question . 
B:  That would say at least for people , having the pitch is really , really important , 
B:  I mean , th the thing is LPC is not a  a really great representation of speech . 
B:  uh , but i I don't know . 
B:  I do don't wanna take you away from other things . 
B:  Yeah . I mean , it 's probably not worth your time . 
B:  It 's  it 's a side thing 
B:  and  and  and there 's a lot to do . 

