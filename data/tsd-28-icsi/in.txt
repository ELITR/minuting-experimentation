and <breath> the main thing that i was gon na ask people to help with today is to give input on what kinds of database format we should use in starting to link up things like word transcripts and annotations of word transcripts , th - there are sort of two choices . <EOS>
i mean , we i sort of already have developed an xml format for this sort of stuff . <EOS> so tha it has a single time - line , i think for word - level , this would be ok . <EOS>
and i thought it was better if you 're looking at a raw file to be t for the tags to say `` it 's an utterance `` , as opposed to the tag to say `` it 's a link `` . <EOS> one of them is that it 's easy to parse . <EOS> can you but you can add to those structures if you the other thing the other way that i sort of established this was as easy translation to and from the transcriber format . <EOS>
so i think it it 's debatable whether you want to do phone - level in the same thing . <EOS>
or or any frame - level stuff i would use p - file . <EOS> it 's ics uh , icsi has a format for frame - level representation of features . <EOS> and we have a lot of tools already to deal with it . <EOS> i mean , it 's something that we developed at icsi . <EOS> but , i mean , it is just something we developed at icsi . <EOS>
more compact , <EOS>
there 's a standard again in xml , specifically for searching xml documents structured x - xml documents , where you can specify both the content and the structural position . <EOS> it 's it 's you would use that to build your tool to do that sort of search . <EOS> what you would do is , someone would build a tool that used that as a library . <EOS>
which was their file format is just nodes and links , <EOS>
and then `` type `` would be `` utterance `` . <EOS>
they 're developing a big infrastructure . <EOS> um , and apparently they 've also developed a lot of tools , one of the things that atlas is doing is they 're trying to define an api which is independent of the back store , so that , uh , you could define a single api and the the storage could be flat xml files or a database . <EOS>
but i thought it would be good to get something that we can that other people can use or adopt for their own kinds of encoding . <EOS> and so i wanted something where all of this can be done in a elegant way and that if somebody wants to try something or compute something else , that it can be done flexibly . <EOS>
we should look at atlas , maybe i should . <EOS> so i 'll i 'll take a closer look at it . <EOS>
do they already have something that 's that would be useful for us in place ? <EOS> well , i think i 'm i think w i had better look at it again but i 'm forgetting the exact level of nesting . <EOS> i mean , i have to look at it again to see whether it can really do what we want , <EOS>
it seems to me you want to keep the frame - level stuff separate . <EOS>
i i think if it 's conceptually close , and they already have or will have tools that everybody else will be using , i mean , <breath> it would be crazy to do something s you know , separate that <EOS>
do they already have something that 's that would be useful for us in place ? <EOS> um , th what would would would what would worry me is that maybe we might miss a little detail i mean , i have to look at it again to see whether it can really do what we want , <EOS>
for lower than word - level , you 're talking about so much data that i just i do n't know . <EOS> these are big files . <EOS>
none <EOS>
i mean these are long meetings but i think , a anything at frame - level , even p - file , is too verbose . <EOS>
no matter what format you choose , you 're gon na have the trou you 're gon na have the difficulty of relating the the frame - level features <EOS>
and sometimes the word sequences even differ slightly because they were edited s at one place but not the other . <EOS> but , see , if you 'd annotate dialogue acts , you do n't necessarily want to or topics you do n't really want to be dealing with time - marks . <EOS>
the hard part is specifying what you mean by `` merge `` . <EOS>
oh , this was about um , inferring intentions from features in context , and the words , <EOS>
so , < clears throat > what we found interesting is , first of all , intentions differ . <EOS> maybe you want to enter a building . <EOS> maybe you want to see it , or maybe you actually want to come as close as possible to the building . <EOS> if you do n't have the intention of entering your building , but you know that something is really close to it , and you just want to approach it , or get to that building . <EOS>
but , um , since we are designing a a a an , compared to this , even bigger data collection effort , um , we will definitely take care to put it in there , <EOS>
um , we can look at some factors that may make a difference . <EOS> um . <EOS> sometimes i found in the uh , looking at the data , in a superficial way , i found some s sort of modifiers that that m may also give us a hint , and this leads us straight to the context which also should be considered . <EOS> and i will try to to sort of come up with a list of factors that we need to get out of there , there 's gon na be contextual things , there 're gon na be linguistic things , there 're gon na be discourse things , the issue is , can we find a way to , basically , featurize it <EOS>
like `` s go to see `` , or `` visit `` , or some this is of course a crucial factor , `` what type of object is it ? `` <EOS> then of course the the actual phrases may give us some idea of what the person wants . <EOS> oh , another thing you want is some information abou i think , about the time of day . <EOS> so if it turns out that , whatever it is , you want to know whether the person 's uh , a tourist or not , ok ? <EOS> that becomes a feature . <EOS>
and we 're able to , by hand , extract the features to put in the belief - net . <EOS> if that goes well , then we can start worrying about how we would extract them . <EOS> so we 'll be like , hand , uh , doing all the probabilities . <EOS>
if we feed it through a belief - net or or something along those lines . <EOS> we 'd get an inferred intention , we we produce a structure that differentiates between the vista , the enter , and the , um , tango mode . <EOS> and , my idea on how to combine them is with a belief - net , which is going to have as output , the conditional pr probability of one of three things , but the idea is to take as a first goal , see if we could actually build a belief - net that would make this three way distinction uh , in a plausible way , here 're the things which , if you get them out of out of the language and discourse , and put them into the belief - net , it would tell you which of these three uh , intentions is most likely . `` <EOS> i think that , uh , if we can get the information , a belief - net is a perfectly good way of doing the inferential combination of it . <EOS> javabayes or something ? <EOS>
so one thing you could do is build a little system that , said , `` whenever you got a question like that i 've got one of three answers . <EOS> u u sort of i 'm , at the moment , curious and i 'm i 'm s w want to approach it from the end where we can s sort of start with this toy system that we can play around with , and then in the longer run , you would figure out how you could derive them . <EOS> from previous discourse or w any anything else you knew . <EOS> and , then as soon as we have it , i think we should start trying to populate it for this problem . <EOS>
maybe for a deep understanding task , that 's a nice sort of playground or first little thing . `` <EOS> so we think it 's a well - formed , uh , starter task for this , uh , deeper understanding in the tourist domain . <EOS>
ok ? <EOS> we we have a we know what the outcomes are gon na be , it all all of a sudden it does much better . <EOS>
these we have all these transcripts but , um , since we are designing a a a an , compared to this , even bigger data collection effort , um , we will definitely take care to put it in there , and start recording subjects probably within a month or something . <EOS>
so , th they 're gon na give us some cr uh or we can assume that y you get this crude information . <EOS> so , yeah , we 're sort of < mike noise > committed to xml as the kind of , uh , interchange . <EOS>
ok ? <EOS> so y so one of th one of the things we wan na do is actually , uh , pick a package , yeah bu w i 'd like that this y yeah , this week , to ha to n to <inbreath> have y guys , uh , you know , pick <mouth> the y you know , belief - net package <EOS>
you know , we do n't need the one that 'll solve massive , uh , belief - nets quickly . <EOS> but we do want one in which it 's easy to interact with and , uh , modify . <EOS> you want it stable , you want it and probably one in which it 's easy to have , um , what amounts to transcript files . <EOS>
an and there 're plenty of people around , students in the department who , you know , live and breathe bayes - nets . <EOS>
well , i 'd like to also , though , uh , ha have a first cut at what the belief - net looks like . <EOS> is show the state and show the system and show that . <EOS>
and you probably need intermediate nodes . <EOS> and then in the longer run , you would figure out how you could derive them . <EOS> from previous discourse or w any anything else you knew . <EOS>
is , uh , one , where do you get this i information from , if that goes well , then we can start worrying about how we would extract them . <EOS> and then we can start worrying about where to get this input , the issue is , can we find a way to , basically , featurize it and , as soon as we have one , we can start trying to , uh , make a first cut at what 's going on . <EOS>
the street network of our geographic information system . <EOS> a lot of parsers , um , that 's way beyond their scope , is of interpreting that . <EOS> because i we can not differentiate , at the moment , between , you know , the intention of wanting to go there or the intention of just know wanting to know where where it is . <EOS>
it would always use the closest point to the object , <EOS>
what we do know , is that the parser we use in the smartkom system will never differentiate between any of these . <EOS> so it 's it 's it 's way too crude to d capture those differences in intentions . <EOS> what he was saying is , the m - three - l does not have any of that . <EOS> that 's way beyond their scope , is of interpreting that . <EOS>
there 's gon na be contextual things , there 're gon na be linguistic things , there 're gon na be discourse things , is , uh , one , where do you get this i information from , but if we ca n't do that , then we 're in trouble . <EOS> the real issue is , do what are the factors involved in determining this ? <EOS> the issue is , can we find a way to , basically , featurize it <EOS>
and two , what 's the structure of the belief - net ? <EOS> so that we get some discrete number of features so that , uh , when we know the values to all those features , or as many as possible , we can w come up with the best estimate of which of the , in this case three little intentions , are most likely . <EOS>
if that goes well , then we can start worrying about how we would extract them . <EOS> and then in the longer run , you would figure out how you could derive them . <EOS> from previous discourse or w any anything else you knew . <EOS>
not from that data . <EOS> but it was never th th the goal of that data collection to to serve for sat for such a purpose . <EOS> so that 's why for example the tasks were not differentiated by intentionality , uh , what one hopes is that when we understand how the analyzer works , we can both worry about converting it to english and worry about how it could ex extract the parameters we need for the belief - net . <EOS>
so . <EOS> uh , we we have to have this discussion of th the experiment , and the data collection , and all that sorta stuff so , we that 's part of what we 'll have to figure out . <EOS>
um right now it 's still kind of in a toy version of it , <EOS>
the probability whether the probability of a vista , tango , or enter . <EOS>
so then the features we decided or we decided we were talked about , you know . <EOS> we had a list of things like `` to go `` and `` to visit `` and what not . <EOS> uh the the prosody , the discourse , verb choice . <EOS> so there are certain cues that are very strong either lexical or topic - based um , concept cues and some of them are sort of <breath> either world knowledge or situational things . <EOS> is that maybe we ob we could observe a couple of um discourse phenomena such as the admission fee , <EOS>
so maybe this could be sort of a separate region of the net , which has two has it 's own middle layer . <EOS> they ra may have there own hidden layer that points to some of the the real hidden layer , um or the general hidden layer . <EOS>
but the middle thing , we were thinking along the lines of maybe trying to figure out , like , the concept of whether they 're a tourist or whether they 're running an errand or something like that so then the hidden variables hair variables we came up with were whether someone was on a tour , running an errand , or whether they were in a hurry , but the other ones , the final destination , the whether they 're doing business , whether they 're in a hurry , and whether they 're tourists , they ra may have there own hidden layer that points to some of the the real hidden layer , um or the general hidden layer . <EOS> and then these should then connect somehow to the more plan - based deep space <EOS>
so there are certain cues that are very strong either lexical or topic - based um , concept cues so <breath> maybe what what what happened what might happen is that we do get this sort of task - based middle layer , entering or som you know like they might be more task - based . <EOS>
so . <EOS> the mode basically has three different outputs . <EOS>
it 's cra has a gui and it 's uh but uh it 's free . <EOS> but actually it had an interface . <EOS> and he 's updated it for an xml version of i guess bayes - nets . <EOS>
like , we totally hand - tuned the probabilities , the probabilities and all are completely ad - hoc . <EOS>
but um in terms of specifying the scenario , <breath> um uh uh we 've gotten a little further so we wanted just to collect data , to get that that that elicits more , uh , that elicits richer language . <EOS>
so then our next idea was to add a middle layer , uh we kept um things from directly affecting the mode beyond the concept , but we just decided to keep all the things we extracted to point at the middle and then down . <EOS> i guess , the fact that the there 's a complete separation between the observed features and in the output . <EOS>
none <EOS>
is that maybe we ob we could observe a couple of um discourse phenomena such as the admission fee , so maybe this could be sort of a separate region of the net , which has two has it 's own middle layer . <EOS> that would all f funnel into one node that would constitute entrance requirements or something like that . <EOS> they ra may have there own hidden layer that points to some of the the real hidden layer , um or the general hidden layer . <EOS>
so , um i suggest w to for to proceed with this in in the sense that maybe throughout this week the three of us will will talk some more about maybe segmenting off different regions , identify four regions , maybe make up some features for each region <EOS>
so one thing that might be helpful which is implicit in the use of `` admission fee discussion `` as a cue for entry , <breath> is thinking about the plans that various people might have . <EOS> they 're in in non in sort of more traditional ai kinds of plan recognition things you sort of have <breath> you know , some idea at each turn of agent doing something , i mean there are some some of them are extremely elaborate , <EOS>
yeah , another thing to do , um , is also to , um i guess to ask around people about other bayes - net packages . <EOS>
we can maybe write an interface th for uh entering probability distributions easily , something like like a little script . <EOS> i think it might it might be simpler to just have a script that , you know <EOS>
like we wan na we wan na be able to collect <breath> as much of the variables that are needed for that , so now i think we should maybe have at least one navigational task with with sort of explicit uh <EOS>
none <EOS>
that 's that that needs a lot of work . <EOS>
because we did n't know the probabilities of or so that 's like a huge uh clue that they 're trying to enter the place rather than uh to tango or vista , like , we totally hand - tuned the probabilities , the probabilities and all are completely ad - hoc . <EOS>
but you could see perhaps discus the `` admission fee `` going directly to the mode pointing at `` enter `` , well for instance , the `` discourse admission fee `` node seems like it should point directly to the or increase the probability of `` enter directly `` versus `` going there via tourist `` . <EOS>
reasons being , you know , it 'd be a pain to set up all the probabilities for that . <EOS> if we moved onto the next step and did learning of some sort , uh according bhaskara we 'd be handicapped . <EOS>
it might be that if you add a new thing pointing to a variable , you just like it just overwrites everything . <EOS> but they 're not very friendly . <EOS>
i did n't think it did learning . <EOS>
so . <EOS> on friday we had our wizard test data test and um <outbreath> these are some of the results . <EOS>
um , we have to refine the tasks more and more , which of course we have n't done at all , so far , in order to avoid this rephrasing , and uh my suggestion is of course we we keep the wizard , because i think she did a wonderful job , um and also if she 's willing to take on the job of organizing all those subjects and stuff that would be wonderful . <EOS>
none <EOS>
so , what i did for this this is uh , a pedagogical belief - net and i grouped things according to what how i thought they would fit in to uh image schemas that would be related . <EOS> well , this is not a working bayes - net . <EOS> but the uh the the nice thing is that you know , it just is a is a visual aid for thinking about these things which has comple clearly have to be specified m more carefully <EOS>
is , if we just do this , we could wind up with a huge uh , combinatoric input to the mode thing . <EOS> uh , we have a d a technical problem with the belief - nets that we we do n't want all the com too many factors if we if we allow them to just go combinatorially . <EOS>
which is there are technical ways of doing it , and the other trick , which is not a technical trick , it 's kind of a knowledge engineering trick , is to make the n each node sufficiently narrow that you do n't get this combinatorics . <EOS>
we have to add , you know , not too much about um object types and stuff , that 's another sort of thing `` ok , here 's a another kind of minimal uh way of tackling this `` . <EOS> add extra properties , a deterministic rule for every property <EOS>
and um and here is exactly where what 's gon na be replaced with our bayes - net , <EOS>
none <EOS>
so . <EOS> what you 're trying to get out of this deep co cognitive linguistics is the fact that w if you know about source source , paths and goals , and nnn all this sort of stuff , that a lot of this is the same , for different tasks . <EOS> and that uh there 's there 's some some important generalities that you 're getting , so that if you have sources , you have trajectors and stuff like that , but what i 'd like to be able to do is to have the way that you extract properties , that will go into different bayes - nets , be the uh general . <EOS> what you 'd really like of course is the same thing you 'd always like which is that you have um a kind of intermediate representation which looks the same o over a bunch of inputs and a bunch of outputs . <EOS>
s so if you just number them `` one `` , `` two `` , `` three `` it 's and uh my suggestion is of course we we keep the wizard , because i think she did a wonderful job , <EOS>
my idea on that was uh , partly we 'll talk about system stuff for the computer scientists , but partly i did want it to get the linguists involved in some of this issue about what the task is and all um you know , what the dialogue is , and what 's going on linguistically , you just uh do `` this is what we did , and here 's the thing , and here 's s some of the dialogue and and so forth . `` <EOS>
none <EOS>
so the belief - net takes as input , a vector , and then we want to look up some more stuff in the ontology but also we definitely want to look up in the dialogue history um some s some stuff . <EOS> there will be rules , but they are n't rules that come to final decisions , they 're rules that gather information for a decision process . <EOS> my guess is it 'll be the same basic agent that um can go off and get information , run it through a a c this belief - net that <EOS>
and you wanted to specialize it to these three ones , then you would have to supply the parameters . <EOS> and that may actually involve getting more information . <EOS>
i think we i i can come up with a a code for a module that we call the `` cognitive dispatcher `` , which does nothing , but it looks of complect object trees and decides how are there parts missing that need to be filled out , and then collect uh sub - objects and then recombine them and put them together . <EOS>
none <EOS>
five minutes is just too long . <EOS> s so if you just number them `` one `` , `` two `` , `` three `` it 's <EOS>
she also agrees that you know if it 's all just gon na be students the data is gon na be less valuable because of that <EOS>
is , if we just do this , we could wind up with a huge uh , combinatoric input to the mode thing . <EOS> uh , we have a d a technical problem with the belief - nets that we we do n't want all the com too many factors if we if we allow them to just go combinatorially . <EOS>
so , what i did for this this is uh , a pedagogical belief - net well , this is not a working bayes - net . <EOS>
i mean , not necessarily in th in this meeting , but to try to informally think about what the decision variables are . <EOS> so the immediate problem is just deciding w which aspects of the x - schema to add . <EOS>
the harder problem is we decide what we want to use , how are we gon na get it ? <EOS>
and i do n't yet see how that goes . <EOS>
and uh see if we can find a way to present this to this linguists group that that is helpful to them . <EOS>
what we think is gon na happen is that , uh , in parallel starting about now <breath> we 're gon na get fey <mouth> to , where you 're working with me and robert , draft a note that we 're gon na send out to various cogsci c and other classes saying , `` here 's an opportunity to be a subject . <EOS> but what i 'd like to do , if it 's o k , <breath> is to s to , as i say , start the recruiting in parallel and possibly start running subjects next week . <EOS>
and , um and now it 's we have a complete english parser that does everything the german parser does . <EOS> and it still is , now in english . <EOS> that , these guys did in a in a day . <EOS>
one thing i was wondering , was , those functions there , are those things that modify the m - three - l basically ? <EOS> i think each of those functions act on the current xml structure , and change it in some way , for example , by adding a a l a field to it , or something . <EOS> there were other actions uh , that that s seemed to step state variables somewhere , which , in german , uh , takes these t sentence templates and produces xml structures . <EOS>
uh you you have a route , and every every element of that e r r f of that every segment we call a `` route element `` . <EOS> where y where you sort of end , and some points of interest along the way . <EOS>
is one , um , <breath> <mouth> also allocating , uh , some tags for our action schema enter - vista - approach , <EOS>
uh , at some point we 're going to have to worry about the language end . <EOS>
eh so , you you you may or so , then you 'd have this little vector of , um , you know , approach mode or eva mode . <EOS> it 's part of what you know about a an object , <breath> is its eva vector . <EOS>
if you know it for a specific landmark you put it there . <EOS> if you do n't , you just go up the hierarchy to the first place you find one . <EOS> if it 's that type of thing , and we want its eva vector , pppt - pppt ! <EOS> it 's that . `` <EOS>
the typical example is that , um , these are all a bunch of cues for something , and this is a certain effect that we 'd like to conclude . <EOS>
and , that 's a lot of probabilities to put here , which is kind of a pain . <EOS>
so noisy - ors are a way to , uh , <breath> sort of deal with this . <EOS> so we come up with these l little tables for each of those and the final thing is that , um <breath> <mouth> this is a deterministic function of these , th - c the full conditional probability table with some function . <EOS>
none <EOS>
in parallel with that , we 're gon na need to actually do the script . <EOS> this the permission form . <EOS>
what we think is gon na happen is that , uh , in parallel starting about now <breath> we 're gon na get fey <mouth> to , where you 're working with me and robert , draft a note that we 're gon na send out to various cogsci c and other classes saying , `` here 's an opportunity to be a subject . <EOS> not necessarily by any means all students but what i 'd like to do , if it 's o k , <breath> is to s to , as i say , start the recruiting in parallel and possibly start running subjects next week . <EOS> which is we gon na check out our social infrastructures for possible subjects . <EOS>
that , these guys did in a in a day . <EOS> eh , but again , there 's one module in which there 's one piece <mouth> that we have to convert to english . <EOS>
and the next thing i would like to be able to do , and it seems like this would not be too difficult either , is <breath> to say , `` ok let 's now pretend we actually wanted to not only change the <breath> the mapping of of , uh , words to the m - three - l but we also wanted to change add a new sentence type so , yeah . <EOS> i definitely think it 's <breath> it 's worth the exercise of trying to actually add something that is n't there . <EOS>
it seems to me we can get <breath> all the complexity we want in actions and in language without going outside of tourists in heidelberg . <EOS> so , at least unless somebody else wants t to suggest otherwise i think <breath> the general domain we do n't have t to uh , broaden . <EOS> that is , tourists in heidelberg . <EOS>
none <EOS>
so it seemed to me , what we ought to do is get our story together . <EOS> and think about it some , internally , before asking them to make changes . <EOS> ok . <EOS> what are the thl class of things we think we might try to do in a year or two ? <EOS> this is this is everything that that , um , <breath> you know , um we might want to do in the next couple years . <EOS>
so noisy - ors are a way to , uh , <breath> sort of deal with this . <EOS> i think we definitely i think it 's a great idea tha to to pursue that . <EOS>
and , um and now it 's we have a complete english parser that does everything the german parser does . <EOS> i think each of those functions act on the current xml structure , and change it in some way , for example , by adding a a l a field to it , or something . <EOS> there were other actions uh , that that s seemed to step state variables somewhere , it 's mystery functions . <EOS> that , these guys did in a in a day . <EOS>
although th the <breath> to get at them from a language may not be trivial . <EOS> so , it 's a relational database with persons , events , <breath> and , um , objects . <EOS>
and , that 's a lot of probabilities to put here , which is kind of a pain . <EOS>
so noisy - ors are a way to , uh , <breath> sort of deal with this . <EOS>
so , if ba - javabayes wo n't do it for you , and then but , the combination functions , and whether we can put those in java bayes , and all that sort of stuff , is , uh is the bigger deal . <EOS>
there 's this whole framework problem that i 'm feeling really uncomfortable about . <EOS>
so they 'll have a little bit more natural interaction ? <EOS>
and um we have a little description of asking peop subjects to contact fey for you know recruiting them for our thing however there is always more people in a in a facul uh in a department than are just taking his class or anybody else 's class at the moment and then we 're going to have another we 're gon na have w another trial run <EOS>
um , the basic requirement is fulfilled almost . <EOS> you can speak into it and ask for tv and movie information <EOS>
none <EOS>
then uh on to the modeling . <EOS> in the future though , the content of a hypothesis will not only be an object and an an action and a domain object but an action , a domain object , and a rich action description , <EOS>
inside of enter there will be roles that can be filled basically . <EOS>
the idea is , so , imagine we have a library of schema such as the source - path - goal well one of the types of action schemas is source - path - goal action . <EOS>
the idea is , so , imagine we have a library of schema so if you wanted to have a new type of action you 'd create a new type of category . <EOS>
meaning we can reference . <EOS> and link it to another one , and this not only within a document but also via documents , <EOS>
and then those actions can be in multiple categories at the same time if necessary . <EOS> it 's it 's crucially necessary , is that we can have multiple schemas and multiple action schemas in parallel . <EOS>
um and i agree that you know this is something we need to discuss , <EOS>
um , so we 're about to collect data and um we have a little description of asking peop subjects to contact fey for you know recruiting them for our thing <EOS>
however there is always more people in a in a facul uh in a department than are just taking his class or anybody else 's class at the moment <EOS>
there the idea is now that next actually we we need to hire one more person to actually do that job <EOS>
meeting time rescheduling . <EOS> i n did n't you say something about friday , <EOS>
so i mean clearly there 's i can talk about the um the parser changes on friday at least , <EOS>
and and this is on on a on on my list of things until next next week . <EOS> yeah , we i will promise for the next time to have fleshed out n xml examples for a a run through and and see how this this then translates , <EOS>
and um if you can get that binding point also maybe with a nice example that would be helpful for johno and me . <EOS>
um and then , secondly , we had , you may remember , um the problem with the re - phrasing , that subject always re - phrase sort of the task that uh we gave them , <EOS>
however , not only was the common census were among all participants of friday 's meeting was it 's gon na be very laborious to to make these drawings for each different things , so all of a sudden we 'll get descriptions of pictures in there . <EOS>
and the problem is , is that the current system does not distinguish between goes of type `` going into `` , goes of type `` want to go to a place where i can take a picture of `` , et cetera . <EOS> in the future though , the content of a hypothesis will not only be an object and an an action and a domain object but an action , a domain object , and a rich action description , <EOS>
well , i think we 're gon na hit a lot of interesting problems um and i agree that you know this is something we need to discuss , <EOS>
it 's also a question of the recursiveness and and a hier hierarchy um in there . <EOS>
and it 's gon na get more and more complex the the l complexer and larger our domains get . <EOS>
none <EOS>
one of which is just lay out the influence structure of what we think influences what <EOS>
the way we had been designing this , there were three intermediate nodes uh which were the endpoint decision as seen from the uh user model as seen from the ontology and as seen from the discourse . <EOS> these are all like saying ev or a , <EOS>
one of which is just lay out the influence structure of what we think influences what <EOS>
if it 's fixing things selling things , or servicing things so the idea would be that you might wan na merge those three you could have a node that 's that was a measure of the match between the object 's feature , you know , the match between the object the entity , i 'm sorry and the user . <EOS>
so here is the the we had that the user 's budget may influence the outcome of decisions . <EOS> well the fir see the first thing is , getting back to thing we left out of the other is the actual discourse . <EOS>
and we do n't know what they are yet . <EOS>
because uh we 're gon na want to know you know , which constructions indicate various of these properties <EOS>
so um recall the basic problem which is that um you have a belief - net and you have like a lot of different nodes all contributing to one node . <EOS> so the problem is to specify the uh so the conditional property of this given all those , <EOS>
and we just take the uh we essentially take um averages , this is a hidden variable . <EOS> w i was just , if you wanted to pay attention to more than one you could pass a w a weighting s system though too , <EOS>
ok , so they they could either be hand coded or learned or <EOS>
none <EOS>
yeah , i i mean , it might soon , if this is gon na be used in a serious way like java base then it might soon be necessary to uh start modifying it for our purposes . <EOS>
none <EOS>
so , i mean i know we 're going for sort of a rough and ready . <EOS>
none <EOS>
we have the user interest is a is a vector of five hundred values , that you 'd have to do see in order to do reference and stuff like that um you 've got ta have both the current discourse and the context to say i wan na go back there , <EOS>
and we do n't know what they are yet . <EOS>
but then the question is do you do we wan na propagate beliefs every single time it 's updated or only when we need to ? <EOS>
none <EOS>
ok , so they they could either be hand coded or learned or based on training data , like , are we able to get these nodes from the data ? <EOS>
yeah , i i mean , it might soon , if this is gon na be used in a serious way like java base then it might soon be necessary to uh start modifying it for our purposes . <EOS>
um so far i 've thought of it as sort of adding it onto the modeler knowledge module . <EOS> we talked about this several times that that the the input end is gon na need a fair amount of feedback from the planning end . <EOS> so we talked about the fact that there 're going to be a certain number of decisions that you want the knowledge modeler to make , that will be then fed to the function module , that does uh , route planning . <EOS>
and then that can be sort of developed uh as needed when we get enter the tourism domain . <EOS> we probably wo n't do this early on , because the current focus is more on the decision making and stuff like that . <EOS>
so anyt we 'll find a time later in the week to uh get together and talk about your understanding of what smartkom plans are . <EOS>
and they 're currently um agreeing or or in the process of agreeing on an x m l - ification of um something like a state - transition network of how dialogues would proceed . <EOS>
it oughta be called a a dialogue manager . <EOS>
no he 's completely gon na rewrite everything . <EOS> in java . <EOS> no , that 's gon na be phased out . <EOS>
and this one of these planning modules comes along and says `` hey , right now we need to ask a question `` . <EOS> so that forces the dialogue manager to change state . <EOS>
and and the the underlying idea of course is that there is something like kernel modules with kernel functionality that you can plug uh certain applications like tourist information or um the home scenario with uh controlling a vcr and so on . <EOS> and keep these things like uh tourist information external . <EOS> that 's an additional reason to have this well - defined interface so if you can do this , then the notion would be that across a very large range of domains , you could use this deep conceptual basis as the interface . <EOS>
um it 's to to integrate and syntactic analysis . <EOS> but but uh given th the constraints , that you want it to be small and fast and so forth , my guess is you 're probably into some kind of chunk parsing . <EOS>
you know th the functional module that that interacts with with where the tourism g stuff is going probably is too restrictive . <EOS> um is based on slots that have to be filled and i 'm not sure if if complex slots of that type are really um being taken into consideration . <EOS>
i think the the true key issues is how does the whatever comes out of the language input pipeline look like and then what the action planner does with it <EOS>
we talked about this several times that that the the input end is gon na need a fair amount of feedback from the planning end . <EOS>
yeah i think just the the spatial planner and the route planner so a printout of the communication between those two fills up i do n't know how many pages <EOS>
yeah , uh the problem is th that it has to be very fast and they also have to be very robust . <EOS> cuz of um speech recognition errors but but uh given th the constraints , that you want it to be small and fast and so forth , my guess is you 're probably into some kind of chunk parsing . <EOS> some extensions uh have to be made . <EOS> for for a english version <EOS>
but the other half of the problem is how would you get that kind of information from the parsed input ? <EOS>
none <EOS>
none <EOS>
right now it 's brittle and you need to ch start it up and then make ts twenty changes on on on on seventeen modules before they actually can stomach it , anything . <EOS> we can even make a sort of an internal demo , um . <EOS> <outbreath> well it was just amazing to to see uh how how instable the whole thing is , <EOS>
n no , to just get the dem get the demos they need . <EOS> so th the demo the demo requirements for this fall are sort of taken care of as of later this week or something . <EOS>
but , more focused on uh an idealized version than just getting the demo out . <EOS>
and so these issues about uh , reference , and spatial reference , discourse reference , uh - uh - uh - uh all this sort of stuff , uh , deixis which is part of what you were talking about , so we got ta do all this . <EOS>
on this scale , you have it either be ego or allocentric . <EOS> so in addition to e ego and allocentric uh which appear all over the place , you also apparently have this proximal - distal thing which is very deeply uh embedded . <EOS>
by the way , there something that i did n't know until about a week ago or so , is apparently , there are separate brain areas for things within reach , and things that are out of reach . <EOS>
so , i think as far as this group goes , um , it 's certainly worth continuing for the next few weeks to get closure on the uh belief - net and the ideas that are involved in that , and what are th what are the concepts . <EOS>
and we went through this , and , i think , more or less convinced ourselves that at least the vast majority of the nodes that we needed for the demo level we were thinking of , were in there . <EOS>
ok ? <EOS> what all you know , what are the considerations and how and what are the ways in which they relate . <EOS>
so `` ok , we 're not using their system . <EOS> that means we need our system . `` <EOS> but to do that we 're gon na need to make some decisions like ontology , i does uh either the uh smartkom project or one of the projects at eml have something that we can just p pull out , for that . <EOS>
so the idea is there 's this uh , other subgroup that 's worrying about formalizing the nota getting a notation . <EOS>
oh there 's yet another one of the incoming first incoming first - year graduate students who 's expressed interest , and actually i talked today to a uh undergraduate who wants to do an honors thesis on this . <EOS> and , while we 're at this level , uh , there 's at least one new doctoral student in computer science who will be joining the project , either next week or the first of august , depending on the blandishments of microsoft . <EOS>
so , c sh we could set that up as actually an institute wide thing ? <EOS> well d we we do wan na have all the bugs out b where you have to sort of pipe in extra xml messages from left and right before you 're <EOS>
so i 'd like to , for the summer turn into science mode . <EOS> but , more focused on uh an idealized version than just getting the demo out . <EOS> uh . <EOS> i mean there are a lot of issues , what 's the ontology look like , you know what do the constructions look like , what 's the execution engine look like , <EOS>
so `` ok , we 're not using their system . <EOS> that means we need our system . `` <EOS> and so , uh , in addition to the business about just getting the linguistics right , and the formalism and stuff , we 're actually gon na build something <EOS>
cuz we 're we 're not only going the plan is not only to lay out this thing , but to actually uh build some of it . <EOS> it looks like we 're now in a position that the construction analyzer that we want for this applied project can be the same as the construction analyzer that nancy needs for the child language modeling . <EOS>
and one of the things we need to do is the um , and this i think is relatively tight tightly constrained , is to finish up this belief - net stuff . <EOS> so , i think as far as this group goes , um , it 's certainly worth continuing for the next few weeks to get closure on the uh belief - net and the ideas that are involved in that , and what are th what are the concepts . <EOS>
so uh what we 're gon na do initially is is do design , and , i if you will , guess . <EOS> but for the limited amount of stuff we have for this particular exercise i think we 'll just design it . <EOS>
none <EOS>
and um , part of my responsibility is uh to use our internal `` group - ware `` server at eml , so that whatever we discuss in terms of parsing and and generating and constructions w we we sort of uh put it in there and they put what they do in there and maybe we can even um , get some overlap , get some synergy out of that . <EOS>
and so anyway we c uh we can m undoubtedly get ami uh to give a talk at uh eml or something like that . <EOS> while he 's in in uh a lot of interest . <EOS> actually , either place , dfki or uh <EOS>
e the version that is , the full version that 's on the server d does not work . <EOS>
right now it 's brittle and you need to ch start it up and then make ts twenty changes on on on on seventeen modules before they actually can stomach it , anything . <EOS> because it 's designed for this seevit thing , where you have the gestural recognition running with this s siemens virtual touch screen , which we do n't have here . <EOS> but it 's working now , and i g i got the feeling that we are the only ones right now who have a running system . <EOS> um . <EOS> <outbreath> well it was just amazing to to see uh how how instable the whole thing is , <EOS>
well d we we do wan na have all the bugs out b where you have to sort of pipe in extra xml messages from left and right before you 're <EOS>
none <EOS>
so uh what we 're gon na do initially is is do design , and , i if you will , guess . <EOS> but for the limited amount of stuff we have for this particular exercise i think we 'll just design it . <EOS>
javabayes does not support that . <EOS> i i also s would suggest not to d spend two weeks in in in changing the the javabayes code . <EOS>
uh one of them is to work on the semantics of the belief - net which is going to be the main inference engine for thi the system uh making decisions . <EOS> and we 're also , sort of uh in the same process , going to work with fey on what there should be in the dialogues . <EOS>
and um and fey has some thirty subjects lined up ? <EOS> so we yeah we do n't know how many we can get next door at the uh shelter for example . <EOS>
and um and then they gon na have to f um um choose from one of these tasks , which are listed here . <EOS> so if if it 's one service , one luxury item , you know , one big - ish place , and so forth and so on , um then my guess is that that the data is going to be easier to handle . <EOS> that w maybe one thing we should do is go through this list and sort of select things that are categories and then o offer only one member of that category ? <EOS>
i b my guess is it 's gon na be ten . <EOS>
none <EOS>
and decisions are going to turn out to be parameter choices for calls on other modules . <EOS>
and that we can actually infer them to a significant de degree , or we ask . <EOS> or eh so , y but there 's th but definitely a back - off position to asking . <EOS>
ok , because uh we do wan na get them r u perfectly but i think we 're gon na have to do a first cut at a lot of them to see how they interact . <EOS>
we are expecting johno to build a parser , uh he 's g he 's hoping to do this for his masters ' thesis s by a year from now . <EOS> uh limited . <EOS> well , the hope is that the parser itself is , uh , pretty robust . <EOS>
maybe i suggest we make some fine tuning of these , get sort of run through ten or so subjects um but anyway yeah , so i think it 's a good idea to start with the sort of relatively straight forward res just response system . <EOS>
and um and then they gon na have to f um um choose from one of these tasks , which are listed here . <EOS> that w maybe one thing we should do is go through this list and sort of select things that are categories and then o offer only one member of that category ? <EOS> like at you know , `` attend a theater , symphony or opera `` is is a group , and `` tour the university , castle or zoo `` , <EOS>
so trying to im um implant the intention of going to a place now , going to a place later on the same tour , and see whether we wan na make it more complex or not , depending on what what sort of results we 're getting . <EOS> um i i would say maybe two weeks . <EOS> let 's plan next monday , ok , to have a review of what we have so far . <EOS> this means audio , but but it would be great if you could , um , not transcribe it all , but pick out uh , some stuff . <EOS> and then if we want to uh get them to start doing uh multiple step planning with a whole bunch of things and then organize them tell them which things are near each other and we 're gon na start thinking a about uh what constructions we want to elicit . <EOS>
see this this this uh ontology node is probably something that i will try to expand . <EOS> and hopefully you can sort of also try to find out , you know , sooner or later in the course of the summer what we can expect to get from the discourse that might , you know or the <EOS>
uh robert and eva and bhaskara are gon na actually build a belief - net that that , um , has cpt 's and , you know , tries to infer this from various kinds of information . <EOS>
ok , because uh we do wan na get them r u perfectly but i think we 're gon na have to do a first cut at a lot of them to see how they interact . <EOS>
whatever you need in order to uh , be able to then , uh , by hand , you know , explain , some fraction of the utterances . <EOS>
so there 's a lot of things where we have no analogous tasks , and that may or may not be a problem . <EOS>
and these are the data tasks where w we can assume the person would like to enter , view or just approach the thing . <EOS> now of course you have this i guess possible danger that somehow there 're certain constructions that people use uh when talking about a museum that they would n't talk about with a university and stuff , <EOS>
none <EOS>
and um we 're still l looking for a room on the sixth floor because they stole away that conference room . <EOS> um behind our backs . <EOS> david and and jane and and lila are working on that as we speak . <EOS>
none <EOS>
and i i me it would it would be completely out of the question to really do more than , say , like , oh i do n't know , ten , over the summer , <EOS>
none <EOS>
that 's basically just specifying the the input for the w what 's it 's based on things like , uh , there 's gon na be a node for go - there or not , and there 's gon na be a node for enter , view , approach . <EOS>
and some some things will always be sort of too not significant enough . <EOS>
cuz , that 's what i mean , in the bayes - net you always ask for the posterior probability of a specific node . <EOS>
i mean , maybe it does make a difference in terms of performance , computational time . <EOS> so basically , you 'd have a decision tree query , go - there . <EOS> and just basically do a binary search through the ? <EOS>
you wo n't it 'll be hard to decide . <EOS>
and for the where - is construction , we know we need to l look at this node , that merges these three things together so my i so , if we were to it with a bayes - net , we 'd have to have a node for every question that we knew how to deal with , every construction . <EOS>
and , um , finish up this bayes - net . <EOS> and we present our results , <EOS>
ok . <EOS> because then , once we have it sort of up and running , then we can start you know , defining the interfaces and then hook it up to some fake construction parser <EOS>
every construction . <EOS>
because there are interdependencies , <EOS>
sometime this week again and finish up the , uh , values of this . <EOS>
and , um , finish up this bayes - net . <EOS> and we present our results , <EOS>
as , if i understand it correctly , it always gives you all the posterior probabilities for all the values of all decision nodes . <EOS> so , it n i mean a all i 'm saying is , whatever our input is , we 're always gon na get the full output . <EOS> what what what i am thinking , or what we 're about to propose here is we 're always gon na get the whole list of values and their posterior probabilities . <EOS> cuz , that 's what i mean , in the bayes - net you always ask for the posterior probability of a specific node . <EOS>
and the question is what to do with it , and now we need an expert system or belief - net or something that interprets that , well , eventually , you still have to pick out which ones you look at . <EOS>
so the person said , um , `` where is x ? `` <EOS> we want to know , um , is does he want info ? <EOS> or does he want to go there ? <EOS>
none <EOS>
yeah , i can worry about the ontology interface and you can keith can worry about the discourse . <EOS>
also , i 'm somewhat boggled by that hugin software . <EOS> i ca n't figure out how to get the probabilities into it . <EOS>
and the other bit of news is we had you know , uh , i was visited by my german project manager and he came up we came up with a pretty strange idea . <EOS> it should be possible to make that system produce questions . <EOS>
but maybe one could do some learning . <EOS>
the basic idea i guess would be to give allow the system to have intentions , basically ? <EOS> well you can observe some user and context stuff and ask , what 's the posterior probabilities of all of our decision nodes . <EOS>
i mean we just i mean it would n't hurt to write up a paper , well , i i also think that if we sort of write about what we have done in the past six months , we we we could sort of craft a nice little paper that if it gets rejected , which could happen , does n't hurt and then we can say , uh well what we do is this . <EOS>
so this will be sort of documenting what we think , and documenting what we have in terms of the bayes - net stuff . <EOS> well , in the moment it 's a bayes - net . <EOS> and it has sort of fifty not - yet - specified interfaces . <EOS>
the sudo - square < writing on whiteboard > is , < three-syllable laugh > `` situation `` , `` user `` , `` discourse `` , right ? `` <EOS> ontology `` . <EOS>
and johno coming up with the idea that if the person discussed the discussed the admission fee , in eh previously , that might be a good indication that , `` how do i get to the castle ? `` <EOS> , actually he wants to enter . <EOS>
and specify um , what what we think the the output uh , observe , out i input nodes for our bayes - nets for the sub sub - d , for the discourse bit , should be . <EOS> so we want to sort of come up with what gets uh , input , and how inter in case of a `` where is `` question . <EOS> so that we actually end up with um , um , nodes for the discourse and ontology so that we can put them into our bayes - net , <EOS>
and we can run our better javabayes , and have it produce some output . <EOS>
none <EOS>
none <EOS>
look at the web page and let 's talk about it maybe tomorrow afternoon ? <EOS>
so this will be sort of documenting what we think , and documenting what we have in terms of the bayes - net stuff . <EOS>
and specify um , what what we think the the output uh , observe , out i input nodes for our bayes - nets for the sub sub - d , for the discourse bit , should be . <EOS> so we want to sort of come up with what gets uh , input , and how inter in case of a `` where is `` question . <EOS> so that we actually end up with um , um , nodes for the discourse and ontology <EOS>
wait , so do , or do not take other kinds of constructions into account ? <EOS> well , if you if you can , oh definitely do , <EOS>
e i 'm sort of have the impression that getting it to say the right thing in the right circumstances is much more difficult than getting it to understand something given the circumstances and so on , it 's not the same as the understanding . <EOS>
just the fact that we 'll get the point is that getting it to understand one construction does n't mean that it will n always know exactly when it 's correct to use that construction . <EOS> right ? <EOS>
but maybe one could do some learning . <EOS> yeah , it 's g anyway , the point is that given all of these different factors , it 's uh e it 's it 's still going to be impossible to run through all of the possible situations or whatever . <EOS>
i mean , it 's obvious that we ca n't do any kind of evaluation , <EOS>
like introducing the formalism might be not really possible in detail , <EOS>
none <EOS>
none <EOS>
but normal statements that seem completely unambiguous , such as `` where is the blah - blah `` , actually are terribly complex , and completely ambiguous . <EOS>
and eva is using the xalan style sheet processor to convert the xml that 's output by the java bayes for the into the , uh , e bayes input . <EOS>
yep . <EOS> we ha we have to change the voice . <EOS>
uh , w which is mental spaces and uh and - or but the other part of it is the way they connect to these , uh , probabilistic relational models . <EOS>
so there probably are some , uh , relatively clean rules , is that people do manage to do this <EOS>
no , i know , i th i i think that is gon na be sort of the key to this wh to th the big project of the summer of of getting the constructions right <EOS>
which is the issue of , um , how do you simulate questions ? <EOS> we did n't we never did figure out how we were gon na do emphasis in in uh , the semspec . <EOS> incl including the questions uh , no , all the focus stuff . <EOS> and we 'll figure out exactly how to write that up and so on , <EOS>
the question of whether the polysemy is sort of like in the construction or pragmatic . <EOS> well the question is basically , is this conventional or conversational implicature ? <EOS>
w we know for sure that we have to be able to do both . <EOS> i mean it th <inbreath> i can thi i can think of arguments in either direction on that . <EOS>
right . <EOS> so . <EOS> <laugh> right . <EOS> so thing that 's part of why we want the formalism , <EOS>
none <EOS>
priming a spreading activation <EOS>
which uh , so far , in terms of like putting up all the constraints as , you know , pushing them into type constraints , the when i 've , you know , propo then proposed it to linguists who have n't yet given me you know , we have n't yet thought of a reason that that would n't work . <EOS> right ? <EOS> as long as we allow our type constraints to be reasonably complex . <EOS>
actually , maybe i could try , like , emailing the guy and see if he has any something already . <EOS>
so i just need to do the , uh write a new set of tree combining rules . <EOS> and fey has foolheartedly agreed to rewrite uh , the german concept uh syntax - to - prosody rules <EOS>
ogi has , uh , crafted a couple of diphone type voices that are really nice <EOS>
anyway , uh , that we were that we 're gon na try to get a uh , first cut at the revised formalism by the end of next week . <EOS> so the idea is on monday at two we 'll we 'll see an intermediate version of the formalism for the constructions , <EOS>
uh , just trying to write up essentially what what you guys have worked out so that everybody has something to look at . <EOS> yeah . <EOS> well , if if i mean , i part of of what the exercise is , t by the end of next week , is to say what are the things that we just do n't have answers for yet . <EOS>
the question of whether the polysemy is sort of like in the construction or pragmatic . <EOS> well the question is basically , is this conventional or conversational implicature ? <EOS> w we know for sure that we have to be able to do both . <EOS> i mean it th <inbreath> i can thi i can think of arguments in either direction on that . <EOS>
that 's the lisp - type scheme . <EOS> well , i guess if you 're not used to functional programming , scheme can be completely incomprehensible . <EOS>
one of the things i w would like to do over the next , uh , month , it may take more , is to st understand to what extent we can not only figure out the constructions for them for multiple worlds uh sort of what the formalism will look like and where the slots and fillers will be , but also what that would translate into in terms of belief - net and the inferences . <EOS> one is the linguistic part of what are the couplings and and when you have a certain , uh , construction , that implies certain couplings and other couplings , and then we have this inference problem of exactly technically how does the belief - net work <EOS>
the question of whether the polysemy is sort of like in the construction or pragmatic . <EOS> the question is whether the construction is semantic or like ambiguous between asking for location and asking for path . <EOS> well the question is basically , is this conventional or conversational implicature ? <EOS> i mean it th <inbreath> i can thi i can think of arguments in either direction on that . <EOS>
so i a i i th i agree with you that , um , it 's a disaster to try to make separate constructions for every uh , pragmatic reading , <EOS>
although there are some that will need to be there . <EOS> so you could just as well tag the lexical construction with the fact that it 's a uh , you know , thirty percent increase in probability of entering . <EOS>
none <EOS>
uh , this is this is , speaking of hard problems , this is a very good time um , to start trying to make explicit where construal comes in and you know , where c where the construction per - se ends and where construal comes in , <EOS>
and so one on one side is on one side is a sort of the revised sort of updated semantic specification . <EOS> and the other side is , um , sort of a revised construction formalism . <EOS>
so they could all have a type at the beginning . <EOS> ok , and then again semantic constraints here are just are just bindings . <EOS>
but , you know , we have the properties of dependency grammars and some properties of constituents constituent - based grammar . <EOS>
i think that 's still in progress . <EOS> other things we did n't <breath> totally deal with , um , well , we 've had a lot of other stuff that keith and i have them working on in terms of like how you deal with like an adjective . <EOS> you know , a a nominal expression . <EOS>
so there 's going to be some extra you know , definitely other notation we 'll need for that which we skip for now . <EOS>
so you see it 's `` scenario `` , `` referent `` and `` discourse segment `` . <EOS> the `` scenario `` box , <EOS>
and actually it 's just a list of various slots from which you would draw draw in order to paint your picture , a bunch of frames , bi and bindings , right ? <EOS>
which is made up entirely of these things and , uh , bindings among them . <EOS> and bindings to ontology items . <EOS> so that that the who that this is the tool kit under whi out of which you can make a semantic specification . <EOS> so this is an that anything you have , in the party line , anything you have as the semantic side of constructions comes , from pieces of this ignoring li but it 's got to be pieces of this along with constraints among them . <EOS>
yeah , but you do n't we ultimately want to handle that analogously to the way we handle time and place , um , we might be able to handle context in the same way that we handle mental spaces so that pulling something out of a discourse context is i think similar to other kinds of , uh , mental space phenomena . <EOS>
um , and we 're going to have to s sort of bound the complexity . <EOS> but just try to describe which ones you think we ought to have . <EOS> just just sort of , you know , define your space . <EOS>
um , imagine you you write a bayes - net , um , completely from scratch every time you do construal . <EOS> and that fills in your cpt 's with which you can then query , um , the the net that you just wrote and find out how thing x is construed as an utterance u . <EOS> you may have some general rules as to how things can be can be construed as what , so that will allow you to craft the the the initial notes . <EOS>
none <EOS>
except for `` cause `` , that i forgot . <EOS> causal stuff we absolutely need . <EOS>
well we were just talking about this sort of evidentiality and stuff like that , right ? <EOS> confidence or something like that . <EOS>
um , it 's often thought of as it is also considered a mental space , same thing . <EOS> yeah , but you do n't we ultimately want to handle that analogously to the way we handle time and place , um , we might be able to handle context in the same way that we handle mental spaces it 's like it 's like what 's happening that , yeah , what what 's happening , uh , there is that you 're moving the base space or something like that , right ? <EOS>
um , and we 're going to have to s sort of bound the complexity . <EOS> but just try to describe which ones you think we ought to have . <EOS> just just sort of , you know , define your space . <EOS>
we we do n't have to care too much about the speaker attitude , right ? <EOS> so so we 're gon na get the w we 're basically dealing with two domains , the tourist domain and the and the child language learning . <EOS>
is this , uh was it intentional to leave off things like `` inherits `` i did n't want to think too much about that for for now . <EOS> can we make it more elegant ? `` <EOS>
um , imagine you you write a bayes - net , um , completely from scratch every time you do construal . <EOS> but the question is do you want to , for example , send the little group , uh , a draft of your thesis proposal we can do it th - thursday again . <EOS>
and so it was an issue whether constraints um , there were some linguists who reacted against `` constraints `` , saying , `` oh , if it 's not used for matching , then it should n't be called a constraint `` . <EOS> the whole the mental space thing is clearly not here . <EOS>
you know , um , basically all of these so - called space builders that are in the sentence are going to sort of assuming that at any point in discourse there 's the possibility that we could be sort of talking about a bunch of different world scenarios , whatever , and the speaker 's supposed to be keeping track of those . <EOS> the , um the construction that you actually get is just gon na sort of give you a cue as to which one of those that you 've already got going , um , you 're supposed to add structure to . <EOS> but it does n't tell you like exactly what it what the point of doing so is . <EOS>
none <EOS>
ok , it 's like uh , it 's not it might be that there 's a syntactic , uh , device that you use to indicate focus uh , so so i think that 's kind of nice to keep `` focus `` being an information structure term . <EOS> i think that 's still in progress . <EOS> but it did one one implication it does f have for the other side , which we 'll get to in a minute is that i could n't think of a good way to say `` here are the possible things that you could focus on `` , but i i ca n't think of like the limited set of possible meanings that you would that you would focu maybe you want to forget stress . <EOS> canonically speaking you can if you look at a a curve over sentence , you can find out where a certain stress is and say , `` hey , that 's my focus exponent . `` <EOS> it does n't tell you anything what the focus is . <EOS>
except for `` cause `` , that i forgot . <EOS> causal stuff we absolutely need . <EOS> mental space we need . <EOS> and and so we 're , uh gon na have to , um , chain those as well . <EOS> it 's as far as i can tell there 's this one major thing we have to do which is the mental the whole s mental space thing . <EOS>
none <EOS>
then i 'm also going to present a little talk at eml , about what we have done here let 's talk about your thesis proposal . <EOS>
then i 'm also going to present a little talk at eml , about what we have done here then i 'm going to talk about the data , <EOS>
e tell a little bit as much as i can about the ntl story . <EOS> and then maybe talk about the big picture here , so x - schemas , then , i would like to do and then at the end about our bayes - net . <EOS>
it basically says , well `` this is construal `` , and then it continues to say that one could potentially build a probabilistic relational model that has some general , domain - general rules and then the idea is to use ontology , situation , user , and discourse model to instantiate elements in the classes of the probabilistic relational model to do some inferences in terms of what is being construed as what <EOS>
none <EOS>
and to make them apply to metaphorical cases and metonymic cases and all those things , requires this additional mechanism , of construal . <EOS> but cover only base cases . <EOS> but there are basic cases . <EOS> w well we have , for example , a canonical use of something and y it 's , you know , we have some constructions and then it 's construed as something , and then we we may get the same constructions with a metaphorical use that 's also relevant to the to the domain . <EOS>
but `` walked into the cafe and ordered a drink , `` and `` walked into the cafe and broke his nose , `` but `` run into `` does . <EOS> so `` in the bus `` and `` on the bus , `` we had we had initially we 'd started discussing the `` out of film . `` <EOS> where is the castle ? <EOS> i mean maybe the `` where is something `` question as a whole , you know , can be construed as , u i locational versus instructional request . <EOS>
but i think the argument should be uh , can be made that , you know , despite the fact that this is not the most met metaphorical domain , <EOS>
e tell a little bit as much as i can about the ntl story . <EOS> and then maybe talk about the big picture here , oh , maybe the fmri stuff . <EOS> well , the time to mention it , if you mention it , is when you talk about mirror neurons , <EOS>
the b the combination of the biology and the leipzig connection might be interesting to these guys , <EOS>
there the there we 've got various i generations of slides that show language analysis , and matching to the underlying image schemas , <EOS>
ok . <EOS> so why do n't we plan to give you feedback electronically . <EOS>
current formalism thing that you presented . <EOS>
i i it looks like the , uh , metaphor did n't get in yet . <EOS> but cover only base cases . <EOS> and to make them apply to metaphorical cases and metonymic cases and all those things , requires this additional mechanism , of construal . <EOS> w well we have , for example , a canonical use of something and y it 's , you know , we have some constructions and then it 's construed as something , and then we we may get the same constructions with a metaphorical use that 's also relevant to the to the domain . <EOS>
none <EOS>
but i think the argument should be uh , can be made that , you know , despite the fact that this is not the most met metaphorical domain , <EOS>
so `` in the bus `` and `` on the bus , `` we had we had initially we 'd started discussing the `` out of film . `` <EOS> ok . <EOS> can we think of a nice metaphorical use of `` where `` in the tourist 's domain ? <EOS> a fixed expression , yeah . <EOS>
none <EOS>
um , and we found another uh , cogsci student who 's interested in playing wizard for us . <EOS> here we 're gon na make it a little bit more complicated for the subjects , uh this round . <EOS>
as for smartkom , i 'm the last smartkom meeting i mentioned that we have some problems with the synthesis , which as of this morning should be resolved . <EOS> so maybe uh uh , when tomorrow is over , we 're done . <EOS>
something happened , in on eva 's side with the prm that we 're gon na look at today , <EOS>
i sorta constructed a couple of classes . <EOS> the red lines on the , um , graph are the um , relations between the different um , classes . <EOS> this is more or less similar to the flat bayes - net that i have , you know , with the input nodes and all that . <EOS>
ok . <EOS> so it only makes two decisions , in this model . <EOS> and one is basically how desirable a site is and the other is the mode of the visit , whether th it 's the eva decision . <EOS>
i mean , the notion of instantiating your el elements from the ontology and stuff fits this very nicely and does n't fit very well into the extended belief - net . <EOS>
we have a visitor from bruchsal from the international university . <EOS> good . <EOS> then , we can move on and see what andreas has got out his sleeve . <EOS>
and so , what i want to build is basically a a smart f a q system . <EOS> so , i want to be able to model information like , um , so in the in the context of in the context of developing distributed systems , of a at a computer science school , uh , i want to build a smart librarian , basically <EOS>
now , what you uh need to do here is you need to provide some context information now , what i plan to do is i want to uh sort of do a uh uh try to improve the quality of the search results , <EOS>
and i 'm guessing that you you wo n't be doing that ? <EOS> no . <EOS> on the other hand , uh , framenet could well be useful . <EOS>
the other person i thought of is dan gildea ? <EOS>
uh in a in a smaller group we had uh , talked and decided about continuation of the data collection . <EOS> here we 're gon na make it a little bit more complicated for the subjects , uh this round . <EOS>
she 's actually suggested to look um , at the psychology department students , because they have to partake in two experiments in order to fulfill some requirements . <EOS>
uh , so one of the things that eva 's gon na do over the next few weeks is see if we can track that down . <EOS>
uh , we are n't gon na build our own interpreter , the people at stanford write papers as if they had one , <EOS>
and supposedly , um , we 'll actually get s deep seriously connected to to their work somebody 'll uh , you know if it 's a group meeting once a week probably someone 'll go down and , whatever . <EOS>
and it turns out , you know , the cpt 's are really big , if i do that , <EOS>
but i do n't remember reading how you specify i remember them learning when , you know , you do n't know the structure for sure , <EOS>
the you you did n't look at all yet to see if there 's anybody has a implementation . <EOS> so w anyway . <EOS> so that 's a a major open issue . <EOS>
uh , we are n't gon na build our own interpreter , uh , so one of the things that eva 's gon na do over the next few weeks is see if we can track that down . <EOS>
now , what i plan to do is i want to uh sort of do a uh uh try to improve the quality of the search results , <EOS>
now , the big problem that i 'm facing right now is um , it 's fairly easy to hack up a system uh quickly , that that works in the small domain , but the problem is obviously the scalability . <EOS> cuz it 's it 's very easy to whip up something quickly , <EOS>
as for smartkom , i 'm the last smartkom meeting i mentioned that we have some problems with the synthesis , <EOS>
topic of this meeting is i wan na talk a little bit about transcription . <EOS> so , well w shall we move on and talk a little bit about transcription then ? <EOS> is and so one of the things was to get an estimate of how long it would take , and then also what tools we would use . <EOS> and so the next decision which has to be made actually pretty soon is how are we gon na do it ? <EOS> i mean , i looked at cyber transcriber <EOS>
what we 're using right now is a tool , um , from this french group , called `` transcriber `` so as far as i 'm concerned those transcription conventions are fixed right now . <EOS> and but so in terms of the con the conventions , then , <breath> uh , basically , <breath> uh , it 's strictly orthographic so i have i have a convention of putting like a dash <breath> arrow just to indicate that this person 's utterance continues . <EOS>
these are linguistics grad students . <EOS> but we can pay a graduate student seven dollars an hour . <EOS>
so that means that even if it takes them thirty times real time it 's cheaper to to do graduate students . <EOS> i mean , that 's why i said originally , that i could n't imagine sending it out 's gon na be cheaper . <EOS>
maybe we should s consider also , um , starting to build up a web site around all of these things . <EOS>
and then get an update on the electronics , um , let 's move on to electronics . <EOS>
cuz they 're distributing it through the ldc . <EOS> maybe we should maybe we should get a copy of it just to see what they did so so that we can we can compare . <EOS> csae . <EOS> corpus of spoken american english . <EOS>
so i mean , i 've been sort of playing with , uh , different ways of mar cuz i 'm thinking , you know , i mean , if you could get optimal instructions you could cut back on the number of hours it would take . <EOS> so i think though it 's a good proposal to be used on a new a new batch of text that i have n't yet done yet in the same meeting . <EOS> could use it on the next segment of the text . <EOS> i could generate the segmentation and and you could do the words , and time yourself on it . <EOS>
so what we wanted to do was have jane do basically one meeting 's worth , you know , forty minutes to an hour , <EOS>
i hope it 's jane . <EOS> yeah , no , that i i would be interested in that in becoming involved in the project in some aspect like that <EOS>
we could program that pretty easily , are are those d delays adjustable ? <EOS> could you get it so that with so it would it would detect volume on a channel and insert a marker ? <EOS> sure . <EOS> so if we want if we did if we did something like programmed in a delay , which actually i think is a great idea , um , i 'm sure they would want that incorporated back in . <EOS> and i said , well , maybe . <EOS> and they have they 've actually asked if we are willing to do any development well , so anyway , are we interested then in writing tools to try to generate any of this stuff automatically ? <EOS>
maybe we should s consider also , um , starting to build up a web site around all of these things . <EOS> i think mostly internal . <EOS> we could do an ht access which would accommodate those things . <EOS>
well i can build a cabinet . <EOS>
which is to say just laptop with a wireless . <EOS> so we certainly could use that as as a constant reminder of what the vu meters are doing . <EOS>
and and so how we do we distribute the transcripts , how do we distribute the audio files , but but so should we do it in the same format as ldc <EOS>
they 're never gon na be able to do a meeting like this . <EOS> and so so what i 'm saying is that if we hire an external service i think we can expect three hundred dollars an hour . <EOS> who knows if they 're gon na be able to m manage multal multiple channel data ? <EOS> they wo n't . <EOS> i mean , that 's why i said originally , that i could n't imagine sending it out 's gon na be cheaper . <EOS>
none <EOS>
there 's some interesting human factors problems like , <breath> yeah , what span of of time is it useful to segment the thing into in order to uh , transcribe it the most quickly . <EOS>
none <EOS>
um , the user interface only allows two . <EOS> and so if if you 're using their interface to specify overlapping speakers you can only do two . <EOS> it 's just there 's no user interface for specifying multiple any more than two . <EOS>
no , but w certainly one of the issues is is the , uh is security . <EOS>
um , so um the one th one thing i know that we have on that is uh we had talked a a couple weeks before um uh about the uh the stuff you were doing with with uh um uh l l attempting to locate events , but anyway some some potential collaboration there about about the about the working with these data . <EOS> um , so , uh , he was interested in the question of you know , relating to his to the research he presented recently , um of inference structures , so um we were trying to think of ways that his interests could interact with ours he 's interested in these these knowledge structures , inferences that you draw i from to to find a a good solution to detect eh , the overlapping zone in eh speech recorded . <EOS> but you have you have time uh , uh marked twelve minute the the the um overlaps in twelve minutes of it . <EOS> like very straightforward question is where we are on the amount of data and the amount of transcribed data , <EOS>
but there 's at least one meeting recorded of uh the uh uh natural language guys . <EOS> uh , we 've started having a morning meeting , today uh i starting a w a week or two ago , on the uh front - end issues , and we 're recording those , uh there 's a network services and applications group here who 's agreed to have their meetings recorded , if anyone knows of one more m or two more wee meetings per week that happen at icsi , um that we could record , i think it would be worth it . <EOS>
so , we need to talk about this later . <EOS>
on - only to mark only to mark overlapping zone , but then i think it 's a good idea . <EOS> but i mean we if uh if he could speed up what he was doing by just getting the speaker overlaps so that we had it , say , for forty - five minutes , then at least we 'd have three hundred examples of it . <EOS>
yeah , whoever we have working on the acoustics for the meeting recorder are gon na start with that . <EOS>
if anyone knows of one more m or two more wee meetings per week that happen at icsi , um that we could record , i think it would be worth it . <EOS>
i mean , one of the things i wanted to do , uh , that i i talked to to don about , is one of the possible things he could do or m also , we could have someone else do it , is to do block echo cancellation , so that 's something i 'd like somebody to do at some point , just take these digits , take the far - field mike signal , and the close uh mike signal , and apply really good echo cancellation . <EOS> somebody look and and the digits would be a reasonable thing to do that with . <EOS>
so this is this is this is gon na be a big , big problem if you want to later do uh , you know , indexing , or speech understanding of any sort . <EOS>
and in fact , it is it is it is sensitive . <EOS> you know , i asked her very specifically about this clause of how , um , you know , it says `` no individuals will be identified <EOS>
i mean , if i 'm tapping on the table , you it 's not gon na show up on any of the mikes , but it 's gon na show up rather loudly in the pzm . <EOS> the the the far - field , because i i found a difference . <EOS>
one of the things that i think is a little a little bit of a limitation , there is a think when the people are not involved uh in our work , we probably ca n't do it every week . <EOS> so , i think it 's gon na be a problem to get people regularly . <EOS>
um , but i 'd rather try to get more regular meetings of types that we know about , and hear , then sort of a mish - mosh of a bunch of one one - time <EOS>
and in that uh , regard , i thought we definitely w will need it 'd b it 'd be nice for us to have a bunch of data from a few different domains , or a few different kinds of meetings . <EOS> um , but i 'd rather try to get more regular meetings of types that we know about , and hear , then sort of a mish - mosh of a bunch of one one - time but also data where we hold some parameters constant or fairly similar , uh , for other kinds of research , particularly the acoustic oriented research , i actually feel the opposite need . <EOS> i 'd like to have many different speakers . <EOS> so , um i think i would also very much like us to have a fair amount of really random scattered meetings , of somebody coming down from campus , and and uh , i 'd love to get people that are not linguists or engineers , cuz these are both weird <EOS>
i have , eh , < clears throat > the result of my work during the last days . <EOS> it 's a real problem , a frequently problem uh , because you have overlapping zones eh , eh , eh , all the time . <EOS> but , eh , my idea is , eh , is very interesting to to work in in the line of , eh , automatic segmenter . <EOS> no , i i plan to do that . <EOS> now , <mouth> eh , i need ehm , <mouth> to detect eh all the overlapping zones exactly . <EOS> so the first thing is for you to to build up something that will detect the overlaps . <EOS>
i would take just a few features . <EOS> instead of taking all the mfcc 's , or all the plp 's or whatever , i would just take a couple . <EOS> uh , and uh on the other hand , the lpc residual , the energy in the lpc residual , <breath> will say how well , uh <breath> the low - order lpc <breath> model 's fitting it , which should be <breath> pretty poorly for two two or more <breath> people speaking at the same time , and it should be pretty well , for w for for one . <EOS> well , that that that 's another reason why very simple features , things like energy , and things things like harmonicity , and <breath> residual energy are uh , yeah are are better to use than very complex ones because they 'll be more reliable . <EOS>
and <breath> then um , i guess another topic would be <breath> where are we in the whole disk resources question <EOS>
um and then also anonymity , how we want to anonymize the data . <EOS> the question becomes what symbol are you gon na put in there for everybody 's name , and whether you 're gon na put it in the text where he says `` hey roger `` or are we gon na put that person 's anonymized name in instead ? <EOS> because if we made the the transcript be the tag that we 're using for roger , someone who had the transcript and the audio would then have a mapping between the anonymized name and the real name , and we wan na avoid that . <EOS> um , how important is it for a person to be identified by first name versus full name ? <EOS> however , taking one step further back , they 'd be identifiable anyway , even if we changed all the names . <EOS> on the other hand , this is a small this is a small pool , and people who say things about topic x e who are researchers and well - known in the field , they 'll be identifiable and simply from the from the first name . <EOS> now , it would be very possible for me to take those data put them in a in a study , and just change everybody 's name for the purpose of the publication . <EOS> and within that , it may be that it 's sufficient to not uh change the to not incorporate anonymization yet , but always , always in the publications we have to . <EOS>
well , i think that , um i think that the only thing we should say in the advertisement is that the meeting should be held in english . <EOS>
so i was i was thinking more in terms of talking to professors uh , and and and uh , senior uh , uh , d and uh , doctoral students who are leading projects and offering to them that they have their hold their meeting down here . <EOS> i mean , it it i think that if we 're aiming at at uh , groups of graduate students and professors and so forth who are talking about things together , and it 's from the berkeley campus , probably most of it will be ok , and my point in m in my note to liz was i think that undergrads are an iff iffy population . <EOS>
and in that uh , regard , i thought we definitely w will need it 'd b it 'd be nice for us to have a bunch of data from a few different domains , or a few different kinds of meetings . <EOS> i 'd like to have many different speakers . <EOS> so , um i think i would also very much like us to have a fair amount of really random scattered meetings , of somebody coming down from campus , and and uh , i 'd love to get people that are not linguists or engineers , cuz these are both weird <EOS>
none <EOS>
um , i had a i spoke with some people up at haas business school who volunteered . <EOS> should i pursue that ? <EOS> oh , definitely , yeah . <EOS>
the o the o the other the other thing is , uh , that we we talked about is give to them uh , burn an extra cd - rom . <EOS> we could burn it after it 's been cleared with the transcript stage . <EOS> so , after the transcript screening phase . <EOS>
if you have people who are using english as a as an interlanguage because they they do n't uh , they ca n't speak in their native languages and but their interlanguage is n't really a match to any existing , uh , language model , <EOS>
uh , i we realized in discussion that the other thing is , what about this business of distant and close microphones ? <EOS> if you 're talking about something simple , where you throw away a lot of these dimensions , then you can do that right away . <EOS>
then , uh , it seems to me that well , maybe i uh it seems to me that if you change the name , the transcript 's gon na disagree with the audio , and you wo n't be able to use that . <EOS> ok , well , but then there 's this issue of if we 're gon na use this for a discourse type of thing , then and , you know , liz was mentioning stuff in a previous meeting about gaze direction and who 's who 's the addressee and all , then to have `` roger `` be the thing in the utterance and then actually have the speaker identifier who was `` roger `` be `` frank `` , that 's going to be really confusing and make it pretty much useless for discourse analysis . <EOS>
well , the current one they do n't do speaker identity . <EOS> and so in their current conventions there are no multiple speaker conventions . <EOS>
and what we 're doing now is , aside from the many other differences in the task , we are considering overlap <EOS>
raw counts . <EOS> so . <EOS> of the times a person spoke and furthermore was involved in a two two - person overlap , <laugh> <inbreath> what percentage of the time were they the overlapper and what percent of the time were they th the overlappee ? <EOS> but , of course , uh i e <laugh> this is just one meeting , uh there 's no statistical testing involved , and that would be required for a for a finding of any kind of scientific reliability . <EOS> well , of course th the biggest , um result here , which is one we 've we 've talked about many times and is n't new to us , but which i think would be interesting to show someone who is n't familiar with this <inbreath> is just the sheer number of overlaps . <EOS> it 's a forty forty plus minute <inbreath> meeting , what we 've learned about is overlaps in this situation , is that the first the first - order thing i would say is that there 's a lot of them . <EOS> in fact <inbreath> and it 's not just an overlap bunch of overlaps second - order thing is <inbreath> it 's not just a bunch of overlaps in one particular point , <inbreath> but that there 's overlaps , uh throughout the thing . <EOS> um <mouth> preliminary analysis of overlaps in the pilot data we have transcribed , <EOS>
um , that some people tend to be overlapped with more often than they 're overlapped , and it would be , you know of course , there 's also the question of what type of overlap was this , and w what were they , so , um then it beco though so just just superficially to give um a couple ideas of the types of overlaps involved , i have at the bottom several that i noticed . <EOS> so , uh , the point is that , um <inbreath> overlap 's not necessarily a bad thing and that it would be im i useful to subdivide these further and see if there are individual differences in styles with respect to the types involved . <EOS> so , the question is , you know , how many more overlaps <inbreath> do you have of , say the two - person type , by adding more people . <EOS> to a meeting , so it may be that having three people <inbreath> is very different from having two people or it may not be . <EOS> but we should still be able to somehow say what what is the added contra contribution to sort of overlap time of each additional person , or something like that . <EOS> these were these were benevolent types , as people finishing each other 's sentences , and stuff . <EOS> and that it would be interesting to look at whether there are these kinds of constraints that jane mentioned , that <inbreath> what maybe the additional people add to this competition that happens right after a turn , <EOS>
yeah , i 've been playing with , um uh , using the close - talking mike to do to try to figure out who 's speaking . <EOS> so my first attempt was just using thresholding and filtering , that we talked about about two weeks ago , ok and then the other thing i did , was i took <inbreath> javier 's speaker - change detector acoustic - change detector , and i implemented that with the close - talking mikes , um so , at any rate , my next attempt , which i 'm in the midst of and have n't quite finished yet was actually using the <inbreath> uh , thresholding as the way of generating the candidates . <EOS> but all of this is close - talking mike , what i 'm doing <inbreath> is trying to use the close - talking mike <inbreath> and just use can - and just generate candidate and just try to get a first pass at something that sort of works . <EOS>
or , this is getting a little extravagant , we could put up some kind of blinds or something to to remove , uh visual contact . <EOS> so this is the things that i think we did <laugh> in the last three months <EOS>
i 'll do that on the next set of forms . <EOS> so i 'm gon na put little labels on all the chairs with the seat number . <EOS>
i think what i what this has , uh , caused me so this discussion caused me to wan na subdivide these further . <EOS> i 'm gon na take a look at the , uh backchannels , how much we have anal i hope to have that for next time . <EOS>
so , from the point of view of studying dialogue , i mean , which dan jurafsky and andreas and i had some projects on , you want to know the sequence of turns . <EOS> so , for things like language modeling or dialogue modeling <inbreath> it 's we know that that 's wrong in real time . <EOS>
but , of course , uh i e <laugh> this is just one meeting , uh there 's no statistical testing involved , and that would be required for a for a finding of any kind of scientific reliability . <EOS> you know , the other thing i was thinking was that , um these all these interesting questions are , of course , pretty hard to answer with , uh u you know , a small amount of data . <EOS>
and that works o k , if you fil if you tune the filter parameters , if you tune <inbreath> how long your median filter is and how high you 're looking for your thresholds . <EOS>
so if you fiddle around with it a little bit and you get good numbers you can actually do a pretty good job of segmenting when someone 's talking and when they 're not . <EOS> but if you try to use the same paramenters on another speaker , it does n't work anymore , it does work for the one speaker throughout the whole meeting . <EOS>
and unfortunately that 's not working real well , and it looks like it 's and what looks like it 's happening is that the even on the close - talking mike the broad phone class classifier 's doing a really bad job . <EOS>
none <EOS>
so we need a barrier that does n't disturb the sound , <EOS>
but um uh jose and i were just talking about <inbreath> the uh < page turn > uh , speech e energy thing , he was he he was taking everything over two hundred milliseconds and so i i his his he 's making the constraint it has to be at least two hundred milliseconds . <EOS>
right now , that he 's not really showing any kind of uh distinction , but uh um . <EOS> and uh one is that uh this is all in log energy um and but one thing he was pointing out is when he he looked at a bunch of examples in log domain , it is actually pretty hard to see <inbreath> the change . <EOS> and when he 's looking in the log domain he 's not really seeing it . <EOS>
and when he 's looking in straight energy he is , um but since <inbreath> uh your intuition from looking at some of the data , is that when you looked at the regular energy , that it did in fact usually go up , <laugh> when two people were talking , <inbreath> that 's eh you know , you should be able to come up with a measure which will match your intuition . <EOS>
and i had plan to go through with it , of of co coding the types of overlaps that people were involved in s just with reference to speaker style so , you know , with reference so i was planning to do a taxonomy of types overlaps with reference to that . <EOS>
yeah this was the problem with these categories , <EOS>
um another is that he needs to play with the the different uh uh temporal sizes . <EOS> uh , and uh he 's going to vary that number and also look at moving windows , as we discussed before . <EOS> um and uh and the other thing is that the yeah doing the <inbreath> subtracting off the mean and the variance in the uh and dividing it by the standard deviation in the log domain , <inbreath> may not be the right thing to do . <EOS> so yeah there there there there 's a good chance then given that different people do talk different amounts that there is there there is still a lot more to be gained from gain norm normalization with some sort uh . <EOS> but we were agreed that in addition to that uh there should be s stuff related to pitch and harmonics and so forth . <EOS> um and then we w i think we 're agreed that pitch - related things are are are going to be a a really likely candidate to help . <EOS> and then you can move on to the uh uh more < mike loud pop > pitch related stuff . <EOS> and then that should give us some indication between those , should give us some indication of whether there 's anything to be achieved f from energy at all . <EOS> not consider the log energy . <EOS> i think i would just sort of look at the energy and then get into the harmonicity as as a suggestion . <EOS>
and i had plan to go through with it , of of co coding the types of overlaps that people were involved in s just with reference to speaker style so , you know , with reference so i was planning to do a taxonomy of types overlaps with reference to that . <EOS>
but it is definitely true that we need to have the time marks , and i was assuming that will be inherited because , if you have the words and they 're roughly aligned in time via forced alignment or whatever we end up using , then you know , this student and i would be looking at the time marks well , we we would n't be able to do any work without a forced alignment anyway , <EOS>
so i 'm not sure what to do about the region field for english variety . <EOS> but i do n't know how to i do n't know how to i do n't know how to categorize them . <EOS> yeah this was the problem with these categories , <EOS>
but the problem with it is the drawing of those waveforms is so slow that every time you do anything it just crawls . <EOS> just about anything , and it it was so slow it was not usable . <EOS>
well , um , i can give you an update on the transcription effort . <EOS> because at present , < spike on / p / > <inbreath> um , because < spike on / b / > of the limitations of <inbreath> th the interface we 're using , overlaps are , uh , not being < spike on / b / > encoded by < spike on / b / > the transcribers in as complete < spike on / p / > and , uh , detailed a way as it might be , what our decision was is that we 'll go ahead with what we have with a not very fine time scale on the overlaps . <EOS>
the we have great great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal . <EOS> um , so , uh , what we basically did so far was using the mixed file to to detect s speech or nonspeech portions in that . <EOS> but < spike on `` but '' > it it saves so much time the the <spike> transcribers <EOS>
uh , maybe < mike spike > raise the issue of microphone , uh , um procedures well , let 's why do n't we talk about microphone issues ? <EOS> but i i think that it it does n't hurt , uh , the naturalness of the situation to try to have people wear the microphones properly , if possible , so , anything to reduce breathing is is is a good thing . <EOS> so you want it enough to the side so that when you exhale through your nose , it does n't the wind does n't hit the mike . <EOS>
but , i mean , the other things that we talked about is , uh , <inbreath> pitch - related things and harmonicity - related things , you know , have a have a couple markov models which is to say , do n't worry so much about the , uh , features . <EOS> and , uh , try to indi try to determine , you know , w when is th when are you in an overlap , when are you not in an overlap . <EOS> and let the , uh , uh , statistical system determine what 's the right way to look at the data . <EOS> and i hope the the next week i will have , eh , some results and we we will show we will see , eh , the the parameter the pitch , <inbreath> eh , tracking in with the program . <EOS> uh , the has has , uh , been exploring , uh , e largely the energy issue so far , um , uh , jose has has been but it may be given that you have a limited time here , it it just may not be the best thing to <inbreath> to to focus on for the remaining of it . <EOS> th - they were suggesting going to markov models , <EOS>
what our decision was is that we 'll go ahead with what we have with a not very fine time scale on the overlaps . <EOS> then < spike on / th / > when they 're encoding the overlaps < spike on / p / > it would be nice for them to be able to specify when you know , the start points and end points of overlaps . <EOS> and i hope that if we do a forced alignment with the close - talking mike , that will be enough to recover at least some of the time the time information of when the overlap occurred . <EOS>
oh , we should definitely get with them then , agree upon a format . <EOS>
i think it would also be interesting to have , uh , a couple of the meetings have more than one transcriber do , cuz i 'm curious about inter - annotator agreement . <EOS>
but i i think that it it does n't hurt , uh , the naturalness of the situation to try to have people wear the microphones properly , if possible , so , anything to reduce breathing is is is a good thing . <EOS> so you want it enough to the side so that when you exhale through your nose , it does n't the wind does n't hit the mike . <EOS>
but , i mean , the other things that we talked about is , uh , <inbreath> pitch - related things and harmonicity - related things , and i hope the the next week i will have , eh , some results and we we will show we will see , eh , the the parameter the pitch , <inbreath> eh , tracking in with the program . <EOS> but , eh , what did you think about the possibility of using the javier software ? <EOS> right . <EOS> so if we if we fed the hand - segmentation to javier 's and it does n't work , then we know something 's wrong . <EOS> yeah . <EOS> i think that 's probably worthwhile doing . <EOS>
cuz there is one thing that we do n't have right now and that is the automatic , um , channel identifier . <EOS> that that , you know , that would g help in terms of encoding of overlaps . <EOS>
well , the thing that you is hard to deal with is whe <inbreath> when they speak while laughing . <EOS>
but the bottom line is it 's still not , uh , separating out very well . <EOS> so far , um , uh , jose has has been and and so far at least has not come up with <inbreath> any combination that really gave you an indicator . <EOS>
and the other one is , um , uh , is there some good use that we can make of the transcribers to do other things ? <EOS> one one was uh , we had s had some discussion in the past about some very high level labelings , um , the other thing is that there was a number of things at the transcription side that , um , transcribers can do , like dialogue act tagging , <EOS>
and actually in addition to that , that the the close talking mikes are worn in such a way as to best capture the signal . <EOS> so it 's towards the corner of your mouth so that breath sounds do n't get on it . <EOS> but if we could actually standardize , you know , the the microphones , uh , as much as possible that would be really helpful . <EOS> but have them all be the same mike . <EOS> it 's the equipment and also how it 's worn . <EOS> it 's really < mike noise > it makes a big difference from the transcribers ' point of view so we might as well get it as uniform as we can . <EOS> and , uh , also dan ellis 's innovation of the , uh the multi - channel to here really helped a r a lot in terms of clearing clearing up h hearings that involve overlaps . <EOS>
uh , well one thing i was gon na say was that , um , i we could get more , uh , of the head mounted microphones so why do n't we just go out and and get an order of of but have them all be the same mike . <EOS> so we might as well get it as uniform as we can . <EOS>
i guess i wanted to , um , sort of make a pitch for trying to collect more meetings . <EOS> because i think it 'd be good if if we can get a few different sort of non - internal types of meetings <EOS>
but that , um , it would be helpful if i can stay in the loop somehow with , um , people who are doing any kind of post - processing , <EOS>
it adds this extra , you know , vari variable for each speaker to to deal with when the microphones are n't similar . <EOS>
but if we could actually standardize , you know , the the microphones , uh , as much as possible that would be really helpful . <EOS> it 's the equipment and also how it 's worn . <EOS> it 's really < mike noise > it makes a big difference from the transcribers ' point of view <EOS>
but if you 're d i the kind of person who 's doing array processing you actually care about funny little times . <EOS> and and so you actually wou would want to have a completely different set up than we have , is there an interest in getting video recordings for these meetings ? <EOS> yes , absolutely . <EOS> but it 's exactly the same problem , that you have an infrastructure problem , they were interested in ours . <EOS> and , uh , it is related to ours . <EOS>
but there could be problems , right ? <EOS> with that . <EOS> then for forced alignment we actually do n't know where you know , in the signal the transcriber heard that word . <EOS> th - but there 's going to be a real problem , uh , even if we chop up based on speech silence these , uh , the transcripts from i b m , we do n't actually know where the words were , which segment they belonged to . <EOS>
so the problem is like , uh , on the microphone of somebody who 's not talking they 're picking up signals from other people and that 's <breath> causing problems ? <EOS>
ok well , the , w uh as you can see from the numbers on the digits we 're almost done . <EOS> and so , once we 're it 's done it would be very nice to train up a recognizer and actually start working with this data . <EOS>
yeah just by way of uh , uh , a uh , order of magnitude , uh , um , we 've been working with this aurora , uh data set . <EOS> and , uh , the best score , on the , nicest part of the data , that is , where you 've got training and test set that are basically the same kinds of noise and so forth , uh , is about , i think the best score was something like five percent , uh , error , per digit . <EOS>
one question i have that that i mean , we would n't know the answer to now but might , do some guessing , but i was talking before about doing some model modeling of arti uh , uh , marking of articulatory , features , with overlap and so on . <EOS> one thought might be to do this uh , on on the digits , or some piece of the digits . <EOS> so , i mean another way to look at this is to , is to , uh , do some stuff on switchboard which has all this other , stuff to it . <EOS> and then , um , as we get , further down the road and we can do more things ahead of time , we can , do some of the same things to the meeting data . <EOS>
uh , oh yeah , um , <breath> i worked a little bit on the on the presegmentation to to get another version which does channel - specific , uh , speech - nonspeech detection . <EOS>
also we discussed some adaptational things , uh you know i had n't , uh , incorporated , a convention explicitly to handle acronyms , for example , and then , a similar conv uh , convention for numbers . <EOS> so if they hear a breath and they do n't know who breath it is it 's better to put it in that channel than to put it in the speaker 's channel <EOS>
and so the the question is , should we have the transcribers do that or should we just do it ? <EOS> and i think it 's a it 's a fine idea partly because , um , it 's not un unrelated to their present skill set , and then , hand off to jane , and the transcribers to do the actual extraction of the digits . <EOS>
well , you know , um i mean if we 're talking about , having the , annotators annotate these kinds of features , it seems like , so i mean i we 'll see wha how much we can , uh , get the people to do , and how much money we 'll have and all this sort of thing , so , i mean another way to look at this is to , is to , uh , do some stuff on switchboard which has all this other , stuff to it . <EOS>
it seems to me that it would be good to have , a few minutes from from different meetings , so , as a first pass through , a first chance without having to do a lot of hand - editing , what we 're gon na do , is , i 'll run it through channelize , give them those data after i 've done the editing process and be sure it 's clean . <EOS> and then we 'll see if the units that we 're getting , uh , with the at that level , are sufficient . <EOS>
none <EOS>
there are some problems with the lapel mike . <EOS> but , there are some some as i said some problems with the lapel mike , <EOS>
then , the , yeah , there are there are some problems with with with n with normalization , and , then , uh , there the system does n't work at all . <EOS>
and , the thing is i i , then the evaluation of of the system is a little bit hard , as i do n't have any references . <EOS>
it drifted into the afternoon , <inbreath> uh , concerning this issue of , um , the , well there 's basically the issue of the interplay between the transcript format and the processing that , they need to do for , the sri recognizer . <EOS> and , um , < mike noise > their recognizer would prefer that the units not be overly long . <EOS>
the other topic i was thinking of was the sta status on microphones and channels , and all that . <EOS> um , the new microphones , the two new ones are in . <EOS> so what we would do is replace the wired mikes with wireless . <EOS> and so we could replace our wired mikes with wireless if we bought another base station and more wireless mikes . <EOS> no , we 're just replacing the wired the two wired that are still working , along with a couple of the wired that are n't working , one of the wired that 's not working , with a wireless . <EOS>
but i think it would come to about eleven hours that are finished uh , transcribing from them right now . <EOS> the next step is to that i 'm working on is to insure that the data are clean first , and then channelized . <EOS> and also that we now incorporate these additional conventions that uh , liz requested in terms of um , um in terms of having a s a systematic handling of numbers , and acronyms which i had n't been specific about . <EOS> that the mark - up is consistent all the way throughout , <EOS>
yeah , and i and i tried t to normalize uh uh the features , there 's loudness and modified loudness , um , within one channel , because they 're , <outbreath> yeah to to be able to distinguish between foreground and background speech . <EOS>
i wan na talk a little bit about getting how we 're gon na to get people to edit bleeps , parts of the meeting that they do n't want to include . <EOS> the transcription part ? <EOS> so i guess the next thing is this uh bleep editing . <EOS> we need to provide the transcripts to every participant of every meeting to give them an opportunity to bleep out sections they do n't want . `` <EOS> if you agree to participate you 'll have the opportunity to have anything ex anything excised , which you would prefer not to have included in the data set . `` <EOS> there again you will be allowed to indicate any sections that you 'd prefer to have excised from the database , and they will m be removed both from the transcript and the recording . `` <EOS>
`` if you agree to participate you 'll have the opportunity to have anything ex anything excised , which you would prefer not to have included in the data set . `` <EOS> and they will m be removed both from the transcript and the recording . `` <EOS> there again you will be allowed to indicate any sections that you 'd prefer to have excised from the database , again let 's you know , sort of circulate the the wording on each of these things and get it right , <EOS>
so what we would do is replace the wired mikes with wireless . <EOS> and so we could replace our wired mikes with wireless if we bought another base station and more wireless mikes . <EOS> no , we 're just replacing the wired the two wired that are still working , along with a couple of the wired that are n't working , one of the wired that 's not working , with a wireless . <EOS>
well we should talk to them about it because i know that sri is also in the process of looking at stuff , and so , you know , what we should try to keep everyone on the same page with that . <EOS>
in listening to some of these meetings that have already been recorded there are sometimes big spikes on particular things , <EOS>
none <EOS>
are are people going to be allowed to bleep out sections of a meeting where they were n't speaking ? <EOS> so that means other people are editing what you say ? <EOS>
but i really think we wan na make it the rare incidence . <EOS> and and uh , i am just a little worried about making it so easy for people to do , <EOS>
i have a short thing about digits and then uh i wan na talk a little bit about naming conventions , so the only thing i wan na say about digits is , we are pretty much done with the first test set . <EOS> uh , the last one was that you had there , was about naming ? <EOS> we want some way of specifying , more than looking in the `` key `` file , what channel and what mike . <EOS> what channel , what mike , and what broadcaster . <EOS>
uh , right , so so i i was just gon na talk briefly about the nsf itr . <EOS> ok , so uh e l i guess , let me , uh , get my my short thing out about the nsf . <EOS> so this was , uh , a , uh , proposal that we put in so is i for it was a <mouth> proposal for the itr program , um , < clears throat > since we have such a short agenda list i guess i wi i will ask how how are the transcriptions going ? <EOS> and and i 'm tried to to , uh , adjust the to to improve , eh , an harmonicity , eh , detector that , eh , i i implement . <EOS> eh , and now i 'm i 'm i 'm trying to to find , eh , some kind of a , um <breath> of h of help , eh , using the energy to to distinguish between possible harmonics , and and other fre frequency peaks , that , eh , corres not harmonics . <EOS> you 're trying distinguish between the case where there is , uh where where there are more than uh , where there 's more than one speaker and the case where there 's only one speaker . <EOS>
that there 's one third thing i wanted to to ex raise as a to as an issue which is , um , how to handle breaths . <EOS>
seems like we should just change the transcripts <EOS>
that also brings up the point that we have to start assembling a speaker database so that we get those links back and forth and keep it consistent . <EOS>
well so then then , maybe the answer is to , uh , listen especially densely in places of overlap , <EOS>
so i would say do n't tell them to transcribe anything that 's outside of a grouping of words . <EOS>
i do n't know think it 'd be ideal . <EOS> and breaths are part of real speech . <EOS>
i will prepare for the next week eh , all my results about the harmonicity <EOS>
but , um , the other problem we were thinking about is if you just put the numerals , they might say forty - three instead of four three . <EOS>
but , um , with the mixed , when you have an overlap , you only have a a choice of one start and end time for that entire overlap , <EOS>
once in a while a backchannel will be overlooked by the transcriber . <EOS>
yeah , but those backchannels will always be a problem i think . <EOS> uh especially if they 're really short and they 're not very loud and so it it can it it will always happen that also the automatic s detection system will miss some of them , <EOS>
none <EOS>
um , aside from the fact that they 're obviously very time - consuming to encode , <EOS>
so the other topic with digits is uh , we have asr results from liz , transcript status from jane , and disk space and storage formats from don . <EOS> don , you had disk space and storage formats . <EOS>
we have about two hours worth . <EOS> so what that means is we have about an hour of transcribed digits that we can play with . <EOS> and you 're saying two hours , uh , is digits , <EOS>
um , leads us to believe that doing a better segmentation , like your channel - based segmentation , or some kind of uh , echo cancellation to get basically back down to the individual speaker utterances would be probably all that we would need to be able to do good recognition on the on the close - talking mikes . <EOS>
alright so , first of all , um , there was a an interest in the transcribe transcription , uh , checking procedures <EOS>
none <EOS>
ok so so i can just add to the instructions to read it as digits <EOS>
none <EOS>
give it give one tr transcriber the the channelized version of of my speech - nonspeech detection and look if if that 's helpful for them , <EOS>
and they 're slowly s trickling in . <EOS> and so we tried last week with them written out in english . <EOS> and it just did n't work at all because no one grouped them together . <EOS>
especially for speakers that do n't talk a lot . <EOS> so we have a problem with acoustic adaptation , <EOS>
it 's the fact that there 's overlap that causes recognition errors . <EOS> so with a one - word utterance and ten insertions you know you got huge error rate . <EOS>
i guess the other neat thing is it shows for sure w that the lapel , you know within speaker is bad . <EOS> and it 's bad because it picks up the overlapping speech . <EOS>
that that 's why we need more disk space <EOS>
you ca n't load an hour of speech into x waves . <EOS>
but if there 's things that um , we change later , then we always have to keep our the dictionary up to date . <EOS>
well , you know the problem the problem is that some some of the adjustments that they 're making are to bring are to combine bins that were time bins which were previously separate . <EOS> but the i guess the ef the effect for you guys , because you 're pulling out the little wave forms into separate ones , that would mean these boundaries are constantly changing but if you split inside something , you do n't where the word which words moved . <EOS>
and then we go in and adjust the boundaries . <EOS> how quickly can the transcribers scan over and fix the boundaries , and then it 's then it 's then it 's the transcribers tightening stuff up , so you 're talking about tightening up time boundaries ? <EOS> well , so so that 's something that the transcribers will have to have to do . <EOS> so , if i excluded the pathological ones , <laugh> by definition , those that had like over ninety - five percent error rate , <inbreath> and the non - natives , then the average error rate was like one point four or something , <EOS>
none <EOS>
so , if i excluded the pathological ones , <laugh> by definition , those that had like over ninety - five percent error rate , <inbreath> and the non - natives , then the average error rate was like one point four or something , <EOS>
so we need some way to push these first chunk of meetings into a state where we get good alignments . <EOS>
and then we go in and adjust the boundaries . <EOS> and then it 's then it 's then it 's the transcribers tightening stuff up , well , so so that 's something that the transcribers will have to have to do . <EOS>
he generated , um , a channel - wise presegmented version of a meeting , and then it 's ibm . <EOS>
i think that if we decide that we need that they need to see the visuals , we need to change the interface so that they can do that . <EOS>
and there really was , then , if it did n't show up in a mixed signal to verify , then it might be overlooked , yeah , but presumably , most of those they should be able to hear from the mixed signal unless they 're embedded in the heavil heavy overlap section i do n't know that you can locate them very well from the mixed signal , <EOS>
so , i mean , the question is `` should should a transcriber listen to the entire thing or can it g can it be based on the mixed signal ? `` <EOS>
except for < clears throat > it does n't do well on short things , remember . <EOS>
the problem is that that the tcl - tk interface with the visuals , it 's very slow to load waveforms . <EOS> but you just ca n't get the visual display to show quickly . <EOS>
oh , well , yeah , so i 've been struggling with the forced alignments . <EOS> but it might well be that we ca n't get clean alignments out of this out of those , uh , <inbreath> channels , so he thought well if we can do something quick and dirty because dan said the cross - cancellation , it 's not straight - forward . <EOS>
none <EOS>
unless maybe we do this , uh , um , cancellation business . <EOS> but it 's clear from dan that this is not something you can do in a short amount of time . <EOS> um , but then if you add the dynamic aspect of adapting distances , then it was n't <EOS>
it 's very tedious to check these . <EOS>
and i really find it a pain in the neck to delete things <EOS>
yeah , for eurospeech we want some results <EOS>
but if they 're not comfortable , we have the same problems we have with these stupid things . <EOS>
and the interesting thing is that even though , <inbreath> yes , it 's a digits task and that 's a relatively small number of words and there 's a bunch of digits that you train on , <inbreath> it 's just not as good as having a a l very large amount of data and training up a a a nice good big <inbreath> hmm . <EOS>
two items , which was , uh , digits and possibly stuff on on , uh , forced alignment , so we we only r hav i only looked at actually alignments from one meeting that we chose , <EOS>
uh , but the other is that , um , the digits <inbreath> recorded here in this room with these close mikes , i uh , are actually a lot harder than the studio - recording ti - digits . <EOS> if you have only one utterance per speaker you might actually screw up on estimating the the warping , uh , factor . <EOS> well , i know there were some speaker labelling problems , um , after interruptions . <EOS> but you 're actually saying that certain , uh , speakers were mis mis - identified . <EOS>
and and w we we were able to get some definite improvement on the forced alignments by looking at them first and then realizing the kinds of errors that were occurring so just sort of working through a bunch of debugging kinds of issues . <EOS>
so so the key thing that 's missing here is basically the ability to feed , you know , other features <outbreath> i into the recognizer and also then to train the system . <EOS> we want to <inbreath> have the ability to feed it different features . <EOS>
yeah , bu although i 'd be i think it 'd be interesting to just take this exact actual system and try it out on ti - digits . <EOS>
so , we might have to modify that script to recognize the , um , speakers , <inbreath> um , in the in the , uh , um , <mouth> ti - digits database . <EOS> because we may have to do an extract to get the amount of data per speaker about right . <EOS>
yeah . <EOS> i i know what i was thinking was that maybe , uh , i i we could actually t t try at least looking at , uh , some of the the large vocabulary speech from a far microphone , but i 'm saying if you do the same kind of limited thing <inbreath> as people have done in switchboard evaluations or as a could we do exactly the same thing that we 're doing now , but do it with a far - field mike ? <EOS> but you use the acoustics from the far - field mike . <EOS>
so , <inbreath> we would need a hand - marked , um , <mouth> word - level alignments or at least sort of the boundaries of the speech betw you know , between the speakers . <EOS> and tune the parameters of the of the model , uh , to op to get the best performance . <EOS>
you know , interface - wise if you 're looking at speech , you wan na be able to know really where the words are . <EOS> um , and see if you can in maybe incorporate it into the transcriber tool some way , yeah , it wou the advantage would just be that when you brought up a bin you would be able if you were zoomed in enough in transcriber to see all the words , you would be able to , like , have the words sort of located in time , <EOS>
none <EOS>
um , also you had the adaptation in the sri system , which we did n't have in this . <EOS> so there was a significant loss from not doing the adaptation . <EOS>
uh , but the other is that , um , the digits <inbreath> recorded here in this room with these close mikes , i uh , are actually a lot harder than the studio - recording ti - digits . <EOS>
uh , but the other is that , um , the digits <inbreath> recorded here in this room with these close mikes , i uh , are actually a lot harder than the studio - recording ti - digits . <EOS> i suspect that to get sort of the last bit out of these higher - quality recordings you would have to in fact , uh , use models that , uh , were trained on wider - band data . <EOS> that 's where the most m acoustic mismatch is between the currently used models and the the r the set up here . <EOS> the near versus far . <EOS>
if you have only one utterance per speaker you might actually screw up on estimating the the warping , uh , factor . <EOS>
because it 's further away from most of the people reading digits . <EOS>
you know , as liz said the we f enforce the fact that , uh , the foreground speech has to be continuous . <EOS> things like words that do occur just by themselves a alone , like backchannels or something that we did allow to have background speech around it those would be able to do that , but the rest would be constrained . <EOS>
we probably want to adapt at least the foreground speaker . <EOS> but , i guess andreas tried adapting both the foreground and a background generic speaker , and that 's actually a little bit of a f funky model . <EOS> like , it gives you some weird alignments , just because often the background speakers match better to the foreground than the foreground speaker . <EOS>
tha - there are some cases like where the the wrong speaker uh , these ca not a lot , but where the the wrong person the the speech is addre attached to the wrong speaker <EOS>
so the only agenda items were jane was jane wanted to talk about some of the ibm transcription process . <EOS> uh , and you just sent off a eurospeech paper , so , we should probably talk about the ibm transcription process stuff that <EOS>
uh , the one was that the just the the amount of overlap but , even if you take out all the backchannels you still have significant overlap . <EOS>
and we rescored things um , a little bit more carefully . <EOS> and then the second one was just basically the <breath> the stuff we had in the in the hlt paper on how overlaps effect the recognition performance . <EOS> but basically what we found is after we take out these regions so we only score the regions that were certified as foreground speech , <breath> the recognition error went down to almost <breath> uh , the level of the non - overlapped speech . <EOS>
so at the end after a discourse marker or after backchannel or after filled pause , you 're much more likely to be interrupted <breath> than before . <EOS>
no . <EOS> well , according to the transcripts . <EOS>
although that 's i i take it that 's something that uh don will will look at <EOS>
and also , um , the other person that wants it there is one person at sri who wants to look at the <breath> um , you know , the uh the data we have so far , and so i figured that ftp is the best approach . <EOS> so what i did is i um <mouth> <breath> @ @ i made a n new directory <EOS>
so the only agenda items were jane was jane wanted to talk about some of the ibm transcription process . <EOS> so , we should probably talk about the ibm transcription process stuff that <EOS>
and , if the chunked files focused on the dominant speakers , <breath> then , when when it got s patched together when it comes back from ibm , we can add the backchannels . <EOS> and you just use the s the segments of the dominant speaker then ? <EOS> for for sending to to ibm but then we could just use the the output of the detector , and do the beeping on it , and send it to i b without having her check anything . <EOS> but but i i i have another suggestion on that , which is , <breath> since , really what this is , is is is trying to in the large , send the right thing to them and there is gon na be this this post - processing step , and we 'll we 'll fix things up <EOS>
so that if you if you play back that bin and have it in the mode where it stops at the boundary , <breath> it sounds like a normal word . <EOS> but my general goal <breath> when there was sufficient space , room , pause after it to have it be kind of a natural feeling gap . <EOS>
we should probably uh give them the non - downsampled versions . <EOS> but , um they probably w want the originals . <EOS>
yeah , in fact after our meeting uh , this morning thilo came in and said that <breath> um , there could be other differences between <breath> the uh already transcribed meeting with the beeps in it and one that has just r been run through his process . <EOS> so tomorrow , <breath> when we go to make the um uh , chunked file <breath> for ibm , we 're going to actually compare the two . <EOS> and then we 're gon na do the beep - ify on both , and listen to them and see if we notice any real differences . <EOS>
so what what we 're probably gon na do is just write a script , that if two , chunks are very close to each other on the same channel we 'll just merge them . <EOS> but i like this idea of uh , for our purposes for the for the ibm preparation , <breath> uh , n having these joined together , <EOS>
uh , because we could use that to fine tune our alignment process i mean w i mean what i would i was interested in is having <breath> a se having time marks for the beginnings and ends of speech i i hand - adjusted two of them so so at some point we will try to fine - tune our forced alignment <EOS>
we need to give brian the beeps file , and we send it to ibm . <EOS> the other one is <breath> we just run his thing and send it to ibm . <EOS> send it off to ibm . <EOS>
but then we could just use the the output of the detector , and do the beeping on it , and send it to i b so the the one suggestion is you know we <breath> we run thilo 's thing the other one is <breath> we just run his thing and send it to ibm . <EOS> and that is <breath> if we go ahead and we <breath> just run his , and we generate the beeps file , then we have somebody listen beeps file . <EOS> put the beeps file , <EOS>
the other problem is , that when it when it uh d i on the breathy ones , where you get <breath> <breath> breathing , uh , inti indicated as speech . <EOS> so , i could run this on those breathy channels , and what that 'll do is just cut the time a little further . <EOS>
why do n't we check through a bunch of things by sampling it ? <EOS>
what can you do ? <EOS> and we 'll correct it when it comes back . <EOS>
we actually exceeded the delayed deadline by o another day , i hope they accept it . <EOS>
well , we did n't get to look at that , yeah , i just wonder if you have to normalize by the numbers of speakers or something . <EOS> i bet there 's a weak dependence . <EOS>
there 's no statement about cause and effect . <EOS>
yeah , in fact after our meeting uh , this morning thilo came in and said that <breath> um , there could be other differences between <breath> the uh already transcribed meeting with the beeps in it and one that has just r been run through his process . <EOS> that 's because of channel overlap . <EOS>
but you know , i wanted to say , his segmentation is so good , that <breath> um , the part that i listened to with her yesterday <breath> did n't need any adjustments of the bins . <EOS> i ca n't really hhh , tsk . <EOS> i do n't have really representative numbers , i think . <EOS>
and i <laugh> the speech - nonspeech detector just assigns randomly the speech to to one of the channels , <EOS>
a hand - transcriber would have trouble with that . <EOS>
none <EOS>
because <inbreath> it occurred to me that this is late may and the darpa meeting is in mid july . <EOS> i mean in particular i would i would really hope that when we do this darpa meeting in july that we sort of have we 're we 're into production mode , somehow um , we are gon na have this darpa meeting in the middle of july , given that we 've been we 've given a couple public talks about it already , spaced by months and months , i think it 'd be pretty bad if we continued to say none of this is available . <EOS>
and so he talked it over with the transcriber and the transcriber thought that the easiest thing for them would be if there was a beep and then the nu a number , a digit , and then a beep , uh , at the beginning of each one e e u u the reason i 'm asking is because , uh , jane and i have just been talking , and she 's just been doing . <EOS> <laugh> uh , e a , you know , further hiring of transcribers . <EOS> and so we do n't sort of really know exactly what they 'll be doing , how long they 'll be doing it , and so forth , because right now she has no choice but to operate in the mode that we already have working . <EOS> i hired two transcribers today . <EOS> i 'm thinking of hiring another one , which will because we 've had a lot of attrition . <EOS> yeah . <EOS> so , um , uh , jane and adam and i had a meeting where we talked about the reorganization of the directory structure for all of the meeting for all the meeting recorder data . <EOS>
i know that we were gon na do something with the transcriber interface is one thing , well , we were gon na do a mock - up , like , question answering or something , i thought , i was gon na ask adam to , uh , say if he thought anymore about the demo stuff <EOS>
is there stuff that 's happened about , um , uh , the sri recognizer et cetera , y y you guys were doing a bunch of experiments with different front - ends and then with now the the you saw the note that the plp now is getting basically the same as the mfcc . <EOS> um , it looks like the vocal tract length normalization is working beautifully , actually , because in all our previous experiments , we had the uh , you know , we were essentially cheating by having the , um , you know , the h the hand - segmentations as the basis of the recognition . <EOS> and so now with thilo 's segmenter working so well , i think we should consider doing a and even the good thing is that since you , um , have high recall , even if you have low precision cuz you 're over - generating , that 's good has has , uh ? <EOS> we just i think , just talked about this the other day , but h has has anybody had a chance to try changing , uh , insertion penalty sort of things with the with the , uh <breath> uh , using the tandem system input for the ? <EOS> but the plp features work um , uh , you know , continue to improve the , <EOS>
but we did find that some of the features that , i gue jane would know about , that are expressing sort of the <breath> distance of , um , <mouth> boundaries from peaks in the utterance and <breath> some local , um , range pitch range effects , like how close people are to their floor , are showing up in these classifiers , um , so we 're starting to see some patterns because the <breath> prosodic features are very noisy and so you you need a lot of data in order to model them . <EOS>
has has , uh ? <EOS> we just i think , just talked about this the other day , but h has has anybody had a chance to try changing , uh , insertion penalty sort of things with the with the , uh <breath> uh , using the tandem system input for the ? <EOS> as i said before , the uh using dan 's , uh , uh , vocal tract normalization option works very well . <EOS> but the plp features work um , uh , you know , continue to improve the , well , but if you add them all up you have , uh , almost five percent difference now . <EOS> and i think i agree with you that if we fixed lots of different things and they would all add up , we would probably have a a a competitive system . <EOS> but i think not that much of it is due to the front - end per se . <EOS> i think maybe a couple percent of it is , as far as i can see from this . <EOS> eh at this point i 'm as i mean , you know e i 'm wondering is it can we expect , uh , a tandem system to do better than a properly trained you know , a gaussian system trained directly on the features with , you know , the right ch choice of parameters ? <EOS>
i mean , are we trying to do them < mike noise > in synchrony ? <EOS> that might be fun . <EOS> well , it 's < breath-laugh > it 's it 's not you know , it 's not gon na work out but we could we could just , uh , uh , see if we find a rhythm , <EOS>
i mean , are we trying to do them < mike noise > in synchrony ? <EOS> that might be fun . <EOS> but we could we could just , uh , uh , see if we find a rhythm , well , it 's < breath-laugh > it 's it 's not you know , it 's not gon na work out <EOS>
and so now with thilo 's segmenter working so well , i think we should consider doing a so we do need some kind of pre - segmentation . <EOS> we should we should consider doing some extra things , like , um , you know , retraining or adapting the <breath> the models for background noise to the to this environment , for instance . <EOS> and , yeah , using thilo 's , you know , posteriors or some kind of or <EOS>
and so he talked it over with the transcriber e e u u the reason i 'm asking is because , uh , jane and i have just been talking , and she 's just been doing . <EOS> <laugh> uh , e a , you know , further hiring of transcribers . <EOS> and so we do n't sort of really know exactly what they 'll be doing , how long they 'll be doing it , and so forth , because right now she has no choice but to operate in the mode that we already have working . <EOS> you know , that we we actually <breath> have a stream going and we know how how well it does and how and how it operates . <EOS> i hired two transcribers today . <EOS> i 'm thinking of hiring another one , which will because we 've had a lot of attrition . <EOS> but if we hire f you know , f we have five on staff five or six on staff at any given time , then <inbreath> it 's a small enough number so we can be flexible either way . <EOS>
i mean in particular i would i would really hope that when we do this darpa meeting in july that we sort of have we 're we 're into production mode , somehow you know , that we we actually <breath> have a stream going and we know how how well it does and how and how it operates . <EOS> right . <EOS> so we can s we we wan na be able to say `` here is a subset that is available right now `` and that 's has been through the legal issues and so forth . <EOS> and they do n't have to approve , you know , th an edited version , they can just give their approval to whatever version well , in principle , yes . <EOS> but , i mean , i if if if somebody actually did get into some legal issue with it then we yeah . <EOS> but th i mean , the editing will continue . <EOS> presumably if if s errors are found , they will be fixed , i it you know , there there is a point at which i agree it becomes ridiculous unfortunately , uh , in in the sign thing that they signed , it says `` transcripts `` . `` <EOS> you 'll be you 'll be provided the transcripts when they 're available . `` <EOS> so let let me just suggest that <inbreath> uh , off - line that , uh , the people involved figure it out and take care of it before it 's july . <EOS>
and , <breath> for instance , uh , dan @ @ dan just sent me a message saying that cmu used , um , <mouth> something like ten gaussians per cluster you know , each each mixture has ten gaussians hmm . <EOS> we 're using sixty - four , so that 's <breath> obviously a big difference and give very poorly trained , uh , you know , gaussians that way , the turn - around time on the training when we train only the a male system with , uh , you know , our small training set , is <breath> less than twenty - four hours , <EOS>
yeah . <EOS> <inbreath> i i would actually double check with stephane at this point , yeah . <EOS> it 's hard with features , cuz you do n't know what they should look like . <EOS> i mean , you ca n't just , like , print the the values out in ascii and , you know , look at them , see if they 're not unless you had a lot of time <EOS>
i was gon na ask adam to , uh , say if he thought anymore about the demo stuff because <inbreath> it occurred to me that this is late may and the darpa meeting is in mid july . <EOS> i know that we were gon na do something with the transcriber interface is one thing , well , we were gon na do a mock - up , like , question answering or something , i thought , i mean in particular i would i would really hope that when we do this darpa meeting in july that we sort of have we 're we 're into production mode , somehow yeah . <EOS> so , um , uh , jane and adam and i had a meeting where we talked about the reorganization of the directory structure for all of the meeting for all the meeting recorder data . <EOS> um , we are gon na have this darpa meeting in the middle of july , given that we 've been we 've given a couple public talks about it already , spaced by months and months , i think it 'd be pretty bad if we continued to say none of this is available . <EOS> right . <EOS> so we can s we we wan na be able to say `` here is a subset that is available right now `` <EOS>
so let let me just suggest that <inbreath> uh , off - line that , uh , the people involved figure it out and take care of it before it 's july . `` <EOS> you 'll be you 'll be provided the transcripts when they 're available . `` <EOS> unfortunately , uh , in in the sign thing that they signed , it says `` transcripts `` . <EOS> i it you know , there there is a point at which i agree it becomes ridiculous yeah . <EOS> but th i mean , the editing will continue . <EOS> presumably if if s errors are found , they will be fixed , well , in principle , yes . <EOS> but , i mean , i if if if somebody actually did get into some legal issue with it then we and they do n't have to approve , you know , th an edited version , they can just give their approval to whatever version and that 's has been through the legal issues and so forth . <EOS>
because right now she has no choice but to operate in the mode that we already have working . <EOS> you know , that we we actually <breath> have a stream going and we know how how well it does and how and how it operates . <EOS> i hired two transcribers today . <EOS> i 'm thinking of hiring another one , which will because we 've had a lot of attrition . <EOS> but but actually i it 's so correct for so much of the time , that it 's an enormous time saver and it just gets tweaked a little around the boundaries . <EOS> the the the pre - segmentations are so much are s so extremely helpful . <EOS> wha - what you 'd really like is that they started with pre - segmented and were pre - segmented all the way through . <EOS> yeah . <EOS> as long as we have a record , i guess , of the original automatic one , we can always find out how well we would do fr from the recognition side by using those boundaries . <EOS> well , le let me put in another sort of a milestone kind of as as i did with the , uh , uh the the pipeline . <EOS>
e e u u the reason i 'm asking is because , uh , jane and i have just been talking , and she 's just been doing . <EOS> <laugh> uh , e a , you know , further hiring of transcribers . <EOS> and so we do n't sort of really know exactly what they 'll be doing , how long they 'll be doing it , and so forth , because right now she has no choice but to operate in the mode that we already have working . <EOS>
because in all our previous experiments , we had the uh , you know , we were essentially cheating by having the , um , you know , the h the hand - segmentations as the basis of the recognition . <EOS> and so now with thilo 's segmenter working so well , i think we should consider doing a <EOS>
because we could n't use the non - native all non - native meetings and <breath> it 's , well , probably below threshold on enough data for us for the things we 're looking at because the <breath> prosodic features are very noisy and so you you need a lot of data in order to model them . <EOS>
you know , chuck and i talked and the @ @ next thing to do is probably to tune the um , the size of the gaussian system , um , @ @ to to this to this feature vector , which we have n't done at all . <EOS> we just used the same <breath> configuration as we used for the <breath> for the standard system . <EOS> and , <breath> for instance , uh , dan @ @ dan just sent me a message saying that cmu used , um , <mouth> something like ten gaussians per cluster you know , each each mixture has ten gaussians hmm . <EOS> we 're using sixty - four , so that 's <breath> obviously a big difference and give very poorly trained , uh , you know , gaussians that way , the turn - around time on the training when we train only the a male system with , uh , you know , our small training set , is <breath> less than twenty - four hours , but the plp features work um , uh , you know , continue to improve the , as i said before , the uh using dan 's , uh , uh , vocal tract normalization option works very well . <EOS>
but there the main point is that , um , you know , it took us a while but we have the procedure for coupling the two systems <inbreath> debugged now and i mean , there 's still conceivably some bug somewhere in the way we 're feeding the tandem features uh , either generating them or feeding them to this to the <breath> sri system , that that i think that 's this that 's essentially the same as we use with the ce with the p l p fe features . <EOS> yeah . <EOS> there could be a bug in in the somewhere before that . <EOS> yeah . <EOS> <inbreath> i i would actually double check with stephane at this point , yeah . <EOS> it 's hard with features , cuz you do n't know what they should look like . <EOS> i mean , you ca n't just , like , print the the values out in ascii and , you know , look at them , see if they 're not unless you had a lot of time <EOS>
none <EOS>
well , maybe uh , since that that was a pretty short one , maybe we should talk about the ibm transcription status . <EOS> so , we , uh we did another version of the beeps , where we separated each beeps with a spoken digit . <EOS> i i hire i 've hired two extra people already , expect to hire two more . <EOS> which are now being edited by my head transcriber , <breath> in terms of spelling errors and all that . <EOS> she 's also checking through and mar and <breath> and monitoring , um , the transcription of another transcriber . <EOS> and and you indicated to me that we have a g a goal now , <breath> for the for the , um , <click> <breath> the , uh , darpa demo , of twenty hours . <EOS> so , i 'm gon na go up to twenty hours , be sure that everything gets processed , and released , and and that 's that 's what my goal is . <EOS> but i guess the other thing is that , um , that that 's kinda twenty hours asap because the longer before the demo we actually have the twenty hours , the more time it 'll be for people to actually do cool things with it . <EOS> yeah , i mean , i guess the so the difference if if , um , if the ibm stuff works out , the difference in the job would be that they p primarily would be checking through things that were already done by someone else ? <EOS>
n i 'm successfully , uh , increasing the error rate . <EOS> we 've always viewed it , anyway , as the major difference between the two , is actually in the smoothing , that the that the , um , <inbreath> plp , and and the reason plp has been advantageous in , uh , slightly noisy situations is because , <inbreath> plp does the smoothing at the end by an auto - regressive model , <EOS>
and the , uh porzel and the , uh , smartkom group are collecting some dialogues . <EOS> i mean , i do n't care what directory tree you have it under . <EOS> well , but but , <mouth> i put it under the same directory tree . <EOS> and just simply in the file you mark somewhere that this is this type of interaction , rather than another type of interaction . <EOS> we 're about we 're about half halfway through our disk right now . <EOS> well , but you can have it nw archive to you can have , <inbreath> uh , a non - backed - up disk nw archived , <EOS>
and the , uh porzel and the , uh , smartkom group are collecting some dialogues . <EOS> it 's just that it 's , you know , different directory , it 's called something different , it 's and just simply in the file you mark somewhere that this is this type of interaction , rather than another type of interaction . <EOS> i mean , i do n't care what directory tree you have it under . <EOS> well , but but , <mouth> i put it under the same directory tree . <EOS> we 're about we 're about half halfway through our disk right now . <EOS> well , but you can have it nw archive to you can have , <inbreath> uh , a non - backed - up disk nw archived , <EOS>
but but , uh , probably , if we had to pick something that we would talk on for ten minutes or so while they 're coming here . <EOS> or i guess it would be , you think , reorganization status , i mean , i think , chuck was the one who added out the agenda item . <EOS> i do n't really have anything to say other than that we still have n't done it . <EOS> so , naming conventions and things like that , that i 've been trying to keep actually up to date . <EOS> and i 've been sharing them with u - d uw folks also . <EOS>
so , is there something quick about absinthe that you ? <EOS> and got <mouth> <inbreath> a speedup roughly proportional to the number of processors times the clock cycle . <EOS> but the what it means is that it 's likely that for net training and forward passes , we 'll absinthe will be a good machine . <EOS> especially if we get a few more processors and upgrade the processors . <EOS>
so i guess the other thing that we were gon na talk about is is , uh , demo . <EOS> and , um , so , these are the demos for the uh , july , uh , meeting and , um darpa mee but maybe , uh maybe we 'll just put that off for now , given that but i think maybe we should have a a sub - meeting , i think , uh , probably , uh , adam and and , uh , chuck and me should talk about should get together and talk about that sometime soon . <EOS>
and the , uh porzel and the , uh , smartkom group are collecting some dialogues . <EOS> i mean , i do n't care what directory tree you have it under . <EOS> well , but but , <mouth> i put it under the same directory tree . <EOS> and just simply in the file you mark somewhere that this is this type of interaction , rather than another type of interaction . <EOS>
and the , uh porzel and the , uh , smartkom group are collecting some dialogues . <EOS> basically they have one person sitting in here , looking at a picture , and a wizard sitting in another room somewhere . <EOS> and , uh , they 're doing a travel task . <EOS> but it starts where the wizard is pretending to be a computer and it goes through a , uh , <breath> speech generation system . <EOS> should this be part of the corpus or not ? <EOS> and my attitude was yes , because there might be people who are using this corpus for acoustics , as opposed to just for language . <EOS> we simulate a computer breakdown halfway through the session , and so then after that , the person 's told that they 're now talking to a , uh to a human . <EOS> but of course they do n't know that it 's the same person both times . <EOS> and i said , `` well that 's silly , if if we 're gon na try to do it for a corpus , there might be people who are interested in acoustics . `` <EOS> i i would not say it was part of the meetings corpus . <EOS> so it 's it it i guess it the begs the question of what is the meeting corpus . <EOS> i think it 's i i think i th think the idea of two or more people conversing with one another is key . <EOS> well , this has two or more people conversing with each other . <EOS> we give everyone who 's involved as their own user id , give it session i ds , <inbreath> let all the tools that handle meeting recorder handle it , or do we wan na special case it ? <EOS> well , it it makes sense to handle it with the same infrastructure , since we do n't want to duplicate things unnecessarily . <EOS> but as far as distributing it , we should n't label it as part of this meeting corpus . <EOS> because we have , like , meetings that have a reason . <EOS> and and those and this sounds like it 's more of an experimental setup . <EOS> it 's scenario - based , it 's it 's human - computer interface <inbreath> it 's really pretty different . <EOS> it 's just that it 's , you know , different directory , it 's called something different , it 's and just simply in the file you mark somewhere that this is this type of interaction , rather than another type of interaction . <EOS> well , i don i would n't call reading digits `` meetings `` . <EOS> well , but but , <mouth> i put it under the same directory tree . <EOS> i mean , i do n't care what directory tree you have it under . <EOS>
well , but you can have it nw archive to you can have , <inbreath> uh , a non - backed - up disk nw archived , <EOS>
and i and i think a crucial part of that is the idea of of not wanting to do it until right before the next level zero back - up so that there wo n't be huge number of of added , <EOS>
um , or we could try some automated methods . <EOS> and my my tendency right now is , well , if ibm comes back with this meeting and the transcript is good , just let them do it . <EOS> yeah , i mean , i guess the so the difference if if , um , if the ibm stuff works out , the difference in the job would be that they p primarily would be checking through things that were already done by someone else ? <EOS> we could let ibm transcribe it . <EOS>
yeah , i mean , i guess the so the difference if if , um , if the ibm stuff works out , the difference in the job would be that they p primarily would be checking through things that were already done by someone else ? <EOS>
but maybe , uh maybe we 'll just put that off for now , given that <EOS>
we 're about we 're about half halfway through our disk right now . <EOS> so , once everything gets converted over to the disks we 're supposed to be using we 'll be probably , uh , seventy - five percent . <EOS> i 'm much more concerned about the backed - up . <EOS> what about putting the stuff on , like , c - cd - rom or dvd or something ? <EOS> i mean , when i say two or three years what i 'm saying is that i have had disks which are gone in a year . <EOS> but , uh i i you do n't want to per p have your only copy on a media that fails . <EOS> so so how about putting them on that plus , like on a on on dat or some other medium that is n't risky ? <EOS> icsi already has a perfectly good tape system and it 's more reliable . <EOS> so for archiving , we 'll just use tape . <EOS> but even without that , the back - up system is becoming saturated . <EOS> but but this back - up system is smart enough to figure out that something has n't changed and does n't need to be backed - up again . <EOS> well , but you can have it nw archive to you can have , <inbreath> uh , a non - backed - up disk nw archived , <EOS>
n i 'm successfully , uh , increasing the error rate . <EOS> so , i mean i 'm just playing with , um , the number of gaussians that we use in the the recognizer , and well , you have to sa you have to tell people that you 're you 're doing you 're trying the tandem features . <EOS> a and i 'm still tinkering with the plp features . <EOS> that was that was before i tried it on the females . <EOS> we had reached the point where , um , on the male portion of the development set , the , um or one of the development sets , i should say <inbreath> the , um the male error rate with , uh , icsi plp features was pretty much identical with , uh , sri features . <EOS> um , and the test data is callhome and switchboard . <EOS> oh , and plus the the vocal tract length normalization did n't actually made things worse . <EOS> so something 's really seriously wrong . <EOS> so but you see , now , between between the males and the females , there 's certainly a much bigger difference in the scaling range , than there is , say , just within the males . <EOS> d so the one thing that i then tried was to put in the low - pass filter , which we have in the so , most most hub - five systems actually band - limit the uh , at about , uh , thirty - seven hundred , um , hertz . <EOS> although , you know , normally , i mean , the channel goes to four four thousand . <EOS> um and it did n't hurt on the males either . <EOS> oh , and suddenly , also the v the vocal tract length normalization only in the test se on the test data . <EOS> maybe between one and two percent , um , for the females . <EOS> we 're looking at the discrepancy between the sri system and the sri system when trained with icsi features . <EOS> or maybe or maybe you 're doing one too many . <EOS> no , but with baum - welch , there should n't be an over - fitting issue , really . <EOS>
that was that was before i tried it on the females . <EOS> we had reached the point where , um , on the male portion of the development set , the , um or one of the development sets , i should say <inbreath> the , um the male error rate with , uh , icsi plp features was pretty much identical with , uh , sri features . <EOS> so but you see , now , between between the males and the females , there 's certainly a much bigger difference in the scaling range , than there is , say , just within the males . <EOS> d so the one thing that i then tried was to put in the low - pass filter , which we have in the um and it did n't hurt on the males either . <EOS> oh , and suddenly , also the v the vocal tract length normalization only in the test se on the test data . <EOS> maybe between one and two percent , um , for the females . <EOS> we 're looking at the discrepancy between the sri system and the sri system when trained with icsi features . <EOS> or maybe or maybe you 're doing one too many . <EOS> no , but with baum - welch , there should n't be an over - fitting issue , really . <EOS>
none <EOS>
darpa demos , <EOS>
but it does mean you need to be running a web server . <EOS> and so it it 's pretty big and complex . <EOS> uh and it would be difficult to port to windows uh the other option is dan did the tcl - tk thisl gui front - end for broadcast news and so i 've written some tools to convert everything into the right for file formats . <EOS> and the command line version of the indexing and the querying is now working . <EOS> so another idea i w t had just now actually for the demo was whether it might be of interest to sh to show some of the prosody uh <mouth> work that don 's been doing . <EOS>
um i hired several more transcribers , they 're making great progress . <EOS> uh i 've been uh finishing up the uh double checking . <EOS>
we 're sort of doing things in parallel , especially for the information retrieval stuff . <EOS>
um i spoke with dave johnson about putting all the meeting recorder stuff on non - backed - up disk to save the overhead of backup and uh , so the only issue here is the timing between getting more disks and uh recording meetings . <EOS> so i guess the idea is that we would be reserving the non - backed - up space for things that took less than twenty - four hours to recreate or something like that , things that are recreatable easily and also yeah , basically things that are recreatable . <EOS>
so i 've been doing a bunch of xml tools yeah and then the other thing also that thilo noticed is , on the microphone , i mean this is why i wan na use a g a tool to do it rather than the plain text the the one that shows up here , that will flash yellow if the mike is n't connected . <EOS>
good . <EOS> crosspads ? <EOS> who basically said `` if you 're not using them , could you return them ? `` <EOS> we we used them a couple times , we we get somebody to buy into the idea of doing this as part of the task . <EOS> uh part of the reason i think part of the reason that adam was so interested in uh the speechcorder sort of f idea from the beginning is he said from the beginning he hated taking notes well if you wanted to do that maybe the right architecture for it is to get a pda with a wireless card . <EOS> and and that way you can synchronize very easily with the the the meeting i mean for what what you 've been describing buttons would be even more convenient than anything else , maybe we could do like a student project , you know , maybe someone who wants to do this as their main like s project for something would be cool . <EOS> i mean if we had them out and sitting on the table people might use them a little more <EOS>
i know that uh that thilo you were , um , bringing the channeltrans interface onto the windows machine ? <EOS> yeah it 's it basically it 's done , and then at the same time i 'll probably rewire the room as per jane 's suggestion w uh we ordered uh more wireless , so that uh the first n channels are wireless , eh are the m the close - talking and the next n are far - field . <EOS> i mean there 's there 's all this stuff going on uh between uh andreas and and and dave and chuck and others with various kinds of runs uh um recognition runs , trying to figure things out about the features but it 's it 's all sort of in process , <EOS>
s so when we here were having this demo meeting , what we 're sort of coming up with is that we wan na have all these pieces together , to first order , by the end of the month next month . <EOS> uh that 'll that 'll give us that 'll give us a week or so to uh to port things over to my laptop and make sure that works , <EOS>
um i spoke with dave johnson about putting all the meeting recorder stuff on non - backed - up disk to save the overhead of backup but he thought it was a bad idea . <EOS> in fact what he said is doing the manual one , doing uh nw archive to copy it is a good idea and we should do that and have it backed up . <EOS> he w he 's a firm believer in in lots of different modalities of backup . <EOS> this data can not be recovered . <EOS> and if then a mistake is made and we lose the archive we should have the backup . <EOS> if it 's stationary it 's not going to go through the increment it 's not gon na burden things in the incremental backups . <EOS> just just the monthly full . <EOS> and we 're far enough away from saturation on full backups that it 's w probably ok . <EOS> and uh , so the only issue here is the timing between getting more disks and uh recording meetings . <EOS> so i guess the idea is that we would be reserving the non - backed - up space for things that took less than twenty - four hours to recreate or something like that , things that are recreatable easily and also yeah , basically things that are recreatable . <EOS>
in fact what he said is doing the manual one , doing uh nw archive to copy it is a good idea and we should do that and have it backed up . <EOS> he w he 's a firm believer in in lots of different modalities of backup . <EOS> this data can not be recovered . <EOS> and if then a mistake is made and we lose the archive we should have the backup . <EOS> if it 's stationary it 's not going to go through the increment it 's not gon na burden things in the incremental backups . <EOS> just just the monthly full . <EOS> and we 're far enough away from saturation on full backups that it 's w probably ok . <EOS> and uh , so the only issue here is the timing between getting more disks and uh recording meetings . <EOS> so i guess the idea is that we would be reserving the non - backed - up space for things that took less than twenty - four hours to recreate or something like that , things that are recreatable easily and also yeah , basically things that are recreatable . <EOS>
so i guess the idea is that we would be reserving the non - backed - up space for things that took less than twenty - four hours to recreate or something like that , things that are recreatable easily and also yeah , basically things that are recreatable . <EOS> and uh , so the only issue here is the timing between getting more disks and uh recording meetings . <EOS>
uh part of the reason i think part of the reason that adam was so interested in uh the speechcorder sort of f idea from the beginning is he said from the beginning he hated taking notes but uh by i i would suggest you return one . <EOS> because we we you know , we we have n't used it at all . <EOS> one would probably be fine . <EOS> maybe we could do like a student project , you know , maybe someone who wants to do this as their main like s project for something would be cool . <EOS> i mean if we had them out and sitting on the table people might use them a little more <EOS>
we we get somebody to buy into the idea of doing this as part of the task . <EOS> well if you wanted to do that maybe the right architecture for it is to get a pda with a wireless card . <EOS> and and that way you can synchronize very easily with the the the meeting i mean for what what you 've been describing buttons would be even more convenient than anything else , maybe we could do like a student project , you know , maybe someone who wants to do this as their main like s project for something would be cool . <EOS> i mean if we had them out and sitting on the table people might use them a little more <EOS>
i mean you hav sorta have to . <EOS> yeah , so we might wan na do it simultaneous . <EOS>
but my intention is to do a prettier user interface based either but it does mean you need to be running a web server . <EOS> and so it it 's pretty big and complex . <EOS> uh and it would be difficult to port to windows uh the other option is dan did the tcl - tk thisl gui front - end for broadcast news <EOS>
so yet again we should probably meet to talk about transcription formats in xml <EOS>
oh , quick question on that . <EOS> is do we have the < clears throat > the seat information ? <EOS> the seat information is on the key files for the ones which but i just had n't ever been putting it in the key files . <EOS> i never knew we were supposed to put it in the key file . <EOS> i mean this is why i wan na use a g a tool to do it rather than the plain text because with the plain text it 's very easy to skip those things . <EOS> yeah and then the other thing also that thilo noticed is , on the microphone , on channel zero it says hand - held mike or crown mike , you actually have to say which one . <EOS> and then uh also in a couple of places instead of filling the participants under `` participants `` they were filled in under `` description `` . <EOS> the the one that shows up here , that will flash yellow if the mike is n't connected . <EOS>
i mean this is why i wan na use a g a tool to do it rather than the plain text because with the plain text it 's very easy to skip those things . <EOS> yeah and then the other thing also that thilo noticed is , on the microphone , the the one that shows up here , that will flash yellow if the mike is n't connected . <EOS>
we we used them a couple times , and i have uh so my my feeling on it is that i think in principle it 's a really nice idea , and you have the time tags which makes it better tha than just taking ra raw notes . <EOS> on the other hand , i the down side for me was that i think the pen is really noisy . <EOS> so that you can you have a record of whatever it is you 've written . <EOS> so i if you take notes it 's a great little device . <EOS> and one of the reasons that it was brought up originally was because uh we were interested in in higher - level things , not just the , you know , microphone stuff but also summarization and so forth so that it 's synchronized with the time on that and then you have to download to an application , and then you have to figure out what the data formats are and convert it over if you wan na do anything with this information . <EOS> and you do n't wan na take notes well , what if you 're sitting there and you just wan na make an x <EOS>
so that it 's synchronized with the time on that and then you have to download to an application , and then you have to figure out what the data formats are and convert it over if you wan na do anything with this information . <EOS>
and you do n't wan na take notes well , what if you 're sitting there and you just wan na make an x <EOS>
so , i think this is gon na be a pretty short meeting because i have four agenda items , <EOS>
three of them were requested by jane who is not gon na be at the meeting today . <EOS> um well first of all with ibm i got a note from brian yesterday saying that they finally made the tape for the thing that we sent them a week or week and a half ago and hopefully next week we 'll have the transcription back from that . <EOS> so we 're doing some in parallel . <EOS>
so i 've been trying to keep a web page uh up to date f showing what the current status is of the trans of all the things we 've collected that 's the thing that i sent out just to foo people saying can you update these pages so jane also wanted to talk about participant approval , but i do n't really think there 's much to talk about . <EOS> i 'm gon na send out to the participants , uh , with links to web pages which contain the transcripts and allow them to suggest edits . <EOS> so but it 's just transcripts , not the not the audio ? <EOS> nope , they 'll have access to the audio also . <EOS> because the transcripts might not be right . <EOS> so , um the audio that they 're gon na have access to , will that be the uncompressed version ? <EOS> or will you have scripts that like uncompress the various pieces and yeah , it 's it 's probably going to have to be the uncompressed versions because , uh , uh , it takes too long to do random access decompression . <EOS>
um , darpa demo status , not much to say . <EOS> the back - end stuff is working out fine . <EOS> and , also um , i was just showing andreas , i got um an x waves kind of display , and i do n't know how much more we can do with it with like the prosodic stuff where we have like stylized pitches and signals and the transcripts on the bottom <EOS>
i 've been putting together uh transcriber things for windows but but it would be cool if the transcriber interface had like another window for the you know , maybe above the waveform where it would show some arbitrary valued function that is that is you know time synchron ti ti time synchronous with the wavform . <EOS> and see the pitch contours also . <EOS> just record the audio clip and show an image and i think that 's and , < clear throat > the more live , the better , but uh , given the crunch of time , we may have to retreat from it to some extent . <EOS> so i think for a lot of reasons , i think it would be very nice to have this transcriber interface be able to show some other interesting signal along with it <EOS>
and the last i item on the agenda is disk issues yet again . <EOS> we 're we 're only about thirty percent on the second disk . <EOS> but we are like ninety five percent , ninety eight percent on the scratch disks for the expanded meetings . <EOS>
uh , you know dave johnson is gone for , like , ten days , i mean it 's just a question of figuring out where they should be and hanging them , so this is a question that 's pretty hard to solve without talking to dave , but at any rate i think that there 's a there 's a longer term thing and there 's immediate need <EOS>
um yot i tested the uh the sort of final version of the plp configuration um on development test data for for this year 's hub - five test set . <EOS> and the recognition performance was exactly , and i mean exactly up to the you know , the first decimal , same as with the uh mel cepstra front - end . <EOS> they they were the males i think were slightly better and the females were slightly worse but nothing really . <EOS> and then the really nice thing was that if if we combine the two systems we get a one and a half percent improvement . <EOS>
and then we had some results on digits , uh , with um and the reason is basically there 's a whole bunch of read speech data in the hub - five training set . <EOS>
and then i th guess chuck and i had some discussions about how to proceed with the tandem uh system <EOS>
so something that we wan na do next meeting is is uh to put together um , a kind of reasonable list for ourselves of what is it , um , that we 've done . <EOS> i mean just sort of bulletize i mean o e do do i can i can dream up text but this is basically gon na lead to the annual report . <EOS> so just a week from tomorrow ? <EOS> um uh one thing i mean we <inbreath> < clear throat > in past meetings we had um also a you know various variously talked about the um work that w uh was happening sort of on the on the recognition side well , it 's that it 's just gon na be ver very boring for people who are not you know , sort of really interested in the details of the recognition system . <EOS> well , ok , so how many how many people here would not be interested in uh in a meeting about recognition ? <EOS> liz and jane probably . <EOS> why do n't we alternate this meeting every other week ? <EOS> but i do i do n't i mean a lot of times lately it seems like we do n't really have enough for a full meeting on meeting recorder . <EOS> and then if we find , you know we 're just not getting enough done , there 's all these topics not coming up , then we can expand into another meeting . <EOS> uh . <EOS> so uh . <EOS> um . <EOS> let 's chat about it with liz and jane when we get a chance , see what they think and <EOS>
because i i need to ask jane whether it 's it would be ok for her um , s some of her people to transcribe uh some of the initial data we got from the smartkom data collection , which is these short like five or seven minute sessions . <EOS> but to get going we would like some of the data transcribed right away so we can get started . <EOS> and so um i wanted to ask jane if if uh , you know , maybe one of their transcribers could could do i mean since these are very short , that should really be uh , <EOS>
i 'll send it out to the list telling people to look at it . <EOS>
so jane also wanted to talk about participant approval , but i do n't really think there 's much to talk about . <EOS> i 'm gon na send out to the participants , uh , with links to web pages which contain the transcripts and allow them to suggest edits . <EOS> so but it 's just transcripts , not the not the audio ? <EOS> nope , they 'll have access to the audio also . <EOS> because the transcripts might not be right . <EOS> so , um the audio that they 're gon na have access to , will that be the uncompressed version ? <EOS> or will you have scripts that like uncompress the various pieces and yeah , it 's it 's probably going to have to be the uncompressed versions because , uh , uh , it takes too long to do random access decompression . <EOS> yeah , i was just wondering because we 're uh running out of the un - backed - up disk space on <EOS>
uh , you know dave johnson is gone for , like , ten days , i mean it 's just a question of figuring out where they should be and hanging them , so this is a question that 's pretty hard to solve without talking to dave , but at any rate i think that there 's a there 's a longer term thing and there 's immediate need <EOS>
i have um i have an eighteen gig drive hanging off of my computer . <EOS> i mean it 's just a question of figuring out where they should be and hanging them , so this is a question that 's pretty hard to solve without talking to dave , but at any rate i think that there 's a there 's a longer term thing and there 's immediate need <EOS>
but but it would be cool if the transcriber interface had like another window for the you know , maybe above the waveform where it would show some arbitrary valued function that is that is you know time synchron ti ti time synchronous with the wavform . <EOS> and see the pitch contours also . <EOS> just record the audio clip and show an image and i think that 's and , < clear throat > the more live , the better , but uh , given the crunch of time , we may have to retreat from it to some extent . <EOS> so i think for a lot of reasons , i think it would be very nice to have this transcriber interface be able to show some other interesting signal along with it <EOS>
um uh one thing i mean we <inbreath> < clear throat > in past meetings we had um also a you know various variously talked about the um work that w uh was happening sort of on the on the recognition side well , it 's that it 's just gon na be ver very boring for people who are not you know , sort of really interested in the details of the recognition system . <EOS> well , ok , so how many how many people here would not be interested in uh in a meeting about recognition ? <EOS> liz and jane probably . <EOS> why do n't we alternate this meeting every other week ? <EOS> but i do i do n't i mean a lot of times lately it seems like we do n't really have enough for a full meeting on meeting recorder . <EOS> and then if we find , you know we 're just not getting enough done , there 's all these topics not coming up , then we can expand into another meeting . <EOS> uh . <EOS> so uh . <EOS> um . <EOS> let 's chat about it with liz and jane when we get a chance , see what they think and <EOS>
uh . <EOS> so uh . <EOS> um . <EOS> let 's chat about it with liz and jane when we get a chance , see what they think and <EOS>
but we are like ninety five percent , ninety eight percent on the scratch disks for the expanded meetings . <EOS> and , my original intention was like we would just delete them as we needed more space , but unfortunately we 're in the position where we have to deal with all the meeting data all at once , in a lot of different ways . <EOS> you we need about a gig per meeting . <EOS> i have um i have an eighteen gig drive hanging off of my computer . <EOS> but at any rate i think that there 's a there 's a longer term thing and there 's immediate need <EOS>
well , maybe it should get another rack . <EOS> uh , you know dave johnson is gone for , like , ten days , i have um i have an eighteen gig drive hanging off of my computer . <EOS> i mean it 's just a question of figuring out where they should be and hanging them , well i sent that message out to , i guess , you and dave asking for if we could get some disk . <EOS> the sysadmins would prefer to have one external drive per machine . <EOS> so they do n't want to stack up external drives . <EOS> so this is a question that 's pretty hard to solve without talking to dave , i think part of the reason why dave ca n't get the the new machines up is because he does n't have room in the machine room right now . <EOS> but at any rate i think that there 's a there 's a longer term thing and there 's immediate need <EOS>
uh the back - end is uh , going more slowly as i s i think i said before just cuz i 'm not much of a tcl - tk programmer . <EOS>
but but it would be cool if the transcriber interface had like another window for the you know , maybe above the waveform where it would show some arbitrary valued function that is that is you know time synchron ti ti time synchronous with the wavform . <EOS> and see the pitch contours also . <EOS> just record the audio clip and show an image and i think that 's and , < clear throat > the more live , the better , but uh , given the crunch of time , we may have to retreat from it to some extent . <EOS> so i think for a lot of reasons , i think it would be very nice to have this transcriber interface be able to show some other interesting signal along with it <EOS>
so something that we wan na do next meeting is is uh to put together um , a kind of reasonable list for ourselves of what is it , um , that we 've done . <EOS> i mean just sort of bulletize i mean o e do do i can i can dream up text but this is basically gon na lead to the annual report . <EOS> so just a week from tomorrow ? <EOS>
so i did n't send out agenda items because until five minutes ago we only had one agenda item and now we have two . <EOS> and if you disagree with it , why do n't you read it and give me comments on it ? <EOS>
so , uh , as most of you should know , i did send out the consent form thingies and we had decided that they have they only needed to sign once . <EOS> no . <EOS> at some point y you go around and get people to sign something ? <EOS> this is in the summer period and presumably people may be out of town . <EOS> but we can make the assumption , ca n't we ? <EOS> that , um , they will be receiving email , uh , most of the month . <EOS> it well , it well , you 're right . <EOS> sometimes somebody will be away that 's it 's , you know , just a certain risk to take . <EOS> cuz i because if we wan na be able to give it to people july fifteenth , if somebody 's gon na come back and say `` ok , i do n't want this and this and this used `` , uh , clearly we need some time to respond to that . <EOS>
but i thought it might be good to remind people two weeks prior to that cuz i because if we wan na be able to give it to people july fifteenth , if somebody 's gon na come back and say `` ok , i do n't want this and this and this used `` , uh , clearly we need some time to respond to that . <EOS> maybe uh , do we have mailing addresses for these people ? <EOS> al - altogether we 've got twenty people . <EOS> these people are people who read their email almost all the time . <EOS> i i really do n't see that it 's a problem . <EOS> i i think that it 's a common courtesy to ask them uh , to expect for them to , uh , be able to have @ @ us try to contact them , u just in case they had n't gotten their email . <EOS> i think they 'd appreciate it . <EOS> and if there 's half the people , say , who do n't respond at all by , you know , some period of time , <breath> we can just make a list of these people and i 'll hand it to administrative staff or whatever , and they 'll just call them up and say , you know , `` have you is is this ok ? <EOS> and would you please mail you know , mail adam that it is , if i if it , you know , is or not . `` <EOS> the other thing that there 's a psychological effect that at least for most people , that if they 've responded to your email saying `` yes , i will do it `` or `` yes , i got your email `` , they 're more likely to actually do it later than to just ignore it . <EOS> so what are we gon na do when we run into someone that we ca n't get in touch with ? <EOS> i do n't think , uh they 're so recent , these visitors . <EOS> i i mean , i i w i 'll be able to if you have any trouble finding them , i really think i could find them . <EOS> well , the way icsi goes , people , uh , who , uh , were here ten years ago still have acc <laugh> have forwards to other accounts and so on . <EOS> that if they give us contact information and that contact information is n't accurate that we fulfilled our burden . <EOS> and it i this discussion has made me think it might be nice to have a follow - up email within the next couple of days saying `` by the way , you know , we wan na hear back from you by x date <EOS>
we heard anything from ibm ? <EOS> so we got the transcript back from that one meeting . <EOS> everything seemed fine . <EOS> adam had a script that will put everything back together and there was well , there was one small problem but it was a simple thing to fix . <EOS> yeah . <EOS> now we have n't actually had anyone go through that meeting , to see whether the transcript is correct yeah . <EOS> it 's gon na have to go through our regular process . <EOS> i mean , the one thing i noticed is it did miss a lot of backchannels . <EOS> do you suppose that was because they were n't caught by the pre - segmenter ? <EOS> yeah . <EOS> they 're they 're not in the segmented . <EOS> it 's not that the ibm people did n't do it . <EOS>
the german ones will be ready for next week . <EOS> nsa . <EOS> um @ @ so , this is from one of the nsa meetings no , no . <EOS> these are these are our local transcriptions of the nsa meetings . <EOS> sometimes some speakers will insert foreign language terms . <EOS> and i 'm hoping that when the checked versions are run through the recognizer that you 'll see s substantial improvements in performance yeah , but i bet i bet they 're acoustically challenging parts anyway , though . <EOS> no , actually no . <EOS> oh , so it 's just jargon . <EOS> i mean this is cuz , you know you do n't realize in daily life how much you have top - down influences in what you 're hearing . <EOS> and it 's jar it 's jargon coupled with a foreign accent . <EOS> but we do n't i mean , our language model right now does n't know about these words anyhow . <EOS>
disk space , that for every meeting any meeting which has any bleeps in it we need yet another copy of . <EOS> well <inbreath> i you know , i think at a certain point , that copy that has the deletions will become the master copy . <EOS> so i i do n't want i really would rather make a copy of it , rather than bleep it out are you del are you bleeping it by adding ? <EOS> we 've been , uh , contacted by university of washington now , of course , to , um we sent them the transcripts that correspond to those six meetings and they 're downloading the audio files . <EOS> uh , the only one was don wanted to , uh , talk about disk space yet again . <EOS> we 've just ordered a hundred gigabytes . <EOS> uh , and there 's also an annual report . <EOS> now , so they so darpa just said do an annual report . <EOS> i mean , the the next thing on our agenda is to go back and look at the , um <mouth> the automatic alignments i i i learned from thilo what data we can use as a benchmark to see how well we 're doing on automatic alignments of the background speech or , of the foreground speech with background speech . <EOS> and then , uh , i guess , the new data that don will start to process you know , before we were working with these segments that were all synchronous we got our abstract accepted for this conference , isca workshop , in , um , uh , new jersey . <EOS> but we 're hoping to have a paper for that as well , which should be an interesting but , i mean , the good news is that that will have sort of the european experts in prosody sort of a different crowd , and i think we 're the only people working on prosody in meetings so far , uh , it 's isca workshop on prosody in speech recognition and understanding , or something like that y you going to , uh , eurospeech ? <EOS> i do n't have a paper but i 'd kinda like to go , <EOS>
so , just to repeat the thing bef that we said last week , it was there 's this suggestion of alternating weeks on <inbreath> more , uh , automatic speech recognition related or not ? <EOS> well we have n't really started , <EOS>
so , just to repeat the thing bef that we said last week , it was there 's this suggestion of alternating weeks on <inbreath> more , uh , automatic speech recognition related or not ? <EOS> well we have n't really started , but i figure also if they 're short agenda items , we could also do a little bit of each . <EOS> so next week we 'll do automatic transcription status , plus anything that 's real timely . <EOS>
and then maybe it 'd be good to set an explicit deadline , something like a week before that , uh , j july fifteenth date , or two weeks before . <EOS>
and we had decided that they have they only needed to sign once . <EOS> no . <EOS> at some point y you go around and get people to sign something ? <EOS> so . <EOS> <inbreath> a and , then , you know , say very clearly that if they do n't if we do n't hear from them , you know , as morgan suggested , by a certain time or after a certain <inbreath> period after we contact them <inbreath> that is implicitly giving their agreement . <EOS> well , they 've already signed a form . <EOS> and the s and the form was approved by human subjects , i i i em email is enough . <EOS> i mean , i 'm not a lawyer , but i 've been through these things a f things f like this a few times with lawyers now so i i <inbreath> i i 'm pretty comfortable with that . <EOS>
so if we get to a boundary case like that then maybe i will call the attorney about it . <EOS>
so i guess if you 're in both these types of meetings , you 'd have a lot . <EOS> i mean , it also depends on how many like , if we release this time it 's a fairly small number of meetings , well , what my s expectation is , is that we 'll send out one of these emails every time a meeting has been checked and is ready . <EOS>
but i thought it might be good to remind people two weeks prior to that cuz i because if we wan na be able to give it to people july fifteenth , if somebody 's gon na come back and say `` ok , i do n't want this and this and this used `` , uh , clearly we need some time to respond to that . <EOS> the other thing that there 's a psychological effect that at least for most people , that if they 've responded to your email saying `` yes , i will do it `` or `` yes , i got your email `` , they 're more likely to actually do it later than to just ignore it . <EOS> you know , an official ok from somebody is better than no answer , even if they responded that they got your email . <EOS> and it i this discussion has made me think it might be nice to have a follow - up email within the next couple of days saying `` by the way , you know , we wan na hear back from you by x date <EOS>
al - altogether we 've got twenty people . <EOS> these people are people who read their email almost all the time . <EOS> i i really do n't see that it 's a problem . <EOS> i i think that it 's a common courtesy to ask them uh , to expect for them to , uh , be able to have @ @ us try to contact them , u just in case they had n't gotten their email . <EOS> i think they 'd appreciate it . <EOS> and if there 's half the people , say , who do n't respond at all by , you know , some period of time , <breath> we can just make a list of these people and i 'll hand it to administrative staff or whatever , and they 'll just call them up and say , you know , `` have you is is this ok ? <EOS> and would you please mail you know , mail adam that it is , if i if it , you know , is or not . `` <EOS> the other thing that there 's a psychological effect that at least for most people , that if they 've responded to your email saying `` yes , i will do it `` or `` yes , i got your email `` , they 're more likely to actually do it later than to just ignore it . <EOS> you know , an official ok from somebody is better than no answer , even if they responded that they got your email . <EOS> so what are we gon na do when we run into someone that we ca n't get in touch with ? <EOS> i do n't think , uh they 're so recent , these visitors . <EOS> i i mean , i i w i 'll be able to if you have any trouble finding them , i really think i could find them . <EOS> well , the way icsi goes , people , uh , who , uh , were here ten years ago still have acc <laugh> have forwards to other accounts and so on . <EOS> that if they give us contact information and that contact information is n't accurate that we fulfilled our burden . <EOS>
disk space , that for every meeting any meeting which has any bleeps in it we need yet another copy of . <EOS> well <inbreath> i you know , i think at a certain point , that copy that has the deletions will become the master copy . <EOS> so i i do n't want i really would rather make a copy of it , rather than bleep it out are you del are you bleeping it by adding ? <EOS> and then overlapping . <EOS> so , it 's it 's exactly a censor bleep . <EOS> so what i really think is `` bleep `` <EOS>
that 's it 's , you know , just a certain risk to take . <EOS> you know , specify exactly uh , what , you know , how how they will be contacted w you know , when the whole thing starts , when they sign the <mouth> the agreement <inbreath> that so . <EOS> <inbreath> a and , then , you know , say very clearly that if they do n't if we do n't hear from them , you know , as morgan suggested , by a certain time or after a certain <inbreath> period after we contact them <inbreath> that is implicitly giving their agreement . <EOS> because if you if you , uh , do this <inbreath> and you then there 's a dispute later and , uh , some <inbreath> you know , someone who understands these matters concludes that they did n't have , uh , you know , enough opportunity to actually <inbreath> exercise their their right because although they signed this , they do n't know by which date to expect your email . <EOS> and so someone whose machine is down or whatever because people do n't read their email , or they 'll read and say `` i do n't care about that , i 'm not gon na delete anything `` and they don just wo n't reply to it . <EOS> maybe uh , do we have mailing addresses for these people ? <EOS> so if we get to a boundary case like that then maybe i will call the attorney about it . <EOS>
this is in the summer period and presumably people may be out of town . <EOS> but we can make the assumption , ca n't we ? <EOS> that , um , they will be receiving email , uh , most of the month . <EOS> it well , it well , you 're right . <EOS> sometimes somebody will be away that 's it 's , you know , just a certain risk to take . <EOS> you know , specify exactly uh , what , you know , how how they will be contacted w you know , when the whole thing starts , when they sign the <mouth> the agreement <inbreath> that so . <EOS> <inbreath> a and , then , you know , say very clearly that if they do n't if we do n't hear from them , you know , as morgan suggested , by a certain time or after a certain <inbreath> period after we contact them <inbreath> that is implicitly giving their agreement . <EOS> because although they signed this , they do n't know by which date to expect your email . <EOS> and so someone whose machine is down or whatever because people do n't read their email , or they 'll read and say `` i do n't care about that , i 'm not gon na delete anything `` and they don just wo n't reply to it . <EOS> maybe uh , do we have mailing addresses for these people ? <EOS>
you know , specify exactly uh , what , you know , how how they will be contacted w you know , when the whole thing starts , when they sign the <mouth> the agreement <inbreath> that so . <EOS> <inbreath> a and , then , you know , say very clearly that if they do n't if we do n't hear from them , you know , as morgan suggested , by a certain time or after a certain <inbreath> period after we contact them <inbreath> that is implicitly giving their agreement . <EOS> well , they 've already signed a form . <EOS> and the s and the form was approved by human subjects , because people do n't read their email , or they 'll read and say `` i do n't care about that , i 'm not gon na delete anything `` and they don just wo n't reply to it . <EOS> maybe uh , do we have mailing addresses for these people ? <EOS> i do n't think , uh they 're so recent , these visitors . <EOS> so what are we gon na do when we run into someone that we ca n't get in touch with ? <EOS> i i mean , i i w i 'll be able to if you have any trouble finding them , i really think i could find them . <EOS> that if they give us contact information and that contact information is n't accurate that we fulfilled our burden . <EOS> so if we get to a boundary case like that then maybe i will call the attorney about it . <EOS>
the german ones will be ready for next week . <EOS> nsa . <EOS> um @ @ so , this is from one of the nsa meetings no , no . <EOS> these are these are our local transcriptions of the nsa meetings . <EOS> sometimes some speakers will insert foreign language terms . <EOS> and i 'm hoping that when the checked versions are run through the recognizer that you 'll see s substantial improvements in performance yeah , but i bet i bet they 're acoustically challenging parts anyway , though . <EOS> no , actually no . <EOS> oh , so it 's just jargon . <EOS> i mean this is cuz , you know you do n't realize in daily life how much you have top - down influences in what you 're hearing . <EOS> and it 's jar it 's jargon coupled with a foreign accent . <EOS> but we do n't i mean , our language model right now does n't know about these words anyhow . <EOS>
disk space , that for every meeting any meeting which has any bleeps in it we need yet another copy of . <EOS> you have spare headsets ? <EOS> because i , uh , i could use one on my workstation , yeah , i still i still need to get a pair , too . <EOS> no . <EOS> just just just just buy them . <EOS> uh , the only one was don wanted to , uh , talk about disk space yet again . <EOS> the disk space was short . <EOS> we 've just ordered a hundred gigabytes . <EOS> yeah . <EOS> i mean , i guess the thing is is , all i need is to hang it off , like , <inbreath> the person who 's coming in , sonali 's , computer . <EOS> if you 're if you 're desperate , i have some space on my drive . <EOS> yeah . <EOS> i think i can find something if i 'm desperate <EOS>
um , we should talk a little bit about the plans for the uh the field trip next week . <EOS> and uh mostly uh first though about the logistics for it . <EOS> then maybe later on in the meeting we should talk about what we actually you know , might accomplish . <EOS> uh , in and kind of go around see what people have been doing talk about that , a r progress report . <EOS> um , essentially . <EOS> um and then uh another topic i had was that uh uh uh dave here had uh said uh `` give me something to do . `` <EOS> and so maybe we can discuss that a little bit . <EOS> uh , and uh , then uh , talk a little bit about about disks and resource resource issues that that 's starting to get worked out . <EOS> did you happen to find out anything about the ogi multilingual database ? <EOS> one they call the multi - language database , and another one is a twenty - two language , something like that . <EOS> but it 's also telephone speech . <EOS> well , actually , for the moment if we w do not want to use these phone databases , we we already have uh english , spanish and french uh , with microphone speech . <EOS> uh , actually , these three databases are um generic databases . <EOS> so w f for for uh italian , which is close to spanish , french and , i i uh , ti - digits we have both uh , digits training data and also more general training data . <EOS> well , we also have this broadcast news that we were talking about taking off the disk , which is <laugh> is microphone data for for english . <EOS> yeah , perhaps yeah , there is also timit . <EOS>
well , the inputs are one dimension of the cube , so , a a actually maybe now you 've got me sort of intrigued . <EOS> can you describe what what 's on the cube ? <EOS> basically , the the cube will have three dimensions . <EOS> the first dimension is the the features that we 're going to use . <EOS> and the second dimension , um , is the training corpus . <EOS> and that 's the training on the discriminant neural net . <EOS> and then , there 's the testing corpus . <EOS>
well , the inputs are one dimension of the cube , which , um , we 've talked about it being , uh , plp , um , m f c cs , um , j - jrasta , jrasta - lda um , for the training corpus corpus , um , we have , um , the the d digits < tapping sounds , writing on whiteboard > from the various languages . <EOS> so that 's , uh , three hundred and forty - three , uh , <laugh> different systems that are going to be developed . <EOS> something like seven things in each , uh each column . <EOS> so it seems like there 's there 's some peculiarities of the , uh of each of these dimensions that are getting sorted out . <EOS> and then , um , if if you work on getting the , uh , assembly lines together , and then the the pieces sort of get ready to go into the assembly line what 's what 's great about this is it sets it up in a very systematic way , so that , uh , once these all of these , you know , mundane but real problems get sorted out , we can just start turning the crank and , uh , the thing is that once you get a better handle on how much you can realistically do , uh , um , <mouth> concurrently on different machines , different sperts , and so forth , uh , and you see how long it takes on what machine and so forth , you can stand back from it and say , `` ok , if we look at all these combinations we 're talking about , and combinations of combinations , and so forth , `` you 'll probably find you ca n't do it all . <EOS> so then at that point , uh , we should sort out which ones do we throw away . <EOS> which of the combinations across you know , what are the most likely ones , <EOS>
um , we should talk a little bit about the plans for the uh the field trip next week . <EOS> and uh mostly uh first though about the logistics for it . <EOS> those of you who are not , you know , used to this area , it can be very tricky to get to the airport at at uh , you know , six thirty . <EOS> ok , so if if everybody can get here at six . <EOS>
dan david , um , put a new , um , drive onto abbott , that 's an x disk , um , i 've been going through and copying data that is , you know , some kind of corpus stuff usually , that that we 've got on a cd - rom or something , onto that new disk to free up space on other disks . <EOS> we have n't deleted them off of the slash - dc disk that they 're on right now in abbott , uh , but we i would like to go through sit down with you about some of these other ones and see if we can move them onto , um , this new disk also . <EOS> yeah , ok . <EOS>
i maybe we 're already there , or almost there , is goals for the for next week 's meeting . <EOS> uh . <EOS> i i i it seems to me that we wan na do is flush out what you put on the board here . <EOS> so w we can say what we 're doing , and , um , also , if you have sorted out , um , this information about how long i roughly how long it takes to do on what and , you know , what we can how many of these trainings , uh , uh , and testings and so forth that we can realistically do , uh , then one of the big goals of going there next week would be to to actually settle on which of them we 're gon na do . <EOS> and , uh , when we come back we can charge in and do it . <EOS>
and , um , also , if you have sorted out , um , this information about how long i roughly how long it takes to do on what and , you know , what we can how many of these trainings , uh , uh , and testings and so forth that we can realistically do , uh , then one of the big goals of going there next week would be to to actually settle on which of them we 're gon na do . <EOS> and , uh , when we come back we can charge in and do it . <EOS>
and uh and the other the the last topic i had here was , um , uh d dave 's fine offer to to , uh , do something <laugh> on this . <EOS> i mean he 's doing <laugh> he 's working on other things , but to to do something on this project . <EOS> so the question is , `` where where could we , uh , uh , most use dave 's help ? `` <EOS> and , um , <mouth> trying to have a closer look at the perhaps the , um , <mouth> speech , uh , noise detection or , uh , voiced - sound - unvoiced - sound detection let 's fall back to that . <EOS> but i think the first responsibility is sort of to figure out if there 's something that , uh , an an additional what an additional clever person could help with when we 're really in a crunch for time . <EOS> but if we could think of some some piece that 's that 's well defined , that he could help with , he 's expressing a will willingness to do that . <EOS> so wh that so the the other suggestion that just came up was , well what about having him work on the , uh , multilingual super f superset kind of thing . <EOS> uh , coming up with that and then , you know , training it training a net on that , say , um , from from , uh from timit or something . <EOS>
stephane , where you 're doing your computations . <EOS> it 's nutmeg and mustard , i think , well , you 're the you 're the disk czar now . <EOS> well , i 'll check on that . <EOS>
an another question occurred to me is is what were you folks planning to do about normalization ? <EOS> well , we were thinking about using this systematically for all the experiments . <EOS> but we think perhaps we can use the the best , uh , um , uh , normalization scheme as ogi is using , so , with parameters that they use there , i mean it 's i i we we seem to have enough dimensions as it is . <EOS>
clearly , there there 's no way we can even begin to do an any significant amount here unless we use multiple machines . <EOS> i mean there 's plenty of machines here and they 're n they 're often not in in a great great deal of use . <EOS> y you did a you did it on a spert board . <EOS> yes . <EOS> again , we do have a bunch of spert boards . <EOS> you could set up , uh , you know , ten different jobs , or something , to run on spert different spert boards or or we 're not going to get through any significant number of these . <EOS> so , with very limited time , we actually have really quite a quite a bit of computational resource available if you , you know , get a look across the institute and how little things are being used . <EOS> and , uh , the thing is that once you get a better handle on how much you can realistically do , uh , um , <mouth> concurrently on different machines , different sperts , and so forth , uh , and you see how long it takes on what machine and so forth , you can stand back from it and say , `` ok , if we look at all these combinations we 're talking about , and combinations of combinations , and so forth , `` you 'll probably find you ca n't do it all . <EOS>
so so , maybe we could look at articulatory type stuff , because that 's that 's the other route to go . <EOS> to really mark articulatory features , you really wan na look at the acoustics and and see where everything is , and we 're not gon na do that . <EOS> uh , the second class way of doing it is to look at the , uh , phones that are labeled and translate them into acoustic uh , uh articulatory , uh , uh , features . <EOS> so it wo n't really be right . <EOS> is that we could we could , uh , just translate instead of translating to a superset , just translate to articulatory features , some set of articulatory features and train with that . <EOS> we could do an interesting cheating experiment with that too . <EOS> so i was thinking , you know it made me think about this , that if it 'd be an interesting experiment just to see , you know , if you did get all of those right . <EOS>
so it seems like there 's there 's some peculiarities of the , uh of each of these dimensions that are getting sorted out . <EOS> and then , um , if if you work on getting the , uh , assembly lines together , and then the the pieces sort of get ready to go into the assembly line what 's what 's great about this is it sets it up in a very systematic way , so that , uh , once these all of these , you know , mundane but real problems get sorted out , we can just start turning the crank and , uh , the thing is that once you get a better handle on how much you can realistically do , uh , um , <mouth> concurrently on different machines , different sperts , and so forth , uh , and you see how long it takes on what machine and so forth , you can stand back from it and say , `` ok , if we look at all these combinations we 're talking about , and combinations of combinations , and so forth , `` you 'll probably find you ca n't do it all . <EOS> so then at that point , uh , we should sort out which ones do we throw away . <EOS> which of the combinations across you know , what are the most likely ones , <EOS>
cuz you do n't know who 's gon na call , clearly , there there 's no way we can even begin to do an any significant amount here unless we use multiple machines . <EOS> and then , um , if if you work on getting the , uh , assembly lines together , and then the the pieces sort of get ready to go into the assembly line what 's what 's great about this is it sets it up in a very systematic way , so that , uh , once these all of these , you know , mundane but real problems get sorted out , we can just start turning the crank and , uh , the thing is that once you get a better handle on how much you can realistically do , uh , um , <mouth> concurrently on different machines , different sperts , and so forth , uh , and you see how long it takes on what machine and so forth , you can stand back from it and say , `` ok , if we look at all these combinations we 're talking about , and combinations of combinations , and so forth , `` you 'll probably find you ca n't do it all . <EOS>
uh how do you know what language it is ? <EOS> somebody picks up the phone . <EOS> so thi this is their image . <EOS> so the phone does n't know what a what what your language is . <EOS> but the particular image that the cellular industry has right now is that it 's distributed speech recognition , where the , uh , uh , probabilistic part , and and s semantics and so forth are all on the servers , and you compute features of the uh , on the phone . <EOS> we might might or might not agree that that 's the way it will be in ten years , but that 's that 's that 's what they 're asking for . <EOS> so so i think that th th it is an important issue whether it works cross - language . <EOS>
were the digits , um , hand - labeled for phones ? <EOS> those were those were automatically derived by by dan using , um , embedded embedded training and alignment . <EOS> i i think you 're doing this test because you want to determine whether or not , uh , having s general speech performs as well as having specific speech . <EOS> so i was just wondering if the fact that timit you 're using the hand - labeled stuff from timit might be confuse the results that you get . <EOS> it would be another interesting scientific question to ask , `` is it because it 's a broad source or because it was , you know , carefully ? `` <EOS>
is n't there like a limit on the computation load , or d latency , or something like that for aurora task ? <EOS> so , there 's not really a limit . <EOS> what it is is that there 's there 's , uh it 's just penalty , that that if you 're using , uh , a megabyte , then they 'll say that 's very nice , but , of course , it will never go on a cheap cell phone . <EOS>
has ogi done anything about this issue ? <EOS> do they have do they have any kind of superset that they already have ? <EOS> well , they they they 're going actually the the other way , defining uh , phoneme clusters , apparently . <EOS> so they just throw the speech from all different languages together , then cluster it into sixty or fifty or whatever clusters ? <EOS> and e i perhaps u using broad phoneme classes , it 's it 's ok for um , uh classifying the digits , but as soon as you will have more words , well , words can differ with only a single phoneme , and which could be the same , uh , class . <EOS> so you 're saying that there may not be enough information coming out of the net to help you discriminate the words ? <EOS> yeah , fact , most confusions are within the phone phone classes , right ? <EOS>
so where are we on on uh <laugh> our runs ? <EOS> uh so . <EOS> uh we so as i was already said , we we mainly focused on uh four kind of features . <EOS> the plp , the plp with jrasta , the msg , and the mfcc from the baseline aurora . <EOS> uh , and we focused for the the test part on the english and the italian . <EOS> um . <EOS> we 've trained uh several neural networks on so on the ti - digits english and on the italian data and also on the broad uh english uh french and uh spanish databases . <EOS> and um , actually what we we @ @ observed is that if the network is trained on the task data it works pretty well . <EOS>
the first testing is with task data the second test is trained on a single language um with broad database , but the same language as the t task data . <EOS> but for italian we choose spanish which we assume is close to italian . <EOS> the third test is by using , um the three language database that 's including the w the the the one that it 's yeah . <EOS> it 's the broad data . <EOS> and the fourth test is uh excluding from these three languages the language that is the task language . <EOS>
but actually we did n't train network on uh both types of data we only did either task task data or uh broad data . <EOS> and then when we jump to the multilingual data it 's uh it become worse uh . <EOS> the error rate increase u of of of ten percent , relative . <EOS> example uh when we go from ti - digits training to timit training uh we lose uh around ten percent , twenty to to thirty percent further . <EOS> ok , but i think that given the pressure of time we probably want to draw because of that especially , we wan na draw some conclusions from this , and make some strong decisions for what we 're gon na do testing on before next week . <EOS>
so they 're they 're doing the the vad i guess they mean voice activity detection so again , it 's the silence so um their uh the results look pretty good . <EOS> so um i think that it 's it 's nice to do that in this because in fact , it 's gon na give a better word error result and therefore will help within an evaluation . <EOS> um . <EOS> uh , as you know , part of the problem with evaluation right now is that the word models are pretty bad and nobody wants has has approached improving them . <EOS> so um the question we 're gon na wan na go through next week when hynek shows up i guess is given that we 've been we 're uh looking at uh , by then i guess , combinations of features and multi - band uh , and we 've been looking at cross - language , cross task issues . <EOS> but they 've been looking at uh at these issues . <EOS> at the on - line normalization and the uh voice activity detection . <EOS> and i guess when he comes here we 're gon na have to start deciding about um what do we choose from what we 've looked at to um blend with some group of things in what they 've looked at and once we choose that , how do we split up the effort ? <EOS>
we have the little tiny ibm machine <laugh> that might someday grow up to be a big ibm machine . <EOS> it 's got s slots for eight , i think we only got two so far , yeah , i mean you can check with uh dave johnson . <EOS> and somebody could do you know , uh , check out uh the multi - threading libraries . <EOS> i mean , i guess the prudent thing to do would be for somebody to do the work on on getting our code running on that machine with two processors even though there are n't five or eight . <EOS>
uh so , act actually we have discussed uh @ @ um , these and and we were thinking perhaps that uh the way we use the tandem is not if we trained the networks on the on a language and a t or a specific task , um , what we ask is to the network is to put the bound the decision boundaries somewhere in the space . <EOS> and uh mmm and ask the network to put one , at one side of the for for a particular phoneme at one side of the boundary decision boundary and so there is kind of reduction of the information there that 's not correct because if we change task and if the phonemes are not in the same context in the new task , obviously the decision boundaries are not should not be at the same place . <EOS> but the way the feature gives the the way the network gives the features is that it reduce completely the it removes completely the information a lot of information from the the features by uh uh placing the decision boundaries at optimal places for one kind of data the way we we do it now is that we have a neural network and basically the net network is trained almost to give binary decisions . <EOS>
and once you the other thing is that once you represent start representing more and more context it is uh much more um specific to a particular task in language . <EOS> for instance you may have some kinds of contexts that will never occur in one language and will occur frequently in the other , the issue of getting enough training for a particular kind of context becomes harder . <EOS> we already actually do n't have a huge amount of training data the way we we do it now is that we have a neural network and basically the net network is trained almost to give binary decisions . <EOS> but it would still be even more of a binary decision . <EOS> that would be even even more distinct of a binary decision . <EOS> i mean we we could disagree about it at length but the the real thing is if you 're interested in it you 'll probably try it and and we 'll see . <EOS>
so i think hynek will be here monday . <EOS> so i think , you know , we need to choose the choose the experiments carefully , so we can get uh key key questions answered uh before then so um the question we 're gon na wan na go through next week when hynek shows up i guess is given that we 've been <EOS>
so um the question we 're gon na wan na go through next week when hynek shows up i guess is given that we 've been we 're uh looking at uh , by then i guess , combinations of features and multi - band uh , and we 've been looking at cross - language , cross task issues . <EOS> but they 've been looking at uh at these issues . <EOS> at the on - line normalization and the uh voice activity detection . <EOS> and i guess when he comes here we 're gon na have to start deciding about um what do we choose from what we 've looked at to um blend with some group of things in what they 've looked at and once we choose that , how do we split up the effort ? <EOS>
and i guess when he comes here we 're gon na have to start deciding about um what do we choose from what we 've looked at to um blend with some group of things in what they 've looked at and once we choose that , how do we split up the effort ? <EOS>
yeah , i mean you can check with uh dave johnson . <EOS> and somebody could do you know , uh , check out uh the multi - threading libraries . <EOS> but . <EOS> notice how i said somebody and <laugh> turned my head your direction . <EOS> that 's one thing you do n't get in these recordings . <EOS> and then we 'd be set for when we did have five or eight , to have it really be useful . <EOS> there 's there 's there 's gon na be debugging hassles i mean , i guess the prudent thing to do would be for somebody to do the work on on getting our code running on that machine with two processors even though there are n't five or eight . <EOS>
and somebody could do you know , uh , check out uh the multi - threading libraries . <EOS> i mean , i guess the prudent thing to do would be for somebody to do the work on on getting our code running on that machine with two processors even though there are n't five or eight . <EOS> there 's there 's there 's gon na be debugging hassles and then we 'd be set for when we did have five or eight , to have it really be useful . <EOS>
so , um i could try to get um the train the neural network trainings or the htk stuff running under linux , and to start with i 'm wondering which one i should pick first . <EOS> uh , probably the neural net <EOS>
uh , probably the neural net <EOS>
but actually there is something important , is that um we made a lot of assumption concerning the on - line normalization and we just noticed uh recently that uh the approach that we were using was not uh leading to very good results when we used the straight features to htk . <EOS> so what we see that is there is that um uh the way we were doing this was not correct , when we use the networks our number are better that uh pratibha results . <EOS> and basically , the first thing is the mmm , alpha uh value . <EOS> um , i used point five percent , which was the default value in the in the programs here . <EOS> and pratibha used five percent . <EOS> i assume that this was not important because uh previous results from from dan and show that basically the both both values g give the same same uh results . <EOS> it was true on uh ti - digits but it 's not true on italian . <EOS> uh , second thing is the initialization of the stuff . <EOS> actually , uh what we were doing is to start the recursion from the beginning of the utterance . <EOS> and using initial values that are the global mean and variances measured across the whole database . <EOS> and pratibha did something different is that he uh she initialed the um values of the mean and variance by computing this on the twenty - five first frames of each utterance . <EOS> mmm . <EOS> there were other minor differences , so . <EOS> uh , i changed the code uh and now we have a baseline that 's similar to the ogi baseline . <EOS> well , the the the networks are retaining with these new features . <EOS>
there there is another difference , is that the noise the noises are different . <EOS> well , for for the italian part i mean the uh the um networks are trained with noise from aurora ti - digits , and perhaps the noise are quite different from the noises in the speech that italian . <EOS> uh <laugh> <inbreath> um now , what 's the noise condition um of the training data the noise condition is the same so there 's not a statistical sta a strong st statistically different noise characteristic between uh the training and test no these are the s s s same noises , at least at least for the first for the well - matched , <EOS>
none <EOS>
yeah , so for the italian the results are <outbreath> uh stranger so what appears is that perhaps spanish is not very close to italian because uh , well , when using the the network trained only on spanish it 's the error rate is almost uh twice the baseline error rate . <EOS>
so they 're they 're doing the the vad i guess they mean voice activity detection so again , it 's the silence so um their uh the results look pretty good . <EOS> so um i think that it 's it 's nice to do that in this because in fact , it 's gon na give a better word error result and therefore will help within an evaluation . <EOS> um . <EOS> uh , as you know , part of the problem with evaluation right now is that the word models are pretty bad and nobody wants has has approached improving them . <EOS>
so , uh , you 've got some , uh , xerox things to pass out ? <EOS> ok , s so there is kind of summary of what has been done summary of experiments since , well , since last week and also since the we 've started to run work on this . <EOS> um . <EOS> so since last week we 've started to fill the column with um <mouth> uh features w with nets trained on plp with on - line normalization <EOS>
so , uh , you 've got some , uh , xerox things to pass out ? <EOS> yeah , i 'm sorry for the table , but as it grows in size , uh , it . <EOS> uh , so for th the last column we use our imagination . <EOS> ok , s so there is kind of summary of what has been done summary of experiments since , well , since last week and also since the we 've started to run work on this . <EOS> um . <EOS> so since last week we 've started to fill the column with um <mouth> uh features w with nets trained on plp with on - line normalization but with delta also , uh when we use the large training set using french , spanish , and english , you have one hundred and six without delta and eighty - nine with the delta . <EOS> a and again all of these numbers are with a hundred percent being , uh , the baseline performance , and training with other languages is a little bit worse . <EOS> we have a ninety - one number , so , it 's multi - english , and , yeah , and here the gap is still more important between using delta and not using delta . <EOS> if y if i take the training s the large training set , it 's we have one hundred and seventy - two , and one hundred and four when we use delta . <EOS> uh . <EOS> even if the contexts used is quite the same , except for the multi - english , which is always one of the best . <EOS> then we started to work on a large dat database containing , uh , sentences from the french , from the spanish , from the timit , from spine , uh from uh english digits , and from italian digits . <EOS> and uh , actually we did this before knowing the result of all the data , uh , so we have to to redo the uh the experiment training the net with , uh plp , but with delta . <EOS> and first in the experiment - one i i do i i use different mlp , yeah , and test across everything . <EOS> so i guess the other thing is to take you know if one were to take , uh , you know , a couple of the most successful of these , yeah , try all these different tests . <EOS> we still have to work on finnish , um , basically , to make a decision on which mlp can be the best across the different languages . <EOS> for the moment it 's the timit network , and perhaps the network trained on everything . <EOS> uh , well , the next part of the document is , well , basically , a kind of summary of what everything that has been done . <EOS> so . <EOS> we have seventy - nine m l ps trained on ten on ten different databases . <EOS>
um , discussion with hynek , sunil and pratibha for trying to plug in their our our networks with their within their block diagram , uh , where to plug in the the network , uh , after the the feature , before as um a as a plugin or as a anoth another path , actually hynek would like to see , perhaps if you remember the block diagram there is , uh , temporal lda followed b by a spectral lda for each uh critical band . <EOS> and he would like to replace these by a network which would , uh , make the system look like a trap . <EOS> um , there are still open questions there , the future work is , well , try to connect to the to make to plug in the system to the ogi where to put the mlp basically . <EOS> so . <EOS> uh , we we wan na get their path running here , if so , we can add this other stuff . <EOS> as an additional path yeah , the way we want to do it perhaps is to just to get the vad labels and the final features . <EOS> so they will send us the well , provide us with the feature files , so we so . <EOS> first thing of course we 'd wan na do there is to make sure that when we get those labels of final features is that we get the same results as them . <EOS> without putting in a second path . <EOS> yeah just th w i i just to make sure that we have we understand properly what things are , our very first thing to do is to is to double check that we get the exact same results as them on htk . <EOS>
so , on the msg uh problem um , i think that in in the um , in the short time solution um , that is , um , trying to figure out what we can proceed forward with to make the greatest progress , i think it 's kind of in category that it 's , it it may be complicated . <EOS> and uh it might be if someone 's interested in it , uh , certainly encourage anybody to look into it in the longer term , once we get out of this particular rush uh for results . <EOS> but in the short term , unless you have some some s strong idea of what 's wrong , but but my my guess would be that it 's something that is a simple thing that could take a while to find . <EOS>
yeah , that seems like a good thing to do , probably , not uh again a short - term sort of thing . <EOS> the two of the main issues perhaps are still the language dependency <inbreath> and the noise dependency . <EOS> and perhaps to try to reduce the language dependency , we should focus on finding some other kind of training targets . <EOS> for moment you use we use phonetic targets but we could also use articulatory targets , soft targets , and perhaps even , um use networks that does n't do classification but just regression and well , basically com com compute features and noit not , nnn , features without noise . <EOS> i mean uh , transform the fea noisy features <inbreath> in other features that are not noisy . <EOS>
so i guess the other thing is to take you know if one were to take , uh , you know , a couple of the most successful of these , yeah , and test across everything . <EOS> yeah , try all these different tests . <EOS> but one of the core quote `` open questions `` for that is um , um , if we take the uh you know , the best ones here , maybe not just the best one , but the best few or something you want the most promising group from these other experiments . <EOS> we know that there 's a mis there 's a uh a a loss in performance when the neural net is trained on conditions that are different than than , uh we 're gon na test on , but well , if you look over a range of these different tests um , how well do these different ways of combining the straight features with the mlp features , uh stand up over that range ? <EOS> look at these different ways of combining it . <EOS> and just look take that case and then look over all the different things . <EOS> how does that how does that compare between the all the different test sets , and for and for the couple different ways that you have of of of combining them . <EOS> um . <EOS> how well do they stand up , over the <EOS>
but you have two two effects , the effect of changing language and the effect of training on something that 's viterbi - aligned instead of hand hand - labeled . <EOS> do you think the alignments are bad ? <EOS> i mean , have you looked at the alignments at all ? <EOS> what the viterbi alignment 's doing ? <EOS> might be interesting to look at it . <EOS> yeah . <EOS> but yeah . <EOS> but , perhaps it 's not really the the alignment that 's bad but the just the ph phoneme string that 's used for the alignment the pronunciation models and so forth there there might be errors just in the in in the ph string of phonemes . <EOS> yeah , so this is not really the viterbi alignment , <EOS>
so . <EOS> uh , we we wan na get their path running here , if so , we can add this other stuff . <EOS> as an additional path yeah , the way we want to do it perhaps is to just to get the vad labels and the final features . <EOS> so they will send us the well , provide us with the feature files , so we so . <EOS> first thing of course we 'd wan na do there is to make sure that when we get those labels of final features is that we get the same results as them . <EOS> without putting in a second path . <EOS> yeah just th w i i just to make sure that we have we understand properly what things are , our very first thing to do is to is to double check that we get the exact same results as them on htk . <EOS>
so , it it 's still it hurts you seems to hurt you a fair amount to add in this french and spanish . <EOS> i wonder why well stephane was saying that they were n't hand - labeled , the french and the spanish . <EOS> yeah ! <EOS> as we mentioned , timit is the only that 's hand - labeled , and perhaps this is what makes the difference . <EOS> yeah , the other are just viterbi - aligned . <EOS> well , the timit network is still the best the fact that it 's it 's hand - labeled . <EOS> but you have two two effects , the effect of changing language and the effect of training on something that 's viterbi - aligned instead of hand hand - labeled . <EOS> do you think the alignments are bad ? <EOS> i mean , have you looked at the alignments at all ? <EOS> what the viterbi alignment 's doing ? <EOS> might be interesting to look at it . <EOS> yeah . <EOS> but yeah . <EOS> but , perhaps it 's not really the the alignment that 's bad but the just the ph phoneme string that 's used for the alignment the pronunciation models and so forth there there might be errors just in the in in the ph string of phonemes . <EOS> yeah , so this is not really the viterbi alignment , we can , we can tell which training set gives the best result , but <mouth> we do n't know exactly why . <EOS>
yeah , i 'm sorry for the table , but as it grows in size , uh , it . <EOS> uh , so for th the last column we use our imagination . <EOS> a and again all of these numbers are with a hundred percent being , uh , the baseline performance , and eighty - nine with the delta . <EOS> uh when we use the large training set using french , spanish , and english , you have one hundred and six without delta yeah , so , it 's multi - english , we have a ninety - one number , and training with other languages is a little bit worse . <EOS> if y if i take the training s the large training set , it 's we have one hundred and seventy - two , and one hundred and four when we use delta . <EOS> and this is the results are on the other document . <EOS> yeah , we ju just to be clear , the numbers here are uh recognition accuracy . <EOS> yes , and the baseline the baseline have i is eighty - two . <EOS> baseline is eighty - two . <EOS> yeah , eh , actually , if w we look at the table , the huge table , when you said the baseline system was uh , uh eighty - two percent , that was trained on what and tested on what ? <EOS> that was , uh italian mismatched d uh , uh , digits , uh , is the testing , and the training is italian digits ? <EOS> yeah . <EOS> so the `` mismatch `` just refers to the noise and and , uh microphone and so forth , so , um so what that says is that in a matched condition , <sniff> we end up with a fair amount worse putting in the uh plp . <EOS> now w would do we have a number , i suppose for the matched i i do n't mean matched , but uh use of italian training in italian digits for plp only ? <EOS> so this is basically this is in the table . <EOS> uh so the number is fifty - two , another table . <EOS> fifty - two percent . <EOS> fift - so no , it 's it 's the no , fifty - two percent of eighty - two ? <EOS> of of of uh eighteen eighty . <EOS> of eighteen . <EOS> so it 's it 's error rate , basically . <EOS> it 's plus six . <EOS> it 's er error rate ratio . <EOS> oh this is accuracy ! <EOS> yeah . <EOS> uh , so we have nine nine let 's say ninety percent . <EOS> oh , i 'm sorry , i k i keep getting confused because this is accuracy . <EOS> yeah , sorry . <EOS>
here the problem seems to be is that we do n't have a hug a really huge net with a really huge amount of training data . <EOS> but we have s f for this kind of task , i would think , sort of a modest amount . <EOS> i mean , a million frames actually is n't that much . <EOS> we have a modest amount of of uh training data from a couple different conditions , and then uh in yeah , that and the real situation is that there 's enormous variability that we anticipate in the test set in terms of language , and noise type uh , and uh , uh , channel characteristic , <EOS>
and then uh in yeah , that and the real situation is that there 's enormous variability that we anticipate in the test set in terms of language , and noise type uh , and uh , uh , channel characteristic , <EOS>
none <EOS>
uh today we 're looking at a number of uh things we 're trying it 's only one small experiment to know what happened . <EOS> to apply also to in include also the the silence of the mlp we have the fifty - six form and the silence to pick up the silence and we include those . <EOS> the silence plus the klt output ? <EOS> no they 're i think there is this silence in addition to the um klt outputs it is because we we we just keep uh we do n't keep all the dimensions after the klt and we not s we are not sure if we pick we have the silence . <EOS> so we try to add the silence also in addition to the these twenty - eight dimensions . <EOS> do you e um they mentioned made some uh when i was on the phone with sunil they they mentioned some weighting scheme that was used to evaluate all of these numbers . <EOS> um well it 's uh forty percent for ti - digit , sixty for all the speechdat - cars , um and we do n't have the ti - digits part yet ? <EOS> generally what you observe with ti - digits is that the result are very close whatever the the system . <EOS> so it looks to me i guess the same given that we have to take the filt ones out of the the running because of this delay problem so it looks to me like the ones you said i agree are are the ones to look at but i just would add the the the second row one so if we can know what how many words are in each and then um dave uh dave promised to get us something tomorrow which will be there as far as they 've gotten <laugh> friday and then we 'll operate with that do we fix the system uh tomorrow or do we fix the system on tuesday ? <EOS> i think we fixed on tuesday , yeah . <EOS> i yeah , ok except that we do have to write it up . <EOS> so maybe what we do is we we we uh as soon as we get the data from them we start the training and so forth but we start the write - up right away because as you say there there 's only minor differences between these . <EOS> i think you we could we could start soon , yeah . <EOS> write up something . <EOS> well anyway , sounds like there 'll be a lot to do just to <laugh> work with our partners to fill out the tables <laugh> over the next uh next few days yes , so i mean i think we have to actually get it done tuesday um so then next thursday we can sort of have a little aftermath but my assumption is that we basically have to be done tuesday . <EOS>
and so have you put all these numbers together into a single number representing that ? <EOS> uh not yet . <EOS> ok so that should be pretty easy to do and that would be good then we could compare the two and say what was better . <EOS>
so you know how many words are in uh one of these test sets ? <EOS> um it 's it depends well the well matched is generally larger than the other sets see the i mean the reason i 'm asking is is is we have all these small differences and i do n't know how seriously to take them , right ? <EOS> so uh i if if you had uh just you know to give an example , if you had uh um if you had a thousand words then uh a a tenth of a percent would just be one word , so so it would n't mean anything . <EOS> um so um yeah it be kind of i 'd kind of like to know what the sizes of these test sets were actually . <EOS> well also just to know the numbers , so anyway if you could just mail out what those numbers are and then then that that be great . <EOS>
so it looks to me i guess the same given that we have to take the filt ones out of the the running because of this delay problem so it looks to me like the ones you said i agree are are the ones to look at but i just would add the the the second row one so basically we will i think we 'll try to to focus on these three architectures and and perhaps i was thinking also a fourth one with just just a single klt yeah , i mean that would be pretty low maintenance to try it . <EOS>
do we fix the system uh tomorrow or do we fix the system on tuesday ? <EOS> i think we fixed on tuesday , yeah . <EOS> i yeah , ok except that we do have to write it up . <EOS> so maybe what we do is we we we uh as soon as we get the data from them we start the training and so forth but we start the write - up right away because as you say there there 's only minor differences between these . <EOS> i think you we could we could start soon , yeah . <EOS> write up something . <EOS> yes , so i mean i think we have to actually get it done tuesday but my assumption is that we basically have to be done tuesday . <EOS> um so then next thursday we can sort of have a little aftermath <EOS>
yeah , and and i i would you know , i would i 'd kind of like to see it maybe i can i can edit it a bit <EOS>
um so then next thursday we can sort of have a little aftermath and and uh maybe next meeting we can start talking a little bit about where we want to go from here uh in terms of uh the research . <EOS> um you know what things uh did you think of when you were uh doing this process that uh you just did n't really have time to adequately work on <EOS>
uh so one of the ideas that you had mentioned last time was having a a second um silence detection . <EOS> um yeah so it seems f for the the well match and mismatched condition it 's uh it brings something . <EOS> uh but uh actually apparently there are there 's no room left for any silence detector at the server side because of the delay . <EOS> oh we ca n't do it . <EOS> no . <EOS>
none <EOS>
so , i got , uh these results from , uh , stephane . <EOS> also , um , i think that , uh um we might hear later today , about other results . <EOS> i think s that , uh , there were some other very good results that we 're gon na wan na compare to . <EOS> but , <inbreath> r our results from other other places , you know most of the time , even i mean even though it 's true that the overall number for danish we did n't improve it i mean , i think that , uh , one of the things that hynek was talking about was understanding what was in the other really good proposals and trying to see if what should ultimately be proposed is some , uh , combination of things . <EOS> so < nasal inbreath > um , since we have a bit farther to travel than <laugh> some of the others , <inbreath> uh , we 'll have to get done a little quicker . <EOS> but , um , i mean , it 's just tracing down these bugs . <EOS> i mean , just exactly this sort of thing of , you know , why why these features seem to be behaving differently , uh , in california than in oregon . <EOS>
we have a little bit of time on that , actually . <EOS> we have a day or so , when when when do you folks leave ? <EOS> sunday ? <EOS> until saturday midnight , or something , we have when is the development set i mean , the , uh , uh , test set results due ? <EOS> uh , probably the day after they leave , but we 'll have to <laugh> we 'll have to stop it the day before we leave . <EOS> i think tha i think the the meeting is on the thirteenth or something . <EOS> this tuesday , and the the , uh , results are due like the day before the meeting or something . <EOS>
the other thing that strikes me , just looking at these numbers is , just taking the best cases , i mean , some of these , of course , even with all of our our wonderful processing , still are horrible kinds of numbers . <EOS> but just take the best case , the well - matched uh , german case after er well - matched danish after we the kind of numbers we 're getting are about eight or nine uh p percent error per digit . <EOS> this is obviously not usable , i mean , if you have ten digits for a phone number i mean , every now and then you 'll get it right . <EOS> so , in a way , that 's , you know , that 's sort of the dominant thing is that even , say on the development set stuff that we saw , the , uh , the numbers that , uh , that alcatel was getting when choosing out the best single numbers , <inbreath> it was just you know , it was n't good enough for for a a for a real system . <EOS> so , uh , we still have stuff to do . <EOS> uh <laugh> uh <laugh> i mean , they 're much better than they were , you know . <EOS> we 're talking about thirty to sixty percent , uh , error rate reduction . <EOS> that 's that 's really great stuff to to do that in relatively short time . <EOS> but even after that it 's still , you know , so poor that that , uh , no one could really use it . <EOS>
because , um , um , when we use the straight features , we are not able to get these nice number you know most of the time , even i mean even though it 's true that the overall number for danish we did n't improve it y actually , uh , um , for the danish , there 's still some kind of mystery with the icsi ogi one , i mean . <EOS> uh , so , uh , that 's probably something wrong with the features that we get from ogi . <EOS> uh , and sunil is working on on trying to to check everything . <EOS> but , um , i mean , it 's just tracing down these bugs . <EOS> i mean , just exactly this sort of thing of , you know , why why these features seem to be behaving differently , uh , in california than in oregon . <EOS>
so what are you doing ? <EOS> uh , well , we 've a little bit worked on trying to see , uh , what were the bugs and the problem with the latencies . <EOS>
none <EOS>
you you had a discussion with sunil about this though ? <EOS> no . <EOS> yeah , you should talk with him . <EOS> uh , cuz they could be doing the same thing and or something . <EOS> we just we just have to be in contact more . <EOS> i think that the the fact that we we did that with had that thing with the latencies was indicative of the fact that there was n't enough communication . <EOS> but , well , when we add up everything it 's it will be alright . <EOS> so it would be around two hundred and forty what 's the allowable ? <EOS> two - fifty , uh , well the people who had very low latency want it to be low uh , very <laugh> very very narrow , uh , latency bound . <EOS> unfortunately we 're the main ones with long latency , a person i do n't think a person can tell the difference between , uh , you know , a quarter of a second and a hundred milliseconds , i 'm not even sure if we can tell the difference between a quarter of a second and half a second . <EOS> i mean it just it feels so quick . <EOS> uh , one thing that would be no good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , um , run a good vad , and and determine boundaries . <EOS> and then given those boundaries , then have everybody do the recognition . <EOS> uh , i guess they argued about that yesterday <EOS>
uh , maybe we can talk about a couple other things briefly , so you 're coming up with your quals proposal , um , but i 'm , uh , looking into extending the work done by larry saul and john allen and uh mazin rahim . <EOS> so , uh , y you want to talk maybe a c two or three minutes about what we 've been talking about today and other days ? <EOS> we 're interested in , um , methods for far mike speech recognition , um , mainly , uh , methods that deal with the reverberation in the far mike signal . <EOS>
do you have news from the conference talk ? <EOS> yesterday morning on video conference . <EOS> no , nobody 's told me anything . <EOS> no , that would have been a good thing to find out before this meeting , i mean , let 's let 's assume for right now that we 're just kind of plugging on ahead , because even if they tell us that , uh , the rules are different , uh , we 're still interested in doing what we 're doing . <EOS> uh , one thing that would be no good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , um , run a good vad , and and determine boundaries . <EOS> and then given those boundaries , then have everybody do the recognition . <EOS> uh , i guess they argued about that yesterday i do n't do n't know the answer but we should find out . <EOS>
i mean , let 's let 's assume for right now that we 're just kind of plugging on ahead , because even if they tell us that , uh , the rules are different , uh , we 're still interested in doing what we 're doing . <EOS>
you you had a discussion with sunil about this though ? <EOS> no . <EOS> yeah , you should talk with him . <EOS> no , i mean , because the the the the whole problem that happened before was coordination , so so you need to discuss with him what we 're doing , uh , cuz they could be doing the same thing and or something . <EOS> we just we just have to be in contact more . <EOS> i think that the the fact that we we did that with had that thing with the latencies was indicative of the fact that there was n't enough communication . <EOS>
do you have news from the conference talk ? <EOS> yesterday morning on video conference . <EOS> no , nobody 's told me anything . <EOS> no , that would have been a good thing to find out before this meeting , i mean , let 's let 's assume for right now that we 're just kind of plugging on ahead , because even if they tell us that , uh , the rules are different , uh , we 're still interested in doing what we 're doing . <EOS>
so it would be around two hundred and forty just just barely in there . <EOS> what 's the allowable ? <EOS> two - fifty , unless they changed the rules . <EOS> which there is there 's some discussion of . <EOS> what were they thinking of changing it to ? <EOS> uh , well the people who had very low latency want it to be low uh , very <laugh> very very narrow , uh , latency bound . <EOS> and the people who have longer latency do n't . <EOS> unfortunately we 're the main ones with long latency , and basically the best proposal had something like thirty or forty milliseconds of latency . <EOS>
also we were thinking to to , uh , apply the eh , spectral subtraction from ericsson and to to change the contextual klt for lda . <EOS> well , there 's a lot of different ways of computing the noise spectrum . <EOS> it seems like this kind of thing could add to the latency . <EOS> i mean , depending on where the window was that you used to calculate the signal - to - noise ratio . <EOS> not necessarily . <EOS> cuz if you do n't look into the future , actually , it 's a mmm if - if you want to have a good estimation on non - stationary noise you have to look in the in the future . <EOS> but what does what what what does alcatel do ? <EOS> and and france telecom . <EOS> they just look in the past . <EOS> i guess it works because the noise are , uh pret uh , almost stationary yeah , y i mean , you 're talking about non - stationary noise but i think that spectral subtraction is rarely is is not gon na work really well for for non - stationary noise , but it 's hard to that 's hard to do . <EOS>
let 's let 's , i mean , i think that as as we said before that one of the things that we 're imagining is that uh there there will be <inbreath> uh in the system we end up with there 'll be something to explicitly uh uh do something about noise so um i suggest actually now we we we sorta move on and and hear what 's what 's what 's happening in in other areas um . <EOS> <inbreath> and uh i do n't know if we 've talked lately about the the plans you 're developing that we talked about this morning uh what 's next ? <EOS>
so y you guys had a a meeting with uh with hynek which i unfortunately had to miss . <EOS> so everybody knows what happened except me . <EOS> well . <EOS> uh first we discussed about some of the points that i was addressing in the mail i sent last week . <EOS> about the um , well the downsampling problem . <EOS> uh and about the f the length of the filters so basically that was that 's <inbreath> all we discussed about . <EOS>
let 's let 's , i mean , i think that as as we said before that one of the things that we 're imagining is that uh there there will be <inbreath> uh in the system we end up with there 'll be something to explicitly uh uh do something about noise so um i suggest actually now we we we sorta move on and and hear what 's what 's what 's happening in in other areas like <inbreath> what 's what 's happening with your <inbreath> investigations <inbreath> about echos and so on . <EOS> well um i have n't started writing the test yet , i 'm meeting with adam today um and he 's going t show me the scripts he has for um <mouth> <inbreath> running recognition on mee meeting recorder digits . <EOS> i have n't asked hynek for for the for his code yet . <EOS> cuz i looked at uh avendano 's thesis and <mouth> i do n't really understand what he 's doing yet um . <EOS> <inbreath> and uh i do n't know if we 've talked lately about the the plans you 're developing that we talked about this morning um . <EOS> so continuing to um extend so i mean , there 's these issues of what are the what are the variables that you use and do you combine them using the soft `` and - or `` or you do something , you know , more complicated uh what 's next ? <EOS> i could say a little bit about w stuff i 've been playing with . <EOS>
i mean i g i guess the key thing for me is is figuring out how to better coordinate between the two sides uh i was talking with hynek about it later and the the sort of had the sense sort of that that neither group of people wanted to to bother the other group too much . <EOS> but i think that <inbreath> you were sort of waiting for them to <inbreath> tell you that they had something for you and <inbreath> they were sort of waiting for you and and and uh we ended up with this thing where they they were filling up all of the possible latency themselves , but there was also problem perhaps a problem of communication . <EOS> now we will try to just talk more . <EOS>
i mean i g i guess the key thing for me is is figuring out how to better coordinate between the two sides uh i was talking with hynek about it later and the the sort of had the sense sort of that that neither group of people wanted to to bother the other group too much . <EOS> but i think that <inbreath> you were sort of waiting for them to <inbreath> tell you that they had something for you and <inbreath> they were sort of waiting for you and and and uh we ended up with this thing where they they were filling up all of the possible latency themselves , but there was also problem perhaps a problem of communication . <EOS> now we will try to just talk more . <EOS>
i mean i g i guess the key thing for me is is figuring out how to better coordinate between the two sides uh i was talking with hynek about it later but i think that <inbreath> you were sort of waiting for them to <inbreath> tell you that they had something for you and the the sort of had the sense sort of that that neither group of people wanted to to bother the other group too much . <EOS> and <inbreath> they were sort of waiting for you and and and uh we ended up with this thing where they they were filling up all of the possible latency themselves , but there was also problem perhaps a problem of communication . <EOS> now we will try to just talk more . <EOS>
so the only the only uh hesitation i had about it since , i mean i have n't see the data is it sounds like it 's it 's <inbreath> continuous variables and a bunch of them . <EOS> i do n't know how complicated it is to go from there what you really want are these binary labels , and just a few of them . <EOS> and maybe there 's a trivial mapping if you wan na do it and it 's e but it i i i worry a little bit that this is a research project in itself , whereas um <inbreath> if you did something instead that like um having some manual annotation by <inbreath> uh you know , linguistics students , this would there 'd be a limited s set of things that you could do a as per our discussions with with john before course then , that 's the other question is do you want binary variables . <EOS>
so the only the only uh hesitation i had about it since , i mean i have n't see the data is it sounds like it 's it 's <inbreath> continuous variables and a bunch of them . <EOS> i do n't know how complicated it is to go from there what you really want are these binary labels , and just a few of them . <EOS> and maybe there 's a trivial mapping if you wan na do it and it 's e but it i i i worry a little bit that this is a research project in itself , whereas um <inbreath> if you did something instead that like um having some manual annotation by <inbreath> uh you know , linguistics students , this would there 'd be a limited s set of things that you could do a as per our discussions with with john before course then , that 's the other question is do you want binary variables . <EOS>
ok , so uh <breath> had some interesting mail from uh dan ellis . <EOS> uh . <EOS> since the last meeting we 've we 've tried to put together um <mouth> the clean low - pass um downsampling , upsampling , i mean , <EOS>
uh . <EOS> since the last meeting we 've we 've tried to put together um <mouth> the clean low - pass um downsampling , upsampling , i mean , uh the new filter that 's replacing the lda filters , <EOS>
ok , so uh <breath> had some interesting mail from uh dan ellis . <EOS> where this came up was that uh i was showing off these wave forms that we have on the web and and uh <breath> i just sort of had n't noticed this , but that the major , major component in the wave in the second wave form in that pair of wave forms is actually the air conditioner . <EOS> can i ask a , i mean a sort of top - level question , which is <breath> um `` if if most of what the ogi folk are working with is trying to <breath> integrate this other other uh spectral subtraction , <breath> why are we worrying about it ? `` <EOS>
um the other thing that i do n't know the answer to , but when people are using feacalc here , uh whether they 're using it with the high - pass filter option or not . <EOS> so when we 're doing all these things using our software there is and <breath> it 's it 's pretty it 's not a very severe filter . <EOS> does n't affect speech frequencies , so . <EOS> we we we want to go and check that in i for anything that we 're going to use the p d but if we do make use of the cheap mikes , <breath> uh we want to be sure to do that that filtering before we <breath> process it . <EOS>
is there any further discussion about this this idea of of having some sort of source code control ? <EOS> well . <EOS> for the moment they 're there is this eurospeech deadline , sounds like a great idea but but i think that that um <breath> he 's saying people are sort of scrambling for a eurospeech deadline . <EOS> but that 'll be uh , uh done in a week . <EOS> so , maybe after <breath> this next one . <EOS> yeah . <EOS> so , i mean , i i think that you could certainly start looking at at the issue uh but but uh <breath> i think it 's probably , on s from what stephane is saying , it 's it 's unlikely to get sort of active participation from the two sides until after they 've but i could try to look into like this uh cvs over the web . <EOS> that seems to be a very popular <breath> way of people distributing changes and over , you know , multiple sites and things so maybe <breath> if i can figure out how do that easily and then pass the information on to everybody so that it 's <breath> you know , as easy to do as possible and and people do n't it wo n't interfere with their regular work , and if you 're interested in using cvs , i 've set it up here , so maybe i can ask you some questions . <EOS> so . <EOS> i 'll be away tomorrow and monday but i 'll be back on tuesday or wednesday . <EOS>
none <EOS>
do you remember when the next meeting is supposed to be ? <EOS> it 's uh in june . <EOS> h hynek will be back in town uh the week after next , back back in the country . <EOS> and start start organizing uh <breath> more visits and connections and so forth , working towards june . <EOS>
um . <EOS> yeah , the other thing is that you saw that that mail about uh the vad v a ds performing quite differently ? <EOS> this there was this experiment of uh `` what if we just take the baseline ? `` <EOS> and you inc incorporate the different v a and it looks like the the french vad is actually uh better significantly better . <EOS> yeah but i do n't know which vad they use . <EOS> it 's pratibha that that did this experiment . <EOS> um . <EOS> we should ask which vad she used . <EOS> he actually , i think that he say with the good vad of from ogi i mean it was enough better that that it would <mouth> uh account for a fair amount of the difference between our performance , actually . <EOS> so if they have a better one , we should use it . <EOS>
ok , so uh <breath> had some interesting mail from uh dan ellis . <EOS> where this came up was that uh i was showing off these wave forms that we have on the web and and uh <breath> i just sort of had n't noticed this , but that the major , major component in the wave in the second wave form in that pair of wave forms is actually the air conditioner . <EOS> i <laugh> <breath> i have to be more careful about using that as a as a <breath> as a good illustration , uh , in fact it 's not , of uh <breath> of the effects of room reverberation . <EOS> it is is n't a bad illustration of the effects of uh room noise . <EOS> <breath> on on uh some mikes since i was talking about reverberation and showing this thing that was noise , it was n't a good match , so i think we 'll change our our picture on the web , when we 're @ @ . <EOS> dave , the other thing , actually , is is this business about this wave form . <EOS> maybe you and i can talk a little bit at some point about <breath> coming up with a better <breath> uh demonstration of the effects of reverberation for our web page , <EOS>
ok , so uh <breath> had some interesting mail from uh dan ellis . <EOS> where this came up was that uh i was showing off these wave forms that we have on the web and and uh <breath> i just sort of had n't noticed this , but that the major , major component in the wave in the second wave form in that pair of wave forms is actually the air conditioner . <EOS> i <laugh> <breath> i have to be more careful about using that as a as a <breath> as a good illustration , uh , in fact it 's not , of uh <breath> of the effects of room reverberation . <EOS> it is is n't a bad illustration of the effects of uh room noise . <EOS> <breath> on on uh some mikes since i was talking about reverberation and showing this thing that was noise , it was n't a good match , it made a good good audio demonstration because when we could play that clip the the the really <breath> obvious difference is that you can hear two voices and <breath> <laugh> in the second one and only hear but i mean you ca n't when you play it back in a room with a you know a big room , <breath> nobody can hear that difference really . <EOS>
so i think we 'll change our our picture on the web , when we 're @ @ . <EOS> um <breath> another , i was thinking of was um <breath> taking some spectral slices , and look at the spectrum or cepstrum that you get out of there , well , i mean um all the recognizers look at frames . <EOS> at one point in time or uh twenty over twenty milliseconds or something , <breath> you have a spectrum or a cepstrum . <EOS> maybe you and i can talk a little bit at some point about <breath> coming up with a better <breath> uh demonstration of the effects of reverberation for our web page , but i mean you ca n't when you play it back in a room with a you know a big room , <breath> nobody can hear that difference really . <EOS> but for the the visual , just , you know , i 'd like to have uh <breath> uh , you know , the spectrogram again , the other thing that we had in there that i did n't like was that um <breath> the most obvious characteristic of the difference uh when you listen to it is that there 's a second voice , and the the the the the uh <mouth> cuts that we have there actually do n't correspond to the full wave form . <EOS> but <breath> it it 's um <breath> it 's the first six seconds or something <breath> of it and it 's in <breath> the seventh or eighth second or something where @ @ the second voice comes in . <EOS> so we we would like to actually see <breath> the voice coming in , too , i think , <EOS>
it seems better when we look at the mismatched case but <mouth> i think we are like like cheated here by the th this problem that <breath> uh in some cases when you modify slight slightly modify the initial condition you end up <breath> completely somewhere air somewhere else in the in the space , <breath> the parameters . <EOS> i do n't i do n't think it means that the new system is more robust but i mean <breath> from this se seventy - eight um percent recognition rate system , <breath> i could change the transition probabilities for the the first hmm and it will end up to eighty - nine also . <EOS> by using point five instead of point six , point four <breath> as in the the htk script . <EOS> yeah i looked at um <breath> looked at the results when stephane did that and it 's it 's really wo really happens . <EOS> i mean th the only difference is you change the self - loop transition probability by a tenth of a percent and it causes ten percent difference in the word error rate . <EOS> and n not tenth of a percent , one tenth , it 's just very um you know get stuck in some local minimum and this thing throws you out of it i guess . <EOS> well , what 's what are according to the rules what what are we supposed to do about the transition probabilities ? <EOS> i think you 're not allowed to that 's supposed to be point six , for the self - loop . <EOS> but changing it to point five i think is which gives you much better results , but that 's <breath> not allowed . <EOS> yeah , but even if you use point five , i 'm not sure it will always give you the better results right . <EOS> we only tested it on the the medium mismatch , <EOS>
yeah , actually the way the final score is computed is quite funny . <EOS> it 's not a mean of word error rate . <EOS> it 's not a weighted mean of word error rate , it 's a weighted mean of improvements . <EOS> which means that <mouth> actually the weight on the well - matched is well i well what what what happened is that if you have a small improvement or a small if on the well - matched case <breath> it will have uh huge influence on the improvement compared to the reference because the reference system is is is quite good for for the well - ma well - matched case also . <EOS> so they do improvement in terms of uh accuracy ? <EOS> rather than word error rate ? <EOS> no , it 's compared to the word er it 's improvement on the word error rate , so if you have uh ten percent error and you get five percent absolute uh <breath> improvement then that 's fifty percent . <EOS> ok . <EOS> so what you 're saying then is that if it 's something that has a small word error rate , <breath> then uh a even a relatively small improvement on it , in absolute terms , <breath> will show up as quite quite large in this . <EOS> but yeah that 's that 's it 's the notion of relative improvement . <EOS> that 's why i 've been saying we should be looking at word error rate uh and and not not at <breath> at accuracies . <EOS> i mean uh we probably should have standardized on that all the way through . <EOS> but you 're but when you look at the numbers , your sense of the relative size of things is quite different . <EOS>
um . <EOS> what i was asking , though , is uh are what 's what 's the level of communication with uh <breath> the o g i gang now , about this well , we are exchanging mail as soon as we < breath-laugh > we have significant results . <EOS> for the moment , they are working on integrating <breath> the um <mouth> spectral subtraction apparently from ericsson . <EOS> we are working on our side on other things like <breath> uh also trying a sup spectral subtraction but of of our own , i mean , another <breath> spectral substraction . <EOS> can i ask a , i mean a sort of top - level question , which is <breath> um `` if if most of what the ogi folk are working with is trying to <breath> integrate this other other uh spectral subtraction , <breath> why are we worrying about it ? `` <EOS> it 's just uh well it 's another they are trying to u to use the um <mouth> the ericsson and we 're trying to use something something else . <EOS> and what they did at ogi is just <breath> uh they do n't use on - line normalization , for the moment , on spectral subtraction i think as soon as they will try on - line normalization <breath> there will be a problem . <EOS> so yeah , we 're working on the same thing but <breath> i think with different different system intellectually it 's interesting to work on things th uh one way or the other but i 'm i 'm just wondering if um <breath> on the list of things that there are to do , if there are things that we wo n't do because <breath> we 've got two groups doing the same thing . <EOS> just just asking . <EOS> in fact if you get if you go into uh a uh harmonics - related thing <breath> it 's definitely going to be different than what they 're doing should have some interesting properties in noise . <EOS>
um . <EOS> <mouth> <breath> what are we talking about today ? <EOS> uh , well , first there are perhaps these uh meeting recorder digits that we tested . <EOS>
uh , well , first there are perhaps these uh meeting recorder digits that we tested . <EOS> perhaps the point is that we 've been working on <breath> is , yeah , we have put the um the good vad in the system and <breath> it really makes a huge difference . <EOS> yeah , and then we 've started to work with this of um voiced - unvoiced stuff . <EOS> no , i w <breath> i begin to play <laugh> with matlab and to found some parameter robust for voiced - unvoiced decision . <EOS> what 's up with you ? <EOS> so i 've been looking at avendano 's work but it 's it 's an approach to deal with <breath> reverberation or that the aspect of his work that i 'm interested in <EOS>
uh , well , first there are perhaps these uh meeting recorder digits that we tested . <EOS> the both the uh <breath> the sri system and the oth y you do i think you read some of the the zeros as o 's and some as zeros . <EOS> is there a particular way we 're supposed to read them ? <EOS> perhaps in the sheets there should be another sign for the i mean . <EOS> i think people will do what they say . <EOS> i mean in digit recognition we 've done before , you have you have two pronunciations for that value , `` o `` and `` zero `` . <EOS> no , they just write and and people pronounce `` o `` or zero and you just they just want people to read the digits as you ordinarily would <EOS>
uh i 'll try to write up in my next stat status report a nice description of <breath> what he 's doing , <EOS>
but to me it just it just meant a practical <breath> point that um if we want to <breath> publish results on digits that that people pay <breath> attention to we probably should uh cuz we 've had the problem before that you get show some <breath> nice improvement on something that 's that 's uh , uh it seems like too large a number , and uh <breath> uh people do n't necessarily take it so seriously . <EOS>
our back - end is is fairly simple but until now , well , the attempts to improve it or have fail i mean so to <breath> so there 's there 's there 's two things being affected . <EOS> i mean . <EOS> one is that that , you know , there 's something simple that 's wrong with the back - end . <EOS> we 've been playing a number of states uh i i do n't know if he got to the point of playing with the uh number of gaussians yet but , yeah , so far he had n't gotten any big improvement , but that 's all with the same amount of data which is pretty small . <EOS> perhaps it 's not related , the amount of data but the um recording conditions . <EOS> i uh but i 'm i 'm almost certain that it it <breath> i mean , that it has to do with the um amount of training data . <EOS> it it 's it 's orders of magnitude off . <EOS> let 's see , in the in these multi - train things did we include noisy data in the training ? <EOS> i mean , that could be hurting us actually , for the clean case . <EOS> you know , i do n't think there 's anything magical here . <EOS> it 's , you know , we used a simple htk system with a modest amount of data . <EOS> and this is a a , you know , modern <breath> uh system uh has has a lot of nice points to it . <EOS> so . <EOS> i mean , the htk is an older htk , even . <EOS> i mean , there 's <breath> even though it 's close - miked there 's still there really is background noise . <EOS> um . <EOS> and <breath> uh i suspect when the ti - digits were recorded if somebody fumbled or said something wrong or something that they probably made them take it over . <EOS> it was not i mean there was no attempt to have it be realistic in any in any sense at all . <EOS> ti - digit is it 's very , very clean and it 's like studio recording whereas these meeting recorder digits sometimes you have breath noise <EOS>
uh . <EOS> the problem is that it 's very big and <breath> <mouth> we still have to think how to where to put it uh either some delay and we if we put it on the server side , it does n't work , because on the server side features you already have lda applied <breath> from the f from the terminal side and <breath> so you accumulate the delay <EOS>
so wha where did this good vad come from ? <EOS> it 's um from ogi . <EOS> this is the one they had originally ? <EOS> yeah , but they had to get rid of it because of the space , but the abso assumption is that we will be able to make a vad that 's small and that works fine . <EOS>
but the other thing is uh to use a different vad entirely . <EOS> i i do n't know what the thinking was amongst the the the <breath> the etsi folk but um if everybody agreed sure let 's use this vad and take that out of there they just want , apparently they do n't want to fix the vad because they think there is some interaction between feature extraction and and vad or frame dropping but they still <mouth> want to just to give some um <breath> requirement for this vad because it 's it will not be part of they do n't want it to be part of the standard . <EOS> so there just will be some requirements that are still not uh not yet uh ready i think . <EOS> but i do n't think we need to be stuck on using our or ogi 's vad . <EOS> we could use somebody else 's if it 's smaller you know , as long as it did the job . <EOS>
yeah , it 's it 's another problem . <EOS> if you look at this um spectrum , is it <breath> the mel - filters ? <EOS> and what we clearly see is that in some cases , and the the harmonics are resolved by the f well , there are still appear after mel - filtering , and it happens <breath> for high pitched voice because the width of the lower frequency mel - filters <breath> is sometimes even smaller than the pitch . <EOS> so we were thinking to modify the mel - spectrum to have something that that 's smoother on low frequencies . <EOS>
so what happened since , um , <breath> last week is well , from ogi , these experiments on putting vad on the baseline . <EOS>
i have something just fairly brief to report on . <EOS> and but it runs much much faster . <EOS> i i think m it only took something like , uh , three or four hours to do the full training , as opposed to wh what , sixteen hours or something like that ? <EOS> oh , the other thing that i did was , um , <breath> i compiled the htk stuff for the linux boxes . <EOS> there was a conference call this tuesday . <EOS> i do n't know yet the <breath> what happened <breath> tuesday , but <breath> the points that they were supposed to discuss is still , <mouth> uh , things like <breath> the weights , so what happened since , um , <breath> last week is there was a <breath> start of some effort on something related to voicing or something . <EOS> so basically we try to , <breath> <breath> uh , find <breath> good features that could be used for voicing detection , <EOS>
n um , not not not much is new . <EOS> um , anything to add ? <EOS> well , i 've been continuing reading . <EOS> i went off on a little tangent this past week , <EOS>
i 've forgotten now what the name of that machine is but i can i can send email around about it . <EOS> um , you have to make you have to make sure that in your dot cshrc , <breath> um , it detects whether you 're running on the linux or a a sparc and points to the right executables . <EOS> and you may not have had that in your dot cshrc before , if you were always just running the sparc . <EOS> uh , i can i can tell you exactly what you need to do to get all of that to work . <EOS>
when you say `` we have that `` , does sunil have it now , too , no . <EOS> because we 're still testing . <EOS> so we 'll perhaps <breath> <mouth> <breath> try to convince ogi people to use the new <breath> the new filters uh , has has anything happened yet on this business of having some sort of standard , uh , source , not yet but i wi i will <breath> call them now they are i think they have more time well , eurospeech deadline is <breath> over <EOS>
there was a conference call this tuesday . <EOS> i do n't know yet the <breath> what happened <breath> tuesday , but <breath> the points that they were supposed to discuss is still , <mouth> uh , things like <breath> the weights , <EOS>
do you know who was who was since we were n't in on it , uh , do you know who was in from ogi ? <EOS> but <breath> the points that they were supposed to discuss is still , <mouth> uh , things like <breath> the weights , i have no idea . <EOS> so the points were the the weights how to weight the different error rates <breath> that are obtained from different language and and conditions . <EOS> it 's not clear that they will keep the same kind of weighting . <EOS> some people are arguing that it would be better to have weights on well , to to combine error rates before computing improvement . <EOS> and so , perhaps they will change the weights to well , i mean , the fact that it 's inconsistent is an obvious mistake . <EOS> but the question is , do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that ? <EOS> and the thing is it 's not just a pure average because there are these weightings . <EOS> it 's just when you when you get all done , i think that they pro but i think they started off this process with the notion that <breath> you should be significantly better than the previous standard . <EOS> so they said `` how much is significantly better ? <EOS> and and so they said `` well , <breath> you know , you should have half the errors , `` or something , `` that you had before `` . <EOS> but it does seem like i i it does seem like it 's more logical to combine them first but there is this this is this still this problem of weights . <EOS> when when you combine error rate it tends to give more importance to the difficult cases , some people think that <breath> it 's more important to look at <breath> to have ten percent imp relative improvement on well - matched case than to have fifty percent on the m mismatched , and other people think that it 's more important to improve a lot on the mismatch it sounds like they do n't really have a good idea about what the final application is gon na be . <EOS> i mean , they don they they do n't they do n't really know , i think . <EOS> so the argument for that being the the the more important thing , <breath> is that you 're gon na try and do that , <breath> but you wan na see how badly it deviates from that when when when the , uh it 's a little different . <EOS> the opposite argument is you 're never really gon na have a good sample of all these different things . <EOS> i gather that in these meetings it 's it 's really tricky to make anything <breath> ac <breath> make any policy change because <breath> < clears throat > everybody has has , uh , their own opinion but there is probably a a big change that will <breath> be made is that the the baseline th they want to have a new baseline , perhaps , and apparently , <mouth> uh , some people are pushing to still keep this fifty percent number . <EOS> so they want <breath> to have at least fifty percent improvement on the baseline , so whose vad uh , they did n't decide yet . <EOS>
and he 's been doing all the talking this is this by the way a bad thing . <EOS> we 're trying to get , um , m more female voices in this record as well . <EOS> make sur make sure carmen <laugh> talks as well . <EOS> i do n't know . <EOS>
i do n't really have , uh , anything new . <EOS> been working on meeting recorder stuff . <EOS> what 's new with you ? <EOS> so there 's nothing new . <EOS> what 's old with you that has developed over the last week or two ? <EOS> well , so we 've been mainly working on the report any - anything new on the thing that , uh , you were working on with the , uh ? <EOS> i do n't have results yet . <EOS> so , what wha <laugh> wh wha what what 's going on ? <EOS> well , we work in the report , too , how are , uh , uh how are things going with what you 're doing ? <EOS> i took a lot of time just getting my taxes out of the way so , i 'm i 'm starting to write code now for my work but i do n't have any results yet . <EOS> do you wan na say something about your stuff here ? <EOS> i just , um , continuing looking at , uh , ph uh , phonetic events , it 's that 's pretty much it . <EOS>
do you think that would be the case for next week also ? <EOS> what 's your projection on ? <EOS> so the experiment is to , um , run our front - end like normal , with the default , uh , insertion penalties and so forth , and then tweak that a little bit and see how much of a difference it makes that 's something i 'd like to understand before we actually use something from it , i i was thinking getting getting us a set of acoustic events to um , to be able to distinguish between , uh , phones and words and stuff . <EOS> can you give an example of an event ? <EOS> so , he in this paper , um , it 's talking about phoneme recognition using acoustic events . <EOS> so , things like frication or , uh , nasality . <EOS> just to expand a little bit on the idea of acoustic event . <EOS> there 's , um in my mind , anyways , there 's a difference between , um , acoustic features and acoustic events . <EOS> and i think of acoustic features as being , um , things that linguists talk about , stuff that 's not based on data , necessarily . <EOS> that 's not based on , you know , acoustic data . <EOS> so they talk about features for phones , which may or may not be all that easy to measure in the acoustic signal . <EOS> versus an acoustic event , which is just < mike noise > some <spikes> something in the acoustic signal <spike> that is fairly easy to measure . <EOS> it 's kinda like the difference between top - down and bottom - up . <EOS> i think of the acoustic you know , phonetic features as being top - down . <EOS> you know , you look at the phone and you say this phone is supposed to be you know , have this feature , this feature , and this feature . <EOS> whether tha those features show up in the acoustic signal is sort of irrelevant . <EOS> whereas , an acoustic event goes the other way . <EOS> here 's the signal . <EOS> here 's some event . <EOS> and then that you know , that may map to this phone sometimes , and so it 's sort of a different way of looking . <EOS>
so , my suggestion , though , is that you you not necessarily finish that . <EOS> but that you put it all together so that it 's you 've got you 've got a clearer structure to it . <EOS> so that , you know so that such a thing can be written . <EOS> and right now it 's kind of important that we actually go forward with experiments . <EOS>
i was saying hynek 'll be here next week , uh , wednesday through friday uh , through saturday , i wo n't be here thursday and friday . <EOS> but my suggestion is that , uh , at least for this meeting , people should go ahead , uh , cuz hynek will be here , so maybe i can have that for next week when hynek 's here . <EOS> maybe that 's maybe a topic especially if you talk with him when i 'm not here , that 's a topic you should discuss with hynek to , you know , double check it 's ok . <EOS> well , this 'll be , i think , something for discussion with hynek next week . <EOS>
so the experiment is to , um , run our front - end like normal , with the default , uh , insertion penalties and so forth , and then tweak that a little bit and see how much of a difference it makes so by `` our front - end `` i mean take , you know , the aurora - two s take some version that stephane has that is , you know , our current best version of something . <EOS> how how much , uh , does it improve if you actually adjust that ? <EOS> when you adjusted those numbers for mel cepstrum , did it ? <EOS> uh , i i do n't remember off the top of my head . <EOS> yeah . <EOS> i did n't even write them down . <EOS> looking at the i wrote down what the deletions , substitutions , and insertions were , for different numbers of states per phone . <EOS> um , but , uh , that that 's all i wrote down . <EOS> i would need to do that . <EOS> i can do that for next week . <EOS> but i think it would be it 'd be good to know that . <EOS> so maybe i can have that for next week when hynek 's here . <EOS>
y yeah . <EOS> basically we we 've stopped , uh , experimenting , so , my suggestion , though , is that you you not necessarily finish that . <EOS> but that you put it all together so that it 's you 've got you 've got a clearer structure to it . <EOS> you know what things are , you have things documented , you 've looked things up that you needed to look up . <EOS> so that , you know so that such a thing can be written . <EOS> when when when do you leave again ? <EOS> first of july . <EOS> and that you figure on actually finishing it in in june . <EOS> because , you know , you 're gon na have another bunch of results to fit in there anyway . <EOS> and right now it 's kind of important that we actually go forward with experiments . <EOS> so so , i i think it 's good to pause , and to gather everything together and make sure it 's in good shape , so that other people can get access to it and so that it can go into a report in june . <EOS> but i think <inbreath> to to really work on on fine - tuning the report n at this point is is probably bad timing , i i think . <EOS>
maybe that 's maybe a topic especially if you talk with him when i 'm not here , that 's a topic you should discuss with hynek to , you know , double check it 's ok . <EOS> there is also the spectral subtraction , i think maybe we should , uh , try to integrate it in in our system . <EOS> i think that would involve to <breath> to mmm <mouth> use a big a al already a big bunch of the system of ericsson . <EOS> because he has spectral subtraction , then it 's followed by , <mouth> um , other kind of processing that 's are dependent on the uh , if it 's speech or noi or silence . <EOS> and s i i think it 's important , um , <mouth> to reduce this musical noise and this this increase of variance during silence portions . <EOS> this was in this would involve to take almost everything from from the this proposal and then just add some kind of on - line normalization in in the neural network . <EOS> well , this 'll be , i think , something for discussion with hynek next week . <EOS>
uh , so the next question to ask , which is i think the one that that that andreas was dre addressing himself to in the lunch meeting , is , um , we 're not supposed to adjust the back - end , but anybody using the system would . <EOS> if you were just adjusting the back - end , how much better would you do , uh , in noise ? <EOS> uh , because the language scaling and insertion penalties and so forth are probably set to be about right for mel cepstrum . <EOS> but , um , they 're probably not at all set right for these things , as for these other things , it may turn out that , uh , <inbreath> it 's kind of reasonable . <EOS> i mean , andreas gave a very reasonable response , and he 's probably not gon na be the only one who 's gon na say this in the future people people within this tight - knit community who are doing this evaluation <inbreath> are accepting , uh , more or less , that these are the rules . <EOS> but , people outside of it who look in at the broader picture are certainly gon na say `` well , wait a minute . <EOS> you 're doing all this standing on your head , uh , on the front - end , when all you could do is just adjust this in the back - end with one s one knob . `` <EOS> so we have to at least , i think , determine that that 's not true , and as you say as you point out finding ways to then compensate for that in the front - end < clears throat > also then becomes a priority for this particular test , that 's permitted ? <EOS> well , ogi does did that . <EOS> at some point they did that for for the voice activity detector . <EOS> the rules as i understand it , is that in principle the italian and the spanish and the english italian and the finnish and the english ? <EOS> were development data and spanish , yeah . <EOS> on which you could adjust things . <EOS> and the and the german and danish were the evaluation data . <EOS> and then when they finally actually evaluated things they used everything . <EOS> it it does n't appear that there 's strong evidence that even though things were somewhat tuned on those three or four languages , that that going to a different language really hurt you . <EOS> and the noises were not exactly the same . <EOS> i mean they were different drives . <EOS> different cars . <EOS> it 's tuned more than , you know , a a a a you 'd really like to have something that needed no particular noise at all , but that 's not really what this contest is . <EOS> that 's something i 'd like to understand before we actually use something from it , it 's probably something that , mmm , the you know , the , uh , experiment designers did n't really think about , because i think most people are n't doing trained systems , or , you know , uh , systems that are like ours , where you actually use the data to build models . <EOS> i mean , they just doing signal - processing . <EOS> except that , uh , that 's what we used in aurora one , and then they designed the things for aurora - two knowing that we were doing that . <EOS> and they did n't forbid us to build models on the data ? <EOS> no . <EOS> but , i think i think that it it it probably would be the case that if , say , we trained on italian , uh , data and then , uh , we tested on danish data and it did terribly , uh , that that it would look bad . <EOS> and i think someone would notice maybe that 's maybe a topic especially if you talk with him when i 'm not here , that 's a topic you should discuss with hynek to , you know , double check it 's ok . <EOS>
as for these other things , it may turn out that , uh , <inbreath> it 's kind of reasonable . <EOS> i mean , andreas gave a very reasonable response , and he 's probably not gon na be the only one who 's gon na say this in the future people people within this tight - knit community who are doing this evaluation <inbreath> are accepting , uh , more or less , that these are the rules . <EOS> but , people outside of it who look in at the broader picture are certainly gon na say `` well , wait a minute . <EOS> you 're doing all this standing on your head , uh , on the front - end , when all you could do is just adjust this in the back - end with one s one knob . `` <EOS> so we have to at least , i think , determine that that 's not true , and as you say as you point out finding ways to then compensate for that in the front - end < clears throat > also then becomes a priority for this particular test , <EOS>
are you discovering anything , uh , that makes you scratch your head as you write this report , like why did we do that , or why did n't we do this , uh . <EOS> yeah . <EOS> actually , there were some tables that were also with partial results . <EOS> we just noticed that , wh while gathering the result that for some conditions we did n't have everything . <EOS>
none <EOS>
and and uh so , i i think that carmen and stephane reported on uh amsterdam meeting , which was kind of interesting because it was for the first time we realized we are not friends really , but we are competitors . <EOS>
because it was for the first time we realized we are not friends really , but we are competitors . <EOS> it seemed like there were still some issues , that they were trying to decide ? <EOS> there is a plenty of there 're plenty of issues . <EOS>
so what we are doing at ogi now is uh uh uh working basically on our parts which we i think a little bit neglected , and then most of the effort is uh now also aimed at this e e trap recognition . <EOS>
how 's your documentation or whatever so have you been running some new experiments ? <EOS>
so we were just discussing , since you mentioned that , in it w driving in the car with morgan this morning , we were discussing a good experiment for b for beginning graduate student who wants to run a lot of who wants to get a lot of numbers on something which is , like , `` imagine that you will you will start putting every co any coefficient , which you are using in your vector , in some general power . <EOS> like sort of you take a s power of two , or take a square root , or something . <EOS> because uh your uh gaussian mixture model , so you 're compressing the range of this coefficient , so it 's becoming less efficient . <EOS> morgan was @ @ and he was he was saying well this might be the alternative way how to play with a with a fudge factor , you know , and i said `` well in that case why do n't we just start compressing individual elements , like when when because we observed that uh higher parameters were more important than lower for recognition . <EOS> and basically the the c - ze c - one contributes mainly slope , <EOS>
none <EOS>
when we talked about aurora still i wanted to m make a plea uh encourage for uh more communication between between uh uh different uh parts of the distributed uh uh center . <EOS> uh even when there is absolutely nothing to to s to say but the weather is good in ore - in in berkeley . <EOS> i 'm sure that it 's being appreciated in oregon and maybe it will generate similar responses down here , <EOS>
if we mail to `` aurora - inhouse `` , does that go up to you guys also ? <EOS> no . <EOS> do we have a mailing list that includes uh the ogi people ? <EOS> uh no . <EOS> we do n't have . <EOS> maybe we should set that up . <EOS> that would make it much easier . <EOS> and then we also can send the the dis to the same address and it goes to everybody <EOS>
there is a plenty of there 're plenty of issues . <EOS> like the voice activity detector , well and what happened was that they realized that if two leading proposals , which was french telecom alcatel , and us both had uh voice activity detector . <EOS> and i said `` well big surprise , i mean we could have told you that n n n four months ago , except we did n't because nobody else was bringing it up `` . <EOS> obviously french telecom did n't volunteer this information either , cuz we were working on mainly on voice activity detector for past uh several months and everybody said `` well but this is not fair . <EOS> we did n't know that . `` <EOS> and of course uh the it 's not working on features really . <EOS> and so then ev ev everybody else says `` well we should we need to do a new eval evaluation without voice activity detector , or we have to do something about it `` . <EOS> and in principle i uh i we agreed . <EOS> but in that case , uh we would like to change the uh the algorithm because uh if we are working on different data , we probably will use a different set of tricks . <EOS> but unfortunately nobody ever officially can somehow acknowledge that this can be done , because french telecom was saying `` no , no , no , now everybody has access to our code , so everybody is going to copy what we did . `` <EOS> well our argument was everybody ha has access to our code , and everybody always had access to our code . <EOS> we never uh uh denied that . <EOS> we thought that people are honest , that if you copy something and if it is protected protected by patent then you negotiate , or something , but and french telecom was saying `` no , no , no , there is a lot of little tricks which uh sort of uh can not be protected and you guys will take them , `` which probably is also true . <EOS> and i think they have to be honest in the long run , because winning proposal again uh what will be available th is will be a code . <EOS> so the uh the people can go to code and say `` well listen this is what you stole from me `` the biggest problem of course is that f that alcatel french telecom cl claims `` well we fulfilled the conditions . <EOS> we are the best . <EOS> and e and other people do n't feel that , because they so they now decided that that is the whole thing will be done on well - endpointed data , still not clear if we are going to run the if we are allowed to run uh uh new algorithms , because uh we would fight for that , really . <EOS> at least our experience is that only endpointing a a mel cepstrum gets uh gets you twenty - one percent improvement overall and twenty - seven improvement on speechdat - car <EOS>
at least our experience is that only endpointing a a mel cepstrum gets uh gets you twenty - one percent improvement overall and twenty - seven improvement on speechdat - car then obvious the database uh i mean the the the uh the baseline will go up . <EOS> and nobody can then achieve fifty percent improvement . <EOS> so they agreed that uh there will be a twenty - five percent improvement required on on uh h u m bad mis badly mismatched and so , so now they want to say `` we we will require fifty percent improvement only for well matched condition , and only twenty - five percent for the serial cases . `` <EOS> and uh and they almost agreed on that except that it was n't a hundred percent agreed . <EOS> and so last time uh during the meeting , i just uh brought up the issue , for two years we are fighting for fifty percent improvement and suddenly you are saying `` oh no we we will do something less `` , and everybody said `` oh we discussed that and you were not a mee there `` and i said `` well a lot of other people were not there because not everybody participates at these teleconferencing c things . `` <EOS> then they said `` oh no no no because uh everybody is invited . `` <EOS> however , there is only ten or fifteen lines , so people ca n't even con you know participate . <EOS> so eh they agreed , and so they said `` ok , we will discuss that . `` <EOS> so now officially , nokia is uh uh complaining and said they they are looking for support , uh i think qualcomm is uh saying , too `` we should n't abandon the fifty percent yet . <EOS> we should at least try once again , one more round . `` <EOS>
none <EOS>
well i <inbreath> tried this mean subtraction method . <EOS> due to avendano , <inbreath> i 'm taking s um <inbreath> six seconds of speech , <EOS>
so uh , he 's not here , ok , and wh when did stephane take off ? <EOS> i think that stephane will arrive today or tomorrow . <EOS> so he 's he 's going to icassp which is good . <EOS> and also mmm i h hynek last week say that if i have time i can to begin to to study well seriously the france telecom proposal to look at the code i begin to to work also in that . <EOS> but the first thing that i do n't understand is that they are using r - the uh log energy that this quite i do n't know why they have some constant in the expression of the lower energy . <EOS>
and also mmm i h hynek last week say that if i have time i can to begin to to study well seriously the france telecom proposal to look at the code i begin to to work also in that . <EOS> but the first thing that i do n't understand is that they are using r - the uh log energy that this quite i do n't know why they have some constant in the expression of the lower energy . <EOS>
sunil 's here for the summer , sunil since you 're <inbreath> have n't have n't been at one of these yet , why do n't yo you tell us what 's what 's up with you ? <EOS>
sunil since you 're <inbreath> have n't have n't been at one of these yet , why do n't yo you tell us what 's what 's up with you ? <EOS> uh , the other other thing what i tried was , i just um , uh , took the baseline and then ran it with the endpoint inf uh th information , just the aurora baseline , to see that how much the baseline itself improves by just supplying the information of the i mean the w speech and nonspeech . <EOS> i found that the baseline itself improves by twenty - two percent by just giving the wuh . <EOS> because the the second the new phase is going to be with the endpointed speech . <EOS> and just to get a feel of how much the baseline itself is going to change by adding this endpoint information , i just , uh , use so people wo n't even have to worry about , uh , doing speech - nonspeech then . <EOS> yeah <EOS>
and then just uh , i guess uh , progress reports individually , and then uh , plans for where we go between now and then , pretty much . <EOS> so maybe uh , just briefly , you could remind us about the related experiments . <EOS> cuz you did some stuff that you talked about last week , the main thing that we did is just to take the spectral subtraction from the france telecom , we are playing we are also playing , trying to put other spectral subtraction mmm , in the code . <EOS> it would be a very simple spectral subtraction , on the um , mel energies anything else going on ? <EOS> i do n't have good result , with the inc including the new parameters , <EOS>
with what what other new p new parameter ? <EOS> so maybe you probably need to back up a bit i tried to include another new parameter to the traditional parameter , that , like , the auto - correlation , the r - zero and r - one over r - zero and another estimation of the var the variance of the difference for of the spec si uh , spectrum of the signal and and the spectrum of time after filt mel filter bank . <EOS> the idea is to found another feature for discriminate between voice sound and unvoice sound . <EOS> and we try to use this new feature feature . <EOS> anything on your end you want to talk about ? <EOS> sunil has n't has n't heard about uh , what i 've been doing . <EOS> so basically that 's just , um , trying to propose um , uh , your next your your following years of of your phd work , trying trying to find a project to to define and and to work on . <EOS> so , i 've been , uh , looking into , um , doing something about r uh , speech recognition using acoustic events . <EOS> um , building robust um , primary detectors for these acoustic events , and using the outputs of these robust detectors to do speech recognition . <EOS>
i could say a few words about um , some of the uh , compute stuff that 's happening around here , so that people in the group know . <EOS> so we just put in an order for about twelve new machines , uh , to use as sort of a compute farm . <EOS> and andreas has sort of gotten that all uh , fixed up and up to speed . <EOS> and he 's got a number of little utilities that make it very easy to um , <mouth> run things using p - make and customs . <EOS> and i can send an email around or , maybe i should do an faq on the web site about it or something . <EOS> how about an email that points to the faq , and , if you say that and then some job that you want to execute , uh , it will find the fastest currently available machine , and export your job to that machine , and , so , soon , when we get all the new machines up , <mouth> um , e then we 'll have lots more compute to use . <EOS> there 's a lot of nice features to it and it kinda helps to balance the load of the machines <EOS>
and i can send an email around or , maybe i should do an faq on the web site about it or something . <EOS> how about an email that points to the faq , <EOS>
i mean we 've had these discussions before , and and one of the things that struck me was that uh , about this line of thought that was particularly interesting to me was that we um whenever you condense things , uh , in an irreversible way , um , you throw away some information . <EOS> and so the question is , uh , can we figure out if there 's something we 've thrown away that we should n't have . <EOS> when they were looking at the difference between the filter bank and the fft that was going into the filter bank , i was thinking `` oh , ok , so they 're picking on something they 're looking on it to figure out noise , or voice voiced property whatever . `` <EOS> but for me sort of the interesting thing was , `` well , but is there just something in that difference which is useful ? `` <EOS> so another way of doing it , maybe , would be just to take the fft uh , power spectrum , and feed it into a neural network , and , you know , maybe if it 's used in combination , it will get at something that we 're missing . <EOS> it 's just a thought . <EOS> yeah , i can i will try to do that . <EOS>
let 's see , maybe we should just get a list of items i guess there 's the usual updates , everybody going around and saying , uh , you know , what they 're working on , the things that happened the last week . <EOS>
well , i 've been working on on t mainly on on - line normalization this week . <EOS> uh , i 've been trying different slightly slightly different approaches . <EOS> yeah . <EOS> i 've been playing a little bit with some kind of thresholding , how about you , sunil ? <EOS> so , um , i 've been , uh , implementing this , uh , wiener filtering for this aurora task . <EOS> oh . <EOS> how about you , carmen ? <EOS> mmm . <EOS> i 'm working with vts . <EOS>
how about you , barry ? <EOS> um , <mouth> still working on my my quals preparation stuff . <EOS> so , um , <mouth> i guess i 'll just pass it on to dave . <EOS> well , in my lunch talk last week i i said i 'd tried phase normalization and gotten garbage results using that l um , long - term mean subtraction approach . <EOS>
none <EOS>
so do you maybe make errors in different places ? <EOS> different kinds of errors ? <EOS> i did n't look , uh , more closely . <EOS> i i really would like to suggest looking , um , a little bit at the kinds of errors . <EOS> i know you can get lost in that and go forever and not see too much , but <breath> sometimes , just seeing that each of these things did n't make things better may not be enough . <EOS> it may be that they 're making them better in some ways and worse in others , or increasing insertions and decreasing deletions , you know , helping with noisy case but hurting in quiet case . <EOS> and if you saw that then maybe you it would <mouth> something would occur to you of how to deal with that . <EOS>
a third thing is that , um , <outbreath> i play a little bit with the , um <outbreath> finding what was different between , um , he had the france telecom blind equalization in the system . <EOS> the number o of mfcc that was were used was different . <EOS> you used thirteen and we used fifteen . <EOS> uh , so the the right now , the the system that is there in the what we have in the repositories , with uses fifteen . <EOS> so , we have n't w we have been always using , uh , fifteen coefficients , not thirteen ? <EOS> um , i 'll t s run some experiments to see whether once i have this < 3 tongue taps > noise compensation to see whether thirteen and fifteen really matters or not . <EOS> never tested it with the compensation , but without , <breath> uh , compensation it was like fifteen was s slightly better than thirteen , <EOS>
well , in my lunch talk last week i i said i 'd tried phase normalization and gotten garbage results using that l um , long - term mean subtraction approach . <EOS> it turned out there was a bug in my matlab code . <EOS> so , um , i 've been , uh , implementing this , uh , wiener filtering for this aurora task . <EOS> i i actually thought it was it was doing fine when i tested it once . <EOS> i it 's , like , using a small section of the code . <EOS> and i got , <breath> like , worse results than not using it . <EOS> so , i 've been trying to find where the problem came from . <EOS> and then it looks like i have some problem in the way there is some some very silly bug somewhere . <EOS> i i mean , i uh , it actually i it actually made the whole thing worse . <EOS> and it 's , like w it 's it 's very horrible . <EOS> i was like i 'm trying to find where the m m problem came , <EOS>
so , should we just do the same kind of deal where we go around and do , uh , status report kind of things ? <EOS>
so , should we just do the same kind of deal where we go around and do , uh , status report kind of things ? <EOS> why do n't you go ahead , barry ? <EOS> well , this past week i 've just been , uh , getting down and dirty into writing my my proposal . <EOS> so , uh , you want to go next , dave ? <EOS> last week i finally got results from the sri system about this mean subtraction approach . <EOS> and i also , um , did some experiments about normalizing the phase . <EOS> do you want to go , stephane ? <EOS> i 'm more interested in trying to figure out what 's still the difference between the sri system and the aurora system . <EOS> th i 've been playing with this wiener filter , like . <EOS> how about you , carmen ? <EOS> well , i am still working with , eh , vts . <EOS>
wh why would that be , um , <breath> considering that we actually got an improvement in near - mike performance using htk ? <EOS> uh , with some input from , uh , andreas , i have a theory in two parts . <EOS> because this is also one big difference between the two systems . <EOS> the other differences were the fact that maybe the acoustic models of the sri are more sri system are more complex . <EOS> you know , they have channel adaptation . <EOS> well , there 's also the normalization . <EOS>
i 'm more interested in trying to figure out what 's still the difference between the sri system and the aurora system . <EOS> uh , the next thing is this this vad problem that , uh , the second thing is the this spectral subtraction . <EOS> which i 've just started yesterday to launch a bunch of , uh , <noise> twenty - five experiments , uh , with different , uh , values for the parameters that are used . <EOS>
none <EOS>
o o one thing , um , i noticed is that , um , the mean subtraction seems to make the pzm signals louder after they 've been re - synthesized . <EOS> so i was wondering , is it possible that one reason it helped with the aurora baseline system is just as a kind of gain control ? <EOS> cuz some of the pzm signals sound pretty quiet if you do n't amplify them . <EOS> i do n't see why why your signal is louder after processing , i do n't think just multiplying the signal by two would have any effect . <EOS> i mean , i think if you really have louder signals , what you mean is that you have better signal - to - noise ratio . <EOS> i think , maybe i did n't look , but one thing that makes a difference is this dc offset compensation . <EOS> uh , eh do y did you have a look at at the meet uh , meeting digits , if they have a dc component , no . <EOS> the dc component could be negligible . <EOS> i mean , any all of the mikes have the dc removal some capacitor sitting right in that bias it . <EOS> yeah . <EOS> the microphone is n't gon na pass any dc . <EOS> actually , there are instrumentation mikes that that do pass go down to dc . <EOS> no , it 's the electronics . <EOS> then there 's amplification afterwards . <EOS> you can have dc offset in the data . <EOS>
well , we s decide to m to to obtain the new expression if we work in the cepstral domain . <EOS> but <laugh> i 'm not sure if that will be usefu useful . <EOS> it 's quite a lot it 's a lot of work . <EOS> and i want to know if if we have some feeling that the result i do n't have any feeling if this will work better than apply vts aft in cepstral domain will work better than apply in m mel in filter bank domain . <EOS> i do n't i do n't know absolutely nothing . <EOS> yeah . <EOS> well , you 're i think you 're the first one here to work with vts , uh , maybe we could call someone else up who has , i do n't i do n't have a good feeling for it . <EOS>
uh , the next thing is this this vad problem that , so , i 'm just talking about the the curves that i i sent <breath> i sent you so , whi that shows that <mouth> when the snr decrease , < clears throat > uh , the current vad approach does n't drop much frames for some particular noises , uh , which might be then noises that are closer to speech , uh , acoustically . <EOS> i i just to clarify something for me . <EOS> they were supp supposedly , in the next evaluation , they 're going to be supplying us with boundaries . <EOS> so does any of this matter ? <EOS> first of all , the boundaries might be , uh like we would have t two hundred milliseconds or before and after speech . <EOS> so removing more than that might still make a difference in the results . <EOS> do we ? <EOS> i mean , is there some reason that we think that 's the case ? <EOS> no . <EOS> but maybe we 'll get some insight on that when when , uh , the gang gets back from crete . <EOS>
so , um , <mouth> i guess we got lots to catch up on . <EOS> and we have n't met for a couple of weeks . <EOS>
so , um , since we 're looking at putting this , um mean log m magnitude spectral subtraction , um , into the smartkom system , i i did a test seeing if , um , it would work using past only and plus the present to calculate the mean . <EOS> so i 've been working on that wiener filtering . <EOS> so , < clears throat > i 've been , uh , working still on the spectral subtraction . <EOS>
i do n't know much about as much as i should about the rest of the system but if you did first pass with , um , the with either without the mean sub subtraction or with a a very short time one , and then , um , once you , uh , actually had the whole utterance in , if you did , um , the , uh , uh , longer time version then , based on everything that you had , um , and then at that point only used it to distinguish between , you know , top n , um , possible utterances or something , you you might it might not take very much time . <EOS> i mean , i know in the large vocabulary stu uh , uh , systems , people were evaluating on in the past , some people really pushed everything in to make it in one pass but other people did n't and had multiple passes . <EOS> the argument , um , against multiple passes was u u has often been `` but we want to this to be r you know have a nice interactive response `` . <EOS> and the counterargument to that which , say , uh , bbn i think had , was `` yeah , but our second responses are second , uh , passes and third passes are really , really fast `` . <EOS> do we know yet ? <EOS> about as far as what they 're what the rules are going to be and what we can use ? <EOS> so actually i received a a new document , describing this . <EOS> and what they did finally is to , mmm , uh , not to align the utterances but to perform recognition , um , only on the close - talking microphone , and to take the result of the recognition to get the boundaries uh , of speech . <EOS> oh , so they will send files so everybody will have the same boundaries to work with ? <EOS> yeah . <EOS> all of that sort of stuff is things that they 're debating in their standards committee . <EOS> and and that 's sort of one of the because if we completely ignore latency , and then we discover that we really have to do something about it , we 're going to be find ourselves in a bind . <EOS>
can i ask just a a high level question ? <EOS> can you just say like one or two sentences about wiener filtering and why why are people doing that ? <EOS> i mean , so the basic principle of wiener filter is like you try to minimize the , uh , d uh , difference between the noisy signal and the clean signal and for this i u simply used some code that , uh , < breath-laugh > i had from from belgium , which is technique that , um , takes a bunch of frame , and for each frequency bands of this frame , takes a look at the minima of the energy . <EOS> and then average these minima and take this as an an energy estimate of the noise for this particular frequency band . <EOS>
just for a visit ? <EOS> uh , we 'll see . <EOS> we might might end up with some longer collaboration or something . <EOS> so he 's gon na look in on everything we 're doing and give us his his thoughts . <EOS> and uh hans - uh , hans - guenter will be here , um , i think by next next tuesday or so . <EOS> so he 's he 's going to be here for about three weeks , <EOS>
th - that 's his spectral subtraction group ? <EOS> is that right ? <EOS> yeah . <EOS> so i guess i should probably talk to him a bit too ? <EOS> yeah . <EOS>
but the spectral subtraction scheme that you reported on also re requires a a noise estimate . <EOS> yeah . <EOS> could n't you try this for that ? <EOS> do you think it might help ? <EOS> yeah , for for sure i will . <EOS> i can try also , mmm , the spectral subtraction . <EOS>
yeah , another thing that i it 's important to mention is , um , that this has a this has some additional latency . <EOS> and i noticed that it 's better if we take into account this latency . <EOS> it 's depending on how all this stuff comes out we may or may not be able to add any latency . <EOS> b but i do n't think we have to worry too much on that right now while you kno . <EOS> i would worry about it a little . <EOS> because if we completely ignore latency , and then we discover that we really have to do something about it , we 're going to be find ourselves in a bind . <EOS> and and that 's sort of one of the all of that sort of stuff is things that they 're debating in their standards committee . <EOS>
none <EOS>
so , yeah , the this past week i 've been main mainly occupied with , um , getting some results , u from the sri system trained on this short hub - five training set for the mean subtraction method . <EOS> so the last week , uh , i showed some results with only speechdat - car so i was like looking into `` why , what is wrong with the ti - digits ? `` <EOS> . <EOS> and i found that , the noise estimation is a reason for the ti - digits to perform worse than the baseline . <EOS> yeah , there are two figures showing actually the , mmm , um , performance of the current vad . <EOS> well , i only say that the this is , a summary of the of all the vts experiments <EOS>
and then there 's um , another thing i wan na start looking at , um , <breath> wi is , um , the choice of the analysis window length . <EOS> with the with the htk set - up i should be able to do some experiments , on just varying that length , say between one and three seconds , in a few different reverberation conditions , i guess one thing that might also be an issue , uh , cuz part of what you 're doing is you 're getting a a spectrum over a bunch of different kinds of speech sounds . <EOS> and so it might matter how fast someone was talking for instance . <EOS> you know , if you if if if there 's a lot of phones in one second maybe you 'll get a a really good sampling of all these different things , and <breath> and , uh , on the other hand if someone 's talking slowly maybe you 'd need more . <EOS> a actually i was just thinking about what i was asking about earlier , wi which is about having <breath> less than say twelve seconds in the smartkom system to do the mean subtraction . <EOS> you said in <breath> systems where you use cepstral mean subtraction , they concatenate utterances and , <breath> do you know how they address this issue of , um , testing versus training ? <EOS> i think what they do is they do it always on - line , i mean , that you just take what you have from the past , that you calculate the mean of this and subtract the mean . <EOS> and , um , so so in tha in that case , wh what do they do when they 're t um , performing the cepstral mean subtraction on the training data ? <EOS> so because you 'd have hours and hours of training data . <EOS> so do they cut it off and start over ? <EOS> and so if you 're splitting things up into utterances so , for instance , in a dialogue system , where you 're gon na be asking , uh , you know , th for some information , there 's some initial th something . <EOS> and i think the heuristics of exactly how people handle that and how they handle their training i 'm sure vary from place to place . <EOS> so you 'd you and so in training you would start over at at every new phone call or at every <breath> new speaker . <EOS> it it seems to be the best what wh wh what what we can do in this moment is multi - condition training . <EOS> and every when we now start introducing some some noise reduction technique we we introduce also somehow artificial distortions . <EOS> and these artificial distortions uh , i have the feeling that they are the reason why why we have the problems in this multi - condition training . <EOS> that means the h m ms we trained , they are they are based on gaussians , and if we introduce now this this u spectral subtraction , or wiener filtering stuff i mean , this is your noise estimate and you somehow subtract it or do whatever . <EOS> and then i think what you do is you introduce some some artificial distribution in this in in the models . <EOS> so so , basically our our position is <breath> that , um , we should n't be unduly constraining the latency at this point because we 're all still experimenting with trying to make the performance better in the presence of noise . <EOS> uh , there is a minority in that group who is a arguing who are arguing for <breath> um , uh , having a further constraining of the latency . <EOS> so we 're s just continuing to keep aware of what the trade - offs are and , you know , what what do we gain from having longer or shorter latencies ? <EOS> well , france telecom was was was very short latency it was in the order of thirty milliseconds <EOS>
maybe you you are leaving in in about two weeks carmen . <EOS> what what i would do is i i i would pick @ @ the best consolation , which you think , and <breath> c create create all the results for the whole database that you get to the final number as as sunil did it and maybe also to to write somehow a document where you describe your approach , and what you have done . <EOS> i was thinking to do that next week . <EOS> i wi i i will do that next week . <EOS>
so the other thing is the i 'm just looking at a little bit on the delay issue where the delay of the system is like a hundred and eighty millisecond . <EOS> so <breath> i just just tried another sk system i mean , another filter which i 've like shown at the end . <EOS> which is very similar to the existing uh , filter . <EOS> only uh , only thing is that the phase is is like a totally nonlinear phase so it 's just like it 's like a three percent relative degradation , but but is there is there a problem with the one hundred eighty milliseconds ? <EOS>
for italian and spanish it 's th this value works good but not necessarily for finnish . <EOS> but unfortunately there is , like , this forty millisecond latency yeah , so i would try to somewhat reduce this @ @ . <EOS> i already know that if i completely remove this latency , so . <EOS> <breath> um , it um there is a three percent hit on italian . <EOS>
anyway we < clears throat > after coming back from qualcomm we had , you know , very strong feedback and , uh , i think it was <breath> hynek and guenter 's and my opinion also that , um , you know , we sort of spread out to look at a number of different ways of doing noise suppression . <EOS> but given the limited time , uh , it was sort of time to choose one . <EOS> uh , and so , uh , th the vector taylor series had n't really worked out that much . <EOS> uh , the subspace stuff , uh , had not been worked with so much . <EOS> um , so it sort of came down to spectral subtraction versus wiener filtering . <EOS> uh , we had a long discussion about how they were the same and how they were d uh , completely different . <EOS>
so instead they went to yosemite and bonded , and and they came out with a single single piece of software . <EOS> so it 's <breath> another another victory for international collaboration . <EOS> so so you guys have combined or you 're going to be combining the software ? <EOS> well , the piece of software has , like , plenty of options , so depending on that , it it becomes either spectral subtraction or wiener filtering . <EOS> but the thing is the important thing is that there is a piece of software that you that we all will be using now . <EOS>
but , still so , there will be a piece of software with , <mouth> < clears throat > uh , will give this system , the fifty - three point sixty - six , by default how how is how good is that ? <EOS> it 's just one percent off of the best proposal . <EOS> it 's between i we are second actually if we take this system . <EOS> compared to the last evaluation numbers ? <EOS> yeah . <EOS> yeah . <EOS> so it so , um , it 's it it 's not using our full bal bag of tricks , if you will . <EOS> and , uh , and it it is , uh , very close in performance to the best thing that was there before . <EOS> uh , but , you know , looking at it another way , maybe more importantly , uh , <breath> we did n't have any explicit noise , uh , handling we did n't explicitly have anything to deal with stationary noise . <EOS>
i mean , i gather you have it sounds like you have a few more days of of nailing things down with the software and so on . <EOS> but and then but , um , <sniff> arguably what we should do is , even though the software can do many things , we should for now pick a set of things , and not change that . <EOS> and then focus on everything that 's left . <EOS> so there 's the neural net issue . <EOS> there 's the vad issue . <EOS> and , uh , there 's the second stream thing . <EOS> what was the issue with the vad ? <EOS> i guess they still allow two hundred milliseconds on either side or some ? <EOS> and all the speech pauses , which is sometimes on the speechdat - car you have pauses that are more than one or two seconds . <EOS> we cou we can do better , i think , so , our current vad is is more than twenty percent , while their is fourteen . <EOS> that 's that 's a good set of work that that , uh just one more thing . <EOS> like , should we do something f more for the noise estimation , yeah . <EOS> i was wondering about that . <EOS>
and i think , you know , that our goal should be by next week , when hynek comes back , <breath> uh , to uh , really just to have a firm path , uh , for the you know , for the time he 's gone , of of , uh , what things will be attacked . <EOS>
we do still , however , have to consider its latency . <EOS> we ca n't have unlimited amounts of latency . <EOS> uh , y you know , that 's still being debated by the by people in europe but , <breath> uh , no matter how they end up there , it 's not going to be unlimited amounts , so we i mean , if so if we if so which is like if we reduce the delay of va yeah . <EOS> the you smooth it and then delay the decision by so that 's that 's really not not bad . <EOS> so we may in fact we 'll see what they decide . <EOS> we may in fact have , <breath> um , the the , uh , latency time available for to have a neural net . <EOS> what amount of latency are you thinking about when you say that ? <EOS> you know , they 're saying , uh one group is saying a hundred and thirty milliseconds and another group is saying two hundred and fifty milliseconds . <EOS>
so the frame - dropping is the last thing that we do . <EOS> did you happen to notice how much , <breath> uh , the change was due to just this frame - dropping problem ? <EOS> just the frame - dropping problem . <EOS> and then we have to be careful with that also with the neural net because in the proposal the neural net was also , uh , working on after frame - dropping . <EOS> oh , that 's a real good point . <EOS>
none <EOS>
so we we had a meeting with , uh with hynek , um , in in which , uh , uh , sunil and stephane , uh <mouth> summarized where they were and and , uh , talked about where we were gon na go . <EOS> so that that happened sort of mid - week . <EOS> but i guess maybe the thing since you were n't yo you guys were n't at that that meeting , might be just just to , um , sort of recap , uh , the the conclusions of the meeting . <EOS> you 're talking about the meeting with hynek ? <EOS> since he 's going out of town like now , and i 'm going out town in a couple weeks , uh , and time is marching , sort of , given all the mu many wonderful things we could be working on , what what will we actually focus on ? <EOS> and , uh and what do we freeze ? <EOS> and , you know , what do we ? <EOS> and then within that , i guess the idea was to freeze a certain set of options for now , to run it , uh , a particular way , and decide on what things are gon na be experimented with , as opposed to just experimenting with everything . <EOS> so keep a certain set of things constant . <EOS> uh , maybe describe roughly what what we are keeping constant for now , well . <EOS> so we 've been working like six weeks on on the noise compensation and we end up with something that seems reasonable . <EOS> are you gon na use which of the two techniques ? <EOS> so finally it 's it 's , um , wiener filtering on fft bins . <EOS> so we are going to fix this for the moment and work on the other aspects of < clears throat > the whole system . <EOS> but structurally it seemed like the things the main things that that we brought up that , uh , are are gon na need to get worked on seriously are , uh , uh , a < clears throat > a significantly better vad , uh , putting the neural net on , um , which , you know , we have n't been doing anything with , the , uh , neural net at the end there , and , uh , the , uh , <breath> opening up the second front . <EOS>
but structurally it seemed like the things the main things that that we brought up that , uh , are are gon na need to get worked on seriously are , uh , uh , a < clears throat > a significantly better vad , uh , putting the neural net on , um , which , you know , we have n't been doing anything with , the , uh , neural net at the end there , and , uh , the , uh , <breath> opening up the second front . <EOS> the other half of the channel ? <EOS> yeah , yeah , i mean , cuz we we have we have , uh , uh , half the the , uh , data rate that they allow . <EOS> and , uh , so the initial thing which came from , uh , the meeting that we had down south was , uh , that , um , we 'll initially just put in a mel spectrum as the second one . <EOS> it 's , you know , cheap , easy . <EOS> there 's a question about exactly how we do it . <EOS> we probably will go to something better later , and and , < clears throat > um , you know , in some sense we 're all doing fairly similar things . <EOS> so how did they fill up this all these these bits ? <EOS> um , why are we using half ? <EOS> we have the on - line normalization and then we have the lda rasta . <EOS> the lda rasta , uh , throws away high modulation frequencies . <EOS> and they 're not doing that . <EOS> so that if you throw away high modulation frequencies , then you can downsample . <EOS> and , uh , so i you know , we we 've found in a lot of ways for quite a while that having a second stream uh , helps a lot . <EOS> so that 's that 's put in , and you know , it may even end up with mel spectrum even though i 'm saying i think we could do much better , just because it 's simple . <EOS> so this second stream , will it add latency to the system no , it 's in parallel . <EOS> we 're not talking about computation time here . <EOS> so it 's just in terms of what data it 's depending on . <EOS> it 's depending on the same data as the other . <EOS>
what about the , um uh , the new part of the evaluation , the , uh , wall street journal part ? <EOS> have you ever worked with the mississippi state h uh , software ? <EOS> not yet . <EOS> well you you may be called upon to help , uh , uh , on account of , uh , all the work in this stuff here has been , uh , with small vocabulary . <EOS> ok . <EOS> oh , so they 're gon na just deliver a system basically . <EOS> yeah , th i i guess it 's almost ready . <EOS> so they have released their , uh , document , describing the system . <EOS> cuz one of the things that might be helpful , if you 've if you 've got time in all of this is , is if if these guys are really focusing on improving , uh , all the digit stuff , uh , maybe and you got the front - end from them , maybe you could do the runs for the sure . <EOS> and and , you know , iron out hassles that that you have to , uh , tweak joe about or whatever , because you 're more experienced with running the large vocabulary stuff . <EOS> so i 'll point you to the web site and the mails corresponding . <EOS> so these sugges these this , uh , period during which people are gon na make suggestions is to know whether it is actually biased towards any set of features or yeah , so i th th certainly the thing that i would want to know about is whether we get really hurt , uh , on in insertion penalty , language model , scaling , sorts of things . <EOS> using our features . <EOS> uh , in which case , um , h hari or hynek will need to , you know , push the case more about about this . <EOS> and we may be able to revisit this idea about , you know , somehow modifying our features to work with <EOS>
got anything to tell us ? <EOS> well , i 've been reading some literature about clustering of data . <EOS> ok , so we 're talking about discovering intermediate categories to , um to classify . <EOS> and , uh , i was looking at some of the work that , uh , sangita was doing on these traps things . <EOS> so she has , um she has temporal patterns for , um , a certain set of phonemes , from from timit , um , and , um , i was thinking about ways to to generalize this because w you 're it 's sort of like a it 's not a completely automatic way of clustering , are you looking at these in narrow bands ? <EOS> yeah , i mean , it seems somehow that needs th uh , there 's a couple things that i wonder about with this . <EOS> i mean , if you 're going for this sort of thing where you have uh , little detectors that are looking at narrow bands , then what you 're going to be looking for should be some category that you can find with the narrow bands . <EOS> um , the sort of standard answer about this sort of thing is that if you 're trying to find the right system in some sense , whether you 're trying by categories or or parameters um , and your goal is discrimination , then having choices based on discrimination as opposed to , um , unsupervised nearness of things , um , is actually better . <EOS> um , and i do n't know if that i mean , since you 're dealing with issues of robustness , you know , maybe maybe this is n't right , but it 'd be something i 'd be concerned about . <EOS> because , for instance , you can imagine , uh , uh , i i if you remember from from , uh from your your quals , john ohala saying that , uh , `` buh `` and `` puh `` differed , uh , not really cuz of voicing but because of aspiration . <EOS> so , um , if you looked if you were doing some coarse clustering , you probably would put those two sounds together . <EOS> and yet , i would gue i would guess that many of your recognition errors were coming from , uh , um , pfft , screwing up on this distinction . <EOS> if you go and take any recognizer that 's already out there and you say , `` how well is it distinguishing between schwas and stops ? `` <EOS> boy , i bet they 're all doing nearly perfectly on this , <EOS>
so , i 'll , um i 'll actually after the meeting i 'll add the second stream to the vad and maybe i 'll start with the feature net in that case . <EOS> ok , so just figure how to take the features from the final <EOS>
have you ever worked with the mississippi state h uh , software ? <EOS> not yet . <EOS> well you you may be called upon to help , uh , uh , on account of , uh , all the work in this stuff here has been , uh , with small vocabulary . <EOS> ok . <EOS> cuz one of the things that might be helpful , if you 've if you 've got time in all of this is , is if if these guys are really focusing on improving , uh , all the digit stuff , uh , maybe and you got the front - end from them , maybe you could do the runs for the sure . <EOS> and and , you know , iron out hassles that that you have to , uh , tweak joe about or whatever , because you 're more experienced with running the large vocabulary stuff . <EOS> so i 'll point you to the web site and the mails corresponding . <EOS> you know joe , just to sort of ask him about the issue of , um , different features having different kinds of , uh , scaling characteristics and so on . <EOS> so sh shall we , like , add chuck also to the mailing lists ? <EOS> that 'd be great . <EOS> yeah , i guess maybe hari or hynek , one of them , has to send a mail to joe . <EOS> i i could send him an email . <EOS> i i was just talking with him on email the other day actually . <EOS> uh , yeah , and just , um , se maybe see . <EOS> yeah , so maybe just cc hari and say that you 've just been asked to handle the large vocabulary part here , why do n't you just ask joe but cc hari , and then in the note say , `` hari , hopefully this is ok with you `` . <EOS> and then if joe feels like he needs a confirmation , hari can answer it . <EOS>
oh , so they 're gon na just deliver a system basically . <EOS> yeah , th i i guess it 's almost ready . <EOS> so they have released their , uh , document , describing the system . <EOS> so these sugges these this , uh , period during which people are gon na make suggestions is to know whether it is actually biased towards any set of features or yeah , so i th th certainly the thing that i would want to know about is whether we get really hurt , uh , on in insertion penalty , language model , scaling , sorts of things . <EOS> using our features . <EOS> uh , in which case , um , h hari or hynek will need to , you know , push the case more about about this . <EOS> and we may be able to revisit this idea about , you know , somehow modifying our features to work with <EOS>
none <EOS>
um , i 've been playing with , first , the , um , vad . <EOS> um , < clears throat > so it 's exactly the same approach , <EOS>
but well , we could probably put the delta , um , <mouth> before on - line normalization . <EOS> what if you used a smaller window for the delta ? <EOS> i mean , i guess there 's a lot of things you could do to so if you if you put the delta before the , uh , ana on - line if uh then then it could go in parallel . <EOS> cuz the time constant of the on - line normalization is pretty long compared to the delta window , and you could experiment with cutting various pieces of these back a bit , i mean , we 're s we 're not we 're not in terrible shape . <EOS> well , what 's your what 's your thought about what to do next with it ? <EOS> i 'm surprised , because i expected the neural net to help more when there is more mismatch , as it was the case for the well , we might uh , we might have to experiment with , uh better training sets . <EOS> i the other thing is , i mean , before you found that was the best configuration , but you might have to retest those things now that we have different the rest of it is different , for instance , what 's the effect of just putting the neural net on without the o other other path ? <EOS> i mean , you know what the straight features do . <EOS> in the , um a lot of the , um the hub - five systems , um , recently have been using lda . <EOS> and and they , um they run lda on the features right before they train the models . <EOS> uh , this lda is different from the lda that you are talking about . <EOS> the lda that you saying is , like , you take a block of features , like nine frames or something , and then do an lda on it , and then reduce the dimensionality to something like twenty - four or something like that . <EOS> so this is a two dimensional tile . <EOS> and the lda that we are f applying is only in time , so it 's like more like a filtering in time , but what if you put ran the other kind of lda , uh , on your features right before they go into the hmm ? <EOS> but it 's it 's like a nonlinear discriminant analysis . <EOS> the tandem stuff is kind of like i nonlinear lda . <EOS> but i mean , w but the other features that you have , um , th the non - tandem ones , well , in the proposal , they were transformed u using pca , yeah , it might be that lda could be better . <EOS> the uh , other thing i was wondering was , um , if the neural net , um , has any because of the different noise con unseen noise conditions for the neural net , where , like , you train it on those four noise conditions , while you are feeding it with , like , a additional some four plus some f few more conditions which it has n't seen , actually , instead of just h having c uh , those cleaned up t cepstrum , sh should we feed some additional information , like the the i mean , should we f feed the vad flag , also , at the input so that it it has some additional discriminating information at the input ? <EOS> we have the vad information also available at the back - end . <EOS> so if it is something the neural net is not able to discriminate the classes so , by having an additional , uh , feature which says `` this is speech and this is nonspeech `` , i mean , it certainly helps in some unseen noise conditions for the neural net . <EOS> so you 're saying , feed that , also , into the neural net . <EOS> yeah . <EOS> so it it 's an additional discriminating information . <EOS> the other thing you could do is just , um , p modify the , uh , output probabilities of the of the , uh , uh , um , neural net , tandem neural net , based on the fact that you have a silence probability . <EOS>
and actually it brought up a question which may be relevant to the aurora stuff too . <EOS> um , i know that when you figured out the filters that we 're using for the mel scale , there was some experimentation that went on at at , uh at ogi . <EOS> but one of the differences that we found between the two systems that we were using , the the aurora htk system baseline system and the system that we were the the uh , other system we were using , the uh , the sri system , was that the sri system had maybe a , um , hundred hertz high - pass . <EOS> still , it 's possible that we 're getting in some more noise . <EOS> so i wonder , is it @ @ was there their experimentation with , uh , say , throwing away that filter or something ? <EOS> so i think when when he gets done with his prelim study i think <laugh> one of the next things we 'd want to do is to take this , uh uh , noise , uh , processing stuff and and , uh uh , synthesize some speech from it . <EOS>
so i wo n't be here for uh , i 'm leaving next wednesday . <EOS> i 'm leaving leaving next wednesday . <EOS> so next week i wo n't , and the week after i wo n't , cuz i 'll be in finland . <EOS> by that time you 'll be uh , you 'll both be gone from here . <EOS> so it 'll be a few weeks , really , before we have a meeting of the same cast of characters . <EOS> and then uh , uh , we 'll start up again with dave and dave and barry and stephane and us on the , uh , twentieth . <EOS>
is there any word yet about the issues about , um , adjustments for different feature sets or anything ? <EOS> you asked me to write to him uh , i 'll i 'll d i 'll double check that and ask him again . <EOS> it 's like that that could r turn out to be an important issue for us . <EOS>
for instance , what 's the effect of just putting the neural net on without the o other other path ? <EOS> i mean , you know what the straight features do . <EOS> when you in in the old experiments when you ran with the neural net only , and did n't have this side path , um , uh , with the the pure features as well , did it make things better to have the neural net ? <EOS> it was b a little bit worse . <EOS> until you put the second path in with the pure features , the neural net was n't helping at all . <EOS> it was helping , uh , if the features are b were bad , as soon as we added lda on - line normalization , and < clears throat > all these things , then well , i still think it would be k sort of interesting to see what would happen if you just had the neural net without the side thing . <EOS> and and the thing i i have in mind is , uh , maybe you 'll see that the results are not just a little bit worse . <EOS> maybe that they 're a lot worse . <EOS> but if on the ha other hand , uh , it 's , say , somewhere in between what you 're seeing now and and and , uh , what you 'd have with just the pure features , then maybe there is some problem of a of a , uh , combination of these things , or correlation between them somehow . <EOS> if it really is that the net is hurting you at the moment , then i think the issue is to focus on on , uh , improving the the net . <EOS>
is there any word yet about the issues about , um , adjustments for different feature sets or anything ? <EOS> it 's like that that could r turn out to be an important issue for us . <EOS> cuz they have , uh , already frozen those in i insertion penalties and all those stuff is what i feel . <EOS> and they have these tables with , uh , various language model weights , insertion penalties . <EOS> so now , we may come back to the situation where we may be looking for a modification of the features to account for the fact that we ca n't modify these parameters . <EOS> but it 's still worth , i think , just since you know , just chatting with joe about the issue . <EOS>
but the problem is still that the latency is too large . <EOS> the the latency of the vad is two hundred and twenty milliseconds . <EOS> wh - what 's the baseline you need to be under ? <EOS> well , we do n't know . <EOS> they 're still arguing about it . <EOS> i mean , if it 's two if if it 's , uh if it 's two - fifty , then we could keep the delta where it is if we shaved off twenty . <EOS> if it 's two hundred , if we shaved off twenty , we could we could , uh , meet it by moving the delta back . <EOS> so , how do you know that what you have is too much if they 're still deciding ? <EOS> i mean , the main thing is that since that we got burned last time , and you know , by not worrying about it very much , we 're just staying conscious of it . <EOS> i mean , if if if a week before we have to be done someone says , `` well , you have to have fifty milliseconds less than you have now `` , it would be pretty frantic around here . <EOS>
you 're just using the full ninety features ? <EOS> from the networks , it 's twenty - eight . <EOS> and from the other side it 's forty - five . <EOS> so it 's you have seventy - three features , there 's a klt after the neural network , as as before . <EOS> that 's how you get down to twenty - eight ? <EOS> i wanted to do something very similar to the proposal as a first first try . <EOS> but we have to for sure , we have to go down , because the limit is now sixty features . <EOS> we have to find a way to decrease the number of features . <EOS> they felt they wanted to set a limit . <EOS> so they chose sixty . <EOS> i i i think it 's kind of r arbitrary too . <EOS>
yeah , actually < clears throat > to s eh , what i observed in the hm case is that the number of deletion dramatically increases . <EOS> it it doubles . <EOS> when i added the num the neural network it doubles the number of deletions . <EOS> yeah , so i do n't you know <laugh> how to interpret that , me either . <EOS> and and did an other numbers stay the same ? <EOS> they p stayed the same , did they increase the number of deletions even for the cases that got better ? <EOS> no . <EOS> so it 's only the highly mismatched ? <EOS> now the only thing that that bothers me about all this is that i i i the the fact i i it 's sort of bothersome that you 're getting more deletions . <EOS> so i might maybe look at , is it due to the fact that um , the probability of the silence at the output of the network , is , too too high <EOS>
none <EOS>
yeah . <EOS> so there was this conference call this morning , and the only topic on the agenda was just to discuss a and to come at uh , to get a decision about this latency problem . <EOS>
uh , yeah . <EOS> there were like two hours of discussions , and then suddenly , <breath> uh , people were tired , i guess , and they decided on < mike noise > a number , two hundred and twenty , included e including everything . <EOS> so , currently d uh , we have system that has two hundred and thirty . <EOS> we have to reduce it by ten milliseconds somehow . <EOS> that 's not a problem , i i guess . <EOS> w it 's it 's p d primary primarily determined by the vad at this point , s so we can make the vad a little shorter . <EOS> yeah . <EOS> we probably should do that pretty soon so that we do n't get used to it being a certain way . <EOS> uh , yeah . <EOS> so , the second thing is the system that we have currently . <EOS> oh , yes . <EOS> we have , like , a system that gives sixty - two percent improvement , but <mouth> if you want to stick to the <breath> this latency well , it has a latency of two thirty , but <breath> if you want also to stick to the number <breath> of features that limit it to sixty , <breath> then we go a little bit down but it 's still sixty - one percent . <EOS>
um , while we 're still on aurora stuff maybe you can talk a little about the status with the , uh , <breath> wall street journal <breath> things for it . <EOS> so i 've , um , downloaded , uh , a couple of things from mississippi state . <EOS> they wrote some scripts that sort of make it easy to run <breath> the system on the wall street journal , uh , data . <EOS> um , so i have n't run the scripts yet . <EOS> uh , i 'm waiting there was one problem with part of it and i wrote a note to joe asking him about it . <EOS> so i 'm waiting to hear from him . <EOS> they 're i i 'm still waiting for them to release the , um , <mouth> multi - cpu version of their scripts , cuz right now their script only handles processing on a single cpu , which will take a really long time to run . <EOS> so , as soon as they get that , then i 'll i 'll grab those too yeah . <EOS> cuz we have to get started , yeah . <EOS> i 'll go ahead and try to run it though with just the single cpu one , and i they they , <breath> um , released like a smaller data set that you can use that only takes like sixteen hours to train and stuff . <EOS> so i can i can run it on that just to make sure that the <breath> the thing works and everything . <EOS> so it could be i mean , chuck and i had actually talked about this a couple times , and and over some lunches , i think , <breath> that , um , <mouth> one thing that we might wan na do the - there 's this question about , you know , what do you wan na scale ? <EOS> suppose y you ca n't adjust <breath> these word insertion penalties and so forth , so you have to do everything at the level of the features . <EOS> and , uh , one thing i had suggested at an earlier time was maybe some sort of scaling , some sort of root or or something of the , um , <mouth> uh , features . <EOS> it occurred to me later , because what you really want to do is scale the , uh , @ @ the range of the likelihoods rather than but , i mean , i guess we still have n't had a <breath> a ruling back on this . <EOS> and we may end up being in a situation where we just you know really ca n't change the <breath> word insertion penalty . <EOS> but the other thing we could do <breath> is also we could i mean , this this may not help us , <breath> uh , in the evaluation but it might help us in our understanding at least . <EOS> we might , <breath> just run it with different insper insertion penalties , and show that , uh , `` well , ok , not changing it , <breath> playing the rules the way you wanted , we did this . <EOS> but in fact if we did that , it made a a big difference . `` <EOS>
so michael kleinschmidt , who 's a phd student from germany , <breath> showed up this week . <EOS> he 'll be here for about six months . <EOS> and he 's done some work using <breath> an auditory model of , um , <breath> human hearing , and using that f uh , to generate speech recognition features . <EOS> and he did <breath> work back in germany <breath> with , um , a toy recognition system <breath> using , um , isolated <breath> digit recognition <breath> as the task . <EOS> he w he 's coming here to u u use it on a <breath> uh , a real speech recognition system . <EOS> th - this is because it 's , um there are these different parameters for the shape of these <breath> basis functions , <breath> um <breath> there are a lot of different possible basis functions . <EOS> and so he <breath> he actually does <breath> an optimization procedure to choose an <breath> an optimal set of basis functions out of all the possible ones . <EOS> is , <breath> um , <mouth> he starts with he has a set of m of them . <EOS> i mean , he t he tries , um , <mouth> using just m minus one of them . <EOS> so there are m possible subsets of this <breath> length - m vector . <EOS> he tries classifying , using each of the m <breath> possible sub - vectors . <EOS> whichever sub - vector , <breath> um , works the the best , i guess , he says <breath> the the fe feature that did n't use was the most useless feature , so we 'll throw it out and we 're gon na randomly select another feature from the set of possible basis functions . <EOS> so i th i think it 's it 's i think it 's kinda neat stuff . <EOS> the thing that i wanted to to add to it also was to have us use this in a multi - stream way . <EOS> so so that , um , <mouth> when you come up with these different things , <breath> and these different functions , <breath> you do n't necessarily just put them all into one huge vector , but perhaps < clears throat > you <breath> have some of them in one stream and some of them in another stream , and so forth . <EOS> well , that sort of segues into what what i 'm doing . <EOS> um , <breath> so , uh , the big picture is k um , <mouth> come up with a set of , <breath> uh , intermediate categories , then build intermediate category classifiers , then do recognition , um , so right now i 'm in in the phase where <breath> i 'm looking at at , um , deciding on a initial set of intermediate categories . <EOS> and <breath> i 'm looking <breath> for data data - driven methods that can help me find , <breath> um , a set of intermediate categories <breath> of speech that , uh , will help me to discriminate later down the line . <EOS> and one of the ideas , <breath> um , that was to take a take a neural net train train an ordinary neural net <breath> to <breath> uh , to learn the posterior probabilities of phones . <EOS> um , <mouth> the other one was , <breath> um , to , <breath> uh , come up with a a a model um , a graphical model , <breath> that treats the intermediate categories <breath> as hidden hidden variables , latent variables , that we do n't know anything about , but that through , <breath> um , s statistical training and the em algorithm , <breath> um , at the end of the day , <breath> we have , um we have learned something about these these latent , um latent variables which happen to correspond to <breath> intermediate categories . <EOS>
ho - how much memory d ? <EOS> h how many ? <EOS> i d i d uh , i i do n't kn remember exactly , yeah . <EOS> i 'd like to see that , cuz maybe i could think a little bit about it , cuz we <mouth> maybe we could make it a little smaller uh , i 'd like to see how far off we are . <EOS> but i guess it 's still within their rules to have have it on the , uh , t uh , server side . <EOS>
uh , yeah . <EOS> there were like two hours of discussions , and then suddenly , <breath> uh , people were tired , i guess , and they decided on < mike noise > a number , two hundred and twenty , included e including everything . <EOS> so , currently d uh , we have system that has two hundred and thirty . <EOS> we have to reduce it by ten milliseconds somehow . <EOS> that 's not a problem , i i guess . <EOS> w it 's it 's p d primary primarily determined by the vad at this point , s so we can make the vad a little shorter . <EOS> yeah . <EOS> we probably should do that pretty soon so that we do n't get used to it being a certain way . <EOS> uh , yeah . <EOS> so , the second thing is the system that we have currently . <EOS> oh , yes . <EOS> we have , like , a system that gives sixty - two percent improvement , but <mouth> if you want to stick to the <breath> this latency well , it has a latency of two thirty , but <breath> if you want also to stick to the number <breath> of features that limit it to sixty , <breath> then we go a little bit down but it 's still sixty - one percent . <EOS>
uh , and if we drop the tandem network , then we have fifty - seven percent . <EOS> uh , but th the two th two thirty includes the tandem network ? <EOS> and i is the tandem network , uh , small enough that it will fit on the terminal size uh , no , i do n't think so . <EOS> it 's still in terms of computation , if we use , like , their way of computing the the maps the the mips , <breath> i think it fits , but it 's , uh , m mainly a problem of memory . <EOS> and i do n't know how much this can be discussed or not , because it 's it could be in rom , so it 's maybe not that expensive . <EOS> ho - how much memory d ? <EOS> h how many ? <EOS> i d i d uh , i i do n't kn remember exactly , yeah . <EOS> i 'd like to see that , cuz maybe i could think a little bit about it , cuz we <mouth> maybe we could make it a little smaller uh , i 'd like to see how far off we are . <EOS> but i guess it 's still within their rules to have have it on the , uh , t uh , server side . <EOS>
yeah . <EOS> the last thing is that i think we are getting close to human performance . <EOS> well , that 's something i would like to investigate further , i did , like , um i did , uh , listen to the m most noisy utterances of the speechdat - car italian and tried to transcribe them . <EOS> so this is a particular human . <EOS> this is this i this is stephane . <EOS> that 's the the flaw of the experiment . <EOS> um , but what happens also is that if i listen to the , um <noise> a re - synthesized version of the speech and i re - synthesized this using a white noise that 's filtered by a lpc , uh , filter while our system is currently at seven percent . <EOS> but still , uh , < breath-laugh > what happens is is that , <mouth> uh , the digit error rate on this is around one percent , well , you can argue , that , uh that this is not speech , so the ear is not trained to recognize this . <EOS> but s actually it sound like whispering , there 's two problems there . <EOS> i mean i mean , so so the first is <breath> that by doing lpc - twelve with synthesized speech w like you 're saying , uh , it 's <breath> i i you 're you 're adding other degradation . <EOS> so it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . <EOS> and the second thing is which is m maybe more interesting is that , um , <breath> if you do it with whispered speech , you get this number . <EOS> what if you had done analysis re - synthesis and taken the pitch as well ? <EOS> so now you put the pitch in . <EOS> what would the percentage be then ? <EOS> see , that 's the question . <EOS> that would say at least for people , having the pitch is really , really important , i mean , th the thing is lpc is not a a really great representation of speech . <EOS> uh , but i i do n't know . <EOS> i do do n't wan na take you away from other things . <EOS> yeah . <EOS> i mean , it 's probably not worth your time . <EOS> it 's it 's a side thing and and and there 's a lot to do . <EOS>
so what we had was that we were gon na talk about data collection , so <outbreath> so the question that that we started with was whether there was anything else we should do during during th during the collection . <EOS> i guess a lot of the stuff we 're doing now really is pilot <EOS>
right . <EOS> i mean , we because you 'd have several people with these pads , you could collect different things . <EOS> and so why do n't we just use the notes that somebody takes ? <EOS> and i guess the crosspads was certainly one idea , so , i j i think we should just say this is not we do n't want to put any extra burden on people , but if they happen to generate minutes , could could they send it to us ? <EOS> but but if there 's some cases where they will , then it would be helpful . <EOS> crosspads we were going to try , <EOS>
but i 'm just saying first of all there 's a whole bunch of fusion issues that darpa 's interested in . <EOS>
i think for this data capture , it would be nice to have a digital camera just to take pictures of who 's there , and then we could also put in what 's on the board . <EOS> well , minimally , i mean , what what dan is referring to at least having some representation of the p the spatial position of the people , like for a meeting like this , at least , uh , take a polaroid of the <laugh> of the of the boards , a couple digital pictures of the the table and boards to set the context of the meeting . <EOS>
and the fir third thing i wanted to say is the summaries afterwards , i e my thought was to have multiple people summarize it , on recording rather than writing you know , a two - minute summary of what the meeting was about , i think you would get , so , my proposal would be that it may be worth considering both of those types , you know , the note - taking and a spontaneous oral summary afterwards , yeah . <EOS> i think that i think doing it orally at the end of the meeting is the best time . <EOS> and then the last thing c would be for those people who are willing to stay afterwards and give an oral summary . <EOS>
i just do n't know how else to generate the queries other than getting an expert to actually listen to the meeting and say `` that 's important , but if we were asking the question , which i thought we were , of of of , um , `` how do we figure out what 's the nature of the queries that people are gon na want to ask of such a system ? `` <EOS> , knowing what 's important does n't tell you what people are going to be asking . <EOS> now i 'm thinking that the summary a summary , uh , is actually a reasonable , uh , bootstrap into this into what we 'd like to get at . <EOS> the question i had about queries was , um , so what we 're planning to do is have people look at the summaries and then generate queries ? <EOS> u i i actually think that that , uh , again , just as a bootstrap , if we do have something like summaries , then having the people who are involved in the meetings themselves , who are cooperative and willing to do yet more , come up with with with queries , uh , could at least give give landay an idea of the kind of things that people might want to know . <EOS>
and and th i think that might then help me to think of things even things that are n't listed in the summary , but just as a as a as a refresh of what the general thing was going on in the meeting . <EOS> and but for some new reason i 'm i 'm i 'm interested in in in the old stuff . <EOS> you know , if this is something that requires a a one - word answer or it 's one place in the recording versus was there general agreement on this issue of all the people who ha absolutely . <EOS> so i think we 're gon na have to start with keywords i i was wondering if if there might be one s more source of queries which is indicator phrases like `` action item `` , <EOS>
i i i mean , i guess what i what i i keep coming back to in my own mind is that , um , the soonest we can do it , we need to get up some kind of system if you know , if uh , as soon as we can get that going at any kind of level , then i think we 'll have a much better handle on what kind of questions people want to ask than in any anything we do before that . <EOS> well , and again , if we can figure out a way to jimmy a a a a very rough system , say in a year , then uh , so that in the second and third years we we actually have something to <EOS>
and then what they 're gon na do is take the cd - rom and transfer it to analog tape oh , is this ibm ? <EOS> yeah . <EOS> and <breath> give it to a transcription service , uh , that will <EOS>
different level , prosody and all that sort of stuff . <EOS> w my my u feeling right now on format is you guys have been doing all the work <EOS>
and , i guess we just left it as @ @ that if there 's found data that can be transformed for use in speech recognition easily , then of course we would do it , but they were recorded anyway , like the congressional hearings and , you know , for legal purposes or whatever . <EOS> but it includes like standard corpora that have been used for years in linguistics and other fields . <EOS>
to c and i 'll put together an overall cover . <EOS> people are supposed to send me u r for their for web pages , uh , you need to put together a mailing list . <EOS> we talked about that we 're getting the recording equipment running at uw . <EOS>
and other tasks during data collection , but if we had the crosspads , we could ask people , you know , if if something comes up < audible closure > write it down and mark it < audible closure > somehow , so , if you could sense just when people are writing , and you tell them not to doodle , or try not to be using that for other purposes , and each person has a note pad . <EOS> they just get it when they come in the room . <EOS> then you c you can just have a fff plot of wh you know , who 's writing when . <EOS> but i bet that 's that will allow you to go into the sort of the hot places where people are writing things down . <EOS> so <outbreath> so the question that that we started with was whether there was anything else we should do during during th during the collection . <EOS> and i guess the crosspads was certainly one idea , ok . <EOS> so crosspads , we 're just gon na try it and see what happens . <EOS> crosspads we were going to try , <EOS>
and the fir third thing i wanted to say is the summaries afterwards , and so why do n't we just use the notes that somebody takes ? <EOS> i e my thought was to have multiple people summarize it , on recording rather than writing you know , a two - minute summary of what the meeting was about , i think you would get , so , my proposal would be that it may be worth considering both of those types , you know , the note - taking and a spontaneous oral summary afterwards , so , i j i think we should just say this is not we do n't want to put any extra burden on people , but if they happen to generate minutes , could could they send it to us ? <EOS> yeah . <EOS> i think that i think doing it orally at the end of the meeting is the best time . <EOS> getting electronic summary from a note - taking person if they happen to do it anyway . <EOS> and then the last thing c would be for those people who are willing to stay afterwards and give an oral summary . <EOS>
i think for this data capture , it would be nice to have a digital camera just to take pictures of who 's there , and then we could also put in what 's on the board . <EOS> well , minimally , i mean , what what dan is referring to at least having some representation of the p the spatial position of the people , like for a meeting like this , at least , uh , take a polaroid of the <laugh> of the of the boards , a couple digital pictures of the the table and boards to set the context of the meeting . <EOS>
i 'm also wondering if we could ask the the people a a question which would be `` what was the most interesting thing you got out of this meeting ? `` <EOS> we 'll i mean , we 'll we 'll be telling them that the reason we 're trying to do this is is to d generate queries in the future , uh , and then going around the room at the end to just say qu ask people to mention something interesting that they learned . <EOS>
and , i guess we just left it as @ @ that if there 's found data that can be transformed for use in speech recognition easily , then of course we would do it , <EOS>
i do n't think we should have rules of participation , so i 'm just writing here , we 're not gon na try to specify rules of interaction <EOS>
but i think we should try to get a variety of meetings . <EOS> that 's something that if we get the the meeting stuff going at uw , that i probably can do more than you guys , so it also depends on the style of the group of people . <EOS> but we 're gon na try to get more variety by i using different groups of people <EOS>
people are supposed to send me u r for their for web pages , to c and i 'll put together an overall cover . <EOS> we talked about that we 're getting the recording equipment running at uw . <EOS>
and , uh , once we get out beyond our little group , the people 's motivation factor , uh , reduces enormously . <EOS> you 're probably not gon na get a lot of people wanting to do this . <EOS> that that y that you ca n't certainly ca n't require it or people are n't gon na want to do this . <EOS>
so , i j i think we should just say this is not we do n't want to put any extra burden on people , but if they happen to generate minutes , could could they send it to us ? <EOS> yeah . <EOS> what i was gon na say is that i do n't want to ask people to do something they would n't normally do in a meeting . <EOS> you 're probably not gon na get a lot of people wanting to do this . <EOS> that that y that you ca n't certainly ca n't require it or people are n't gon na want to do this . <EOS>
but that it 's gon na be a lot of effort on our part to create it , and store it , i think that if you have that , then people who are interested in vision can use this database . <EOS> the problem with it is you 'll have more people who do n't want to be filmed than who do n't want to be recorded . <EOS> but , you know , that 's a lot of infrastructure and work . <EOS>
i mean , the down - side to that is that he sort of indicated that the , uh , quality of <outbreath> the handwriting recognition was quite poor . <EOS>
but as far as i know they did n't offer that data to the community at this meeting . <EOS> so there 's lots of recordings that they 're not close - talk mike , <EOS>
i just do n't know how else to generate the queries other than getting an expert to actually listen to the meeting and say `` that 's important , but if we were asking the question , which i thought we were , of of of , um , `` how do we figure out what 's the nature of the queries that people are gon na want to ask of such a system ? `` <EOS> , knowing what 's important does n't tell you what people are going to be asking . <EOS> uh , i would n't have any idea what kind of questions i want to ask . <EOS>
i mean , < long inbreath > < forceful outbreath > the level of the query could be , you know , very low - level or very high - level . <EOS>
uh , so do they how are they gon na do the multi - channel ? <EOS>
i just wanted to be sure that we will not be having a lot of data which ca n't be processed . <EOS>
so the there will be jargon that we he there 'll be transcription errors . <EOS>