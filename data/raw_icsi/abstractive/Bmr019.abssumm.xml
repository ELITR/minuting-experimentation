<?xml version="1.0" encoding="ISO-8859-1" standalone="yes"?>
<nite:root xmlns:nite="http://nite.sourceforge.net/">
<abstract nite:id="Bmr019.abstract.1">
<sentence nite:id="Bmr019.s.1">The Berkeley Meeting Recorder group discussed efforts to train and test the Aurora group's HTK-based recognition system on ICSI's digits corpus.</sentence> <sentence nite:id="Bmr019.s.2">Members also discussed efforts to produce forced alignments from a selection of Meeting Recorder data.</sentence> <sentence nite:id="Bmr019.s.3">Performance in both tasks was adversely affected by the manner of recording conditions implemented and difficulties attributing utterances to the appropriate speakers.</sentence> <sentence nite:id="Bmr019.s.4">While debugging efforts resulted in improved forced alignments, dealing with mixed channel speech and speaker overlap remains a key objective for future work.</sentence> <sentence nite:id="Bmr019.s.5">The group is additionally focused on a continued ability to feed different features into the recognizer and then train the system accordingly.</sentence>   

</abstract>
<decisions nite:id="Bmr019.decisions.1">
<sentence nite:id="Bmr019.s.6">For comparing Meeting Recorder digits results, it was decided that the Aurora HTK-based system should be tested on data from the TI digits corpus.</sentence> <sentence nite:id="Bmr019.s.7">The script for extracting speaker ID information will require modifications to obtain a more accurate estimation of the amount of data recorded per speaker.</sentence> <sentence nite:id="Bmr019.s.8">Subsequent recognition experiments will look at large vocabulary speech from a far-field microphone (as performed in Switchboard evaluations).</sentence> <sentence nite:id="Bmr019.s.9">Hand-marked, word-level alignments are needed to reveal speaker boundaries and tune the parameters of the model.</sentence> <sentence nite:id="Bmr019.s.10">Modifications to the Transcriber tool are required for allowing transcribers to simultaneously view the signal in XWaves and see where words are located in time.</sentence> 

</decisions>
<problems nite:id="Bmr019.problems.1">
<sentence nite:id="Bmr019.s.11">Digits training needs to be performed on a larger data set.</sentence> <sentence nite:id="Bmr019.s.12">A significant loss in recognition resulted from not having included the type of phone-loop adaptation found in the SRI system.</sentence> <sentence nite:id="Bmr019.s.13">Recognition performance was worse for digits recorded in closed microphone conditions versus those recorded in a studio (e.g. TI-digits).</sentence> <sentence nite:id="Bmr019.s.14">A mismatch between the manner in which data were collected and the models used for doing recognition---e.g. bandwidth parameterization and the use of near- versus far-field microphones---was identified.</sentence> <sentence nite:id="Bmr019.s.15">Too little data per speaker can have a negative effect on VTL estimation.</sentence> <sentence nite:id="Bmr019.s.16">The PZM channel selected for obtaining digits data was too far away from most of the speakers.</sentence> <sentence nite:id="Bmr019.s.17">Current speech alignment techniques assume that foreground speech must be continuous and, barring some isolated words and backchannels, can not cope with overlapping background speech.</sentence> <sentence nite:id="Bmr019.s.18">Performing adaptations on both the foreground and background speaker produced a new variety of misalignments, a problem resulting, in part, from the fact that background speakers often match better to foreground conditionss.</sentence> <sentence nite:id="Bmr019.s.19">Transcribers occasionally misidentified speakers and omitted backchannels that were more hidden in the mixed signal.</sentence>   

</problems>
<progress nite:id="Bmr019.progress.1">
<sentence nite:id="Bmr019.s.20">Good recognition performance was achieved with the lapel microphones.</sentence> <sentence nite:id="Bmr019.s.21">The recognizer performed well on time-aligned segments labelled as 'non-overlap' (i.e. one person talking), while segments labelled as 'overlap' (i.e. multiple speakers talking at the same time) yielded poor results.</sentence> <sentence nite:id="Bmr019.s.22">Future recognition efforts will include looking at reverberation.</sentence> <sentence nite:id="Bmr019.s.23">Forced alignment improvements were gained by examining the types of errors generated and making the necessary adjustments.</sentence> <sentence nite:id="Bmr019.s.24">More accurate alignments were achieved by significantly increasing the pruning value.</sentence> <sentence nite:id="Bmr019.s.25">Future alignment efforts will include cloning the reject model, and adapting it to both the foreground and background speaker.</sentence> <sentence nite:id="Bmr019.s.26">Members of the group will also compare Meeting Recorder data with other corpora (e.g. Switchboard) to determine whether speaker overlap is a feature that is more specific to meetings versus other modes of spoken interaction.</sentence> <sentence nite:id="Bmr019.s.27">A cursory analysis of background speech revealed that backchannels frequently occurred after a question was asked.</sentence> <sentence nite:id="Bmr019.s.28">Backchannels also featured a high proportion of 'yeahs' and a substantially fewer 'uh-huhs'.</sentence> <sentence nite:id="Bmr019.s.29">Several group members are preparing Eurospeech submissions.</sentence> <sentence nite:id="Bmr019.s.30">Speakers fe016 and mn017 are preparing a paper about the 'spurt' format, wherein spurts from individual channels---i.e. continuous speech regions delineated by pauses---will be extracted, merged with alignments from different channels, and time-aligned.</sentence>

</progress>
</nite:root>