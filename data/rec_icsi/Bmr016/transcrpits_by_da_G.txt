G:  Right . 
G:  Well , we actually figured out a way to  
G:  the  the groupings are randomly generated . 
G:  I think they may still do it , 
G:  um , 
G:  And  
G:  I  well OK  I  it might help , 
G:  I would like to g get away from having only one specific grouping . 
G:  Um , so if that 's your question , 
G:  but I mean it seems to me that , at least for us , we can learn to read them as digits 
G:  if that 's what people want . 
G:  I  I 'm 
G:  don't think that 'd be that hard to read them as single digits . 
G:  Um , 
G:  and it seems like that might be better for you guys 
G:  since then you 'll have just more digit data , 
G:  and that 's always a good thing . 
G:  It 's a little bit better for me too 
G:  because the digits are easier to recognize . 
G:  They 're better trained than the numbers . 
G:  Right . Right , read them as single digits , 
G:  so sixty - one w is read as six one , 
G:  and if people make a mistake we  
G:  But it 's nice to get it in this room with the acous 
G:  I mean  for  it 's  
G:  Right . 
G:  Well that 's fine with me as long as  It 's just that I didn't want to cause the people who would have been collecting digits the other way to not have the digits . 
G:  So  
G:  OK . 
G:  OK . 
G:  It 's actually unclear right now . 
G:  I just thought well we 're  if we 're collec collecting digits , and Adam had said we were running out of the TI forms , I thought it 'd be nice to have them in groups , 
G:  and probably , all else being equal , it 'd be better for me to just have single digits 
G:  since it 's , you know , a recognizer 's gonna do better on those anyway , 
G:  um , and it 's more predictable . 
G:  So we can know from the transcript what the person said and the transcriber , in general . 
G:  But if they make mistakes , it 's no big deal 
G:  if the people say a hundred instead of " one OO " . 
G:  and also w maybe we can just let them choose " zero " versus " O " as they  as they like 
G:  because even the same person c sometimes says " O " and sometimes says " zero " in different context , 
G:  and that 's sort of interesting . 
G:  So I don't have a Specific need 
G:  cuz if I did I 'd probably try to collect it , you know , without bothering this group , 
G:  but If we can try it  
G:  Right , and you can give an example 
G:  like , you know , " six  sixty - one would be read as six one " . 
G:  And I think people will get it . 
G:  Right , right . 
G:  It 's just easier to read . 
G:  Right . 
G:  OK 
G:  I also had a hard  hard time with the words , 
G:  but then we went back and forth on that . 
G:  OK , so let 's give that a try 
G:  and  
G:  I mean what do other people think 
G:  cuz you guys are reading  them . 
G:  OK . 
G:  Great . OK . 
G:  Well let 's give it a try . 
G:  Righ - right , and you just  they 're randomly <mike noise> generated and randomly assigned to digits . 
G:  Right , just groupings in terms of number of groups in a line , 
G:  and number of digits in a group , 
G:  and the pattern of groupings . 
G:  Um , I  I just roughly looked at what kinds of digit strings are out there , 
G:  and they 're usually grouped into either two , three , or four , four digits at a time . 
G:  And they can have , 
G:  I mean , actually , things are getting longer and longer . 
G:  In the old days you probably only had three sequences , and telephone numbers were less , and so forth . 
G:  So , there 's between , um  
G:  Well if you look at it , there are between like three and five groups , 
G:  and each one has between two and four groupings 
G:  and  
G:  I purposely didn't want them to look like they were in any kind of pattern . 
G:  So 
G:  Right . 
G:  But I think it 'd be great i to be able to compare digits , 
G:  whether it 's these digits or TI - digits , to speakers , um , and compare that to their spontaneous speech , 
G:  and then we do need you know a fair amount of  of digit data 
G:  because you might be wearing a different microphone 
G:  and , I mean  so it 's  it 's nice to have the digits you know , replicated many times . 
G:  Especially for speakers that don't talk a lot . 
G:  So <laugh> um , 
G:  for adaptation . 
G:  No , I 'm serious , 
G:  so we have a problem with acoustic adaptation , 
G:  and we 're not using the digit data now , 
G:  but you know  
G:  Not for adaptation , nope . 
G:  v W we 're not  we were running adaptation only on the data that we ran recognition on 
G:  and I 'd  As soon as someone started to read transcript number , that 's read speech 
G:  and I thought " well , we 're gonna do better on that , 
G:  that 's not fair to use " . 
G:  But , it might be fair to use the data for adaptation , 
G:  so . 
G:  So those speakers who are very quiet ,  shy  
G:  r Right  
G:  Well , it sh I mean it 's the same micropho 
G:  see the nice thing is we have that in the  in the same meeting , 
G:  and so you don't get  
G:  Right , and so I still like the idea of having some kind of  digit data . 
G:  The only thing that we don't have , 
G:  I know this sounds weird , 
G:  and maybe it 's completely stupid , 
G:  but we don't have any overlapping digits . 
G:  An - yea I know it 's weird , 
G:  but um  
G:  Alright everybody 's laughing . 
G:  OK . 
G:  I 'm just talkin for the stuff that like Dan Ellis is gonna try , 
G:  you know , cross - talk cancellation . 
G:  OK . 
G:  Wait  oh it  these are all the same forms . 
G:  OK  So but  
G:  So you plu you plug your ears . 
G:  Well , what I mean is actually no not the overlaps that are well - governed linguistically , 
G:  but the actual fact that there is speech coming from two people 
G:  and the beam - forming stuf 
G:  all the acoustic stuff that like Dan Ellis and  and company want to do . 
G:  Digits are nice and well behaved , 
G:  I mean 
G:  Anyway , it 's just a thought . 
G:  It  it would go faster . 
G:  It would take one around  amount of ti 
G:  That 's right . 
G:  I  I mea I 'm  I was sort of serious , 
G:  but I really , I mean , I 'm  I don't feel strongly enough that it 's a good idea , 
G:  so . 
G:  A and that prosody was great , by the way . 
G:  It  it sort of sounded like a duet , or something . 
G:  OK . 
G:  I 'm sorry . 
G:  I 'm mean I think it 's doable , 
G:  I 'm just  
G:  So , we  we could have a round like where you do two at a time , 
G:  and then the next person picks up when the first guy 's done , or something . 
G:  Like a , 
G:  what do you call it ? 
G:  Li - a r like  
G:  yeah , like that . 
G:  Then it would go like h twice as fast , or  a third as fast . 
G:  Anyway , it 's just a thought . 
G:  I 'm actually sort of serious if it would help people do that kind o 
G:  but the people who wanna work on it we should talk to them . 
G:  So . 
G:  OK . 
G:  Yeah . 
G:  Can try it out . 
G:  If we have nothing  if we have no agenda we could do it some week . 
G:  OK . 
G:  I 'm all " four " it . 
G:  Oh , oh ! 
G:  Right , yeah . 
G:  Well I was actually gonna skip the ASR results part , in favor of getting the transcription stuff talked about 
G:  since I think that 's more important to moving forward , 
G:  but I mean Morgan has this paper copy 
G:  and if people have questions , 
G:  um , it 's pretty preliminary in terms of ASR results 
G:  because we didn't do anything fancy , 
G:  but I think e just having the results there , 
G:  and pointing out some main conclusions 
G:  like it 's not the speaking style that differs , 
G:  it 's the fact that there 's overlap that causes recognition errors . 
G:  And then , the fact that it 's almost all insertion errors , 
G:  which you would expect 
G:  but you might also think that in the overlapped regions you would get substitutions and so forth , 
G:  um , leads us to believe that doing a better segmentation , like your channel - based segmentation , or some kind of uh , echo cancellation to get basically back down to the individual speaker utterances would be probably all that we would need to be able to do good recognition on the  on the close - talking mikes . 
G:  So , that 's about the summary  
G:  But this is  Morgan has this paper . 
G:  I mean he  he  
G:  it  it 's that paper . 
G:  Yeah , yeah . 
G:  So , we basically , um , did a lot of work on that 
G:  and it 's  
G:  Let 's see , th 
G:  I guess the other neat thing is it shows for sure w that the lapel , you know within speaker is bad . 
G:  And it 's bad because it picks up the overlapping speech . 
G:  Yes , cuz that 's all that w had been transcribed at the time , 
G:  um 
G:  but as we  I mean I wanted to here more about the transcription . 
G:  If we can get the channel asynchronous or the  
G:  the closer t 
G:  that would be very interesting for us 
G:  because we  
G:  Right . 
G:  That 's  
G:  cuz  
G:  That 's right . In fact I  I pulled out a couple classic examples in case you wanna u use them in your talk of 
G:  Chuck on the lapel , 
G:  so Chuck wore the lapel three out of four times . 
G:  Um , yeah , 
G:  and I wore the lapel once , 
G:  and for me the lapel was OK . 
G:  I mean I still  
G:  and I don't know why . 
G:  I 'm  
G:  But um , 
G:  for you it was  
G:  Or who was next to me or something like that . 
G:  Right , but when Chuck wore the lapel and Morgan was talking there 're a couple really long utterances 
G:  where Chuck is saying a few things inside , 
G:  and it 's picking up all of Morgan 's words pretty well 
G:  and so the rec you know , there 're error rates 
G:  because of insertion  
G:  Insertions aren't bounded , 
G:  so with a one - word utterance and ten insertions you know you got huge error rate . 
G:  And that 's  that 's where the problems come in . 
G:  So I this is sort of what we expected , 
G:  but it 's nice to be able to  to show it . 
G:  And also I just wanted to mention briefly that , um , uh Andreas and I called up Dan Ellis who 's still stuck in Switzerland , 
G:  and we were gonna ask him if  if there 're  you know , what 's out there in terms of echo cancellation and things like that . 
G:  Not that we were gonna do it , 
G:  but we wanted to know what would need to be done . 
G:  And he  We 've given him the data we have so far , 
G:  so these sychronous cases where there are overlap . 
G:  And he 's gonna look into trying to run some things that are out there 
G:  and see how well it can do 
G:  because right now we 're not able to actually report on recognition in a real paper , 
G:  like a Eurospeech paper , 
G:  because it would look sort of premature . 
G:  Right . Or who 's  At any point in time who 's the foreground speaker , 
G:  who 's the background speaker . 
G:  So . 
G:  So there 's like  
G:  Well ther there 's  
G:  Yeah . 
G:  Yeah . Exactly , so it 's  it 's a  
G:  Um , it would be techniques used from adaptive  adaptive echo cancellation 
G:  which I don't know enough about to talk about . 
G:  Um . 
G:  But , 
G:  right , um , and that would be similar to what you 're also trying to do , 
G:  but using um , you know , more than energy  
G:  I  I don't know what exactly would go into it . 
G:  So the idea is to basically run this on the whole meeting . 
G:  and get the locations , which gives you also the time boundaries of the individual speak 
G:  Right . Except that there are many techniques for the kinds of cues , um , that you can use to do that . 
G:  So . 
G:  And I guess Espen ? 
G:  This  is  uh  is he here too ? 
G:  May also be working  
G:  So it would just be ver 
G:  that 's really the next step 
G:  because we can't do too much , you know , on term in terms of recognition results knowing that this is a big problem 
G:  um , until we can do that kind of processing . 
G:  And so , once we have some  some of yours , 
G:  and @ @ we 'll move on . 
G:  OK . 
G:  Oh right . 
G:  Yeah . 
G:  Right . 
G:  Definitely  
G:  Uh , and Don should  
G:  OK . 
G:  Great . 
G:  So that that 's it for the  
G:  Speakers and  
G:  OK . 
G:  Great . 
G:  Right . 
G:  So at this point we can sort of finalize the naming , and so forth , 
G:  and we 're gonna basically re rewrite out these waveforms that we did 
G:  because as you notice in the paper your " M O 
G:  in one meeting 
G:  and " M O - two " in another meeting and it 's  we just need to standardize the 
G:  um , 
G:  no it 's  it 's  
G:  um , that 's why those comments are s <laugh> are in there . 
G:  So  
G:  Right . 
G:  OK . 
G:  Great , great . 
G:  Terrific . 
G:  Eigh - eighteen . 
G:  It was a spare that Dave had around  
G:  Yeah it 's  
G:  Well the  
G:  Yeah . 
G:  Well I 'd leave all the  All the transcript stuff shouldn't  should be backed up , 
G:  but all the waveform   Sound files should not be backed up , 
G:  the ones that you write out . 
G:  We can downsample them , 
G:  so . 
G:  Yeah . 
G:  Yeah , we get the same performance . 
G:  I mean the r the front - end on the SRI recognizer just downsamples them on the fly , 
G:  so  
G:  So that 's  
G:  Yeah , if 
G:  fe 
G:  You 'd  you wanna not . 
G:  OK . 
G:  So we 're  what we 're doing is we 're writing out  
G:  I mean , this is just a question . 
G:  We 're writing out these individual segments , that wherever there 's a time boundary from Thilo , or  or Jane 's transcribers , you know , we  we chop it  there . 
G:  And the reason is so that we can feed it to the recognizer , 
G:  and throw out ones that we 're not using and so forth . 
G:  And those are the ones that we 're storing . 
G:  So  
G:  Yeah . 
G:  Yeah . 
G:  So we can't shorten them , 
G:  but we can downsample them . 
G:  So . 
G:  Oh yeah th 
G:  Yeah . Yeah . 
G:  That  that 's why we need more disk space 
G:  cuz we 're basically duplicating the originals , um  
G:  Oh yeah . 
G:  No . We always have the original long ones . 
G:  Um , it 's better if they 're chopped out , 
G:  and  and it  it will be  
G:  yeah , y we could probably write something to do that , 
G:  but it 's actually convenient to have them chopped out 
G:  cuz you can run them , you know , in different orders . 
G:  You c you can actually move them around . 
G:  Uh , you can get rid of 
G:  Yeah , it it 's a lot faster . 
G:  Right . You can grab everything with the word " the " in it , 
G:  and it 's  
G:  That 's a lot quicker than actually trying to access the wavefile each time , 
G:  find the time boundaries and  
G:  So in principle , yeah , you could do that , 
G:  but it 's  
G:  but it 's um  
G:  These are long  These are long  
G:  You know . 
G:  This is an hour of speech . 
G:  We - yeah that 's  so that 's part of it 
G:  is  
G:  Right . 
G:  And the other part is just that once they 're written out it  it is a lot faster to  to process them . 
G:  So . 
G:  Otherwise , you 're just accessing  
G:  Right . 
G:  Right . 
G:  The other thing is that , believe it or not  I mean , we have some  
G:  So we 're also looking at these in Waves like for the alignments and so forth . 
G:  You can't load an hour of speech into X Waves . 
G:  You need to s have these small files , 
G:  and in fact , even for the Transcriber program Um  
G:  Yeah , if you try to load s really long waveform into X Waves , you 'll be waiting there for  
G:  Oh 
G:  Loading the long  
G:  It takes a very long ti 
G:  Right . 
G:  It takes a l very long time . 
G:  Huh . 
G:  Actually , you 're talking about Transcriber , right ? 
G:  Huh . 
G:  Well we  we have a problem with that , 
G:  you know , time - wise on a  
G:  It - it 's a lot slower to load in a long file , 
G:  and also to check the file , 
G:  so if you have a transcript , um , 
G:  I mean it 's  
G:  I  I think overall you could get everything to work by accessing the same waveform and trying to find two  you know , the begin and end times . 
G:  Um , 
G:  but I think it 's more efficient , if we have the storage space , to have the small ones . 
G:  Yeah , it 's  
G:  Yeah . 
G:  It 's  it 's just  
G:  Right . 
G:  Yeah , so these wouldn't be backed up , the  
G:  Right . 
G:  And  
G:  Um , can I ask a question about the glossing , uh before we go on ? 
G:  So , for a word like " because " is it that it 's always predictably " because " ? 
G:  I mean , is " CUZ " always meaning " because " ? 
G:  Beca - because  
G:  Right .  <inbreath> Right . 
G:  Um , so , I guess  So , from the point of view of  
G:  The  the only problem is that with  for the recognition we  we map it to " because " , 
G:  and so if we know that " CUZ "  
G:  but , we don't  
G:  S 
G:  Right . 
G:  But , if it 's  
G:  OK . 
G:  But then there are other glosses that we don't replace , right ? 
G:  Because  
G:  OK . So , then it 's fine . 
G:  OK . 
G:  Yeah . 
G:  Right . 
G:  Actually we  we gave this to our pronunciation person , 
G:  she 's like , " I don't know what that is either " . 
G:  So . 
G:  No , we just gave her a list of words that , you know , weren't in our dictionary 
G:  and so of course it picked up stuff like this , 
G:  and she just didn't listen 
G:  so she didn't know . 
G:  We just  we 're waiting on that  just to do the alignments . 
G:  Maybe it 's " argh " ? 

G:  Yeah . 
G:  Right , no one say 
G:  Ah . 
G:  But it has a different prosody . 
G:  So , Jane , what 's the  d 
G:  I have one question about the the " EH " versus like the " AH " and the " UH " . 
G:  OK . 

G:  S OK . 
G:  " Eh , " yeah right , 
G:  cuz there were  were some speakers that did definite " eh 's " 
G:  but right now we  
G:  So , it  it 's actually probably good for us to know the difference between the real " eh " and the one that 's just like " uh " or transcribed " aaa " 
G:  cuz in  like in Switchboard , you would see e all of these forms , 
G:  but they all were like " uh " . 
G:  No , no , I mean like the  the " UH " , 
G:  or  the " UH " , " EH " , " AH " were all the same . 
G:  And then , we have this additional non - native version of  uh , like " eeh " . 
G:  Right . 
G:  But you 're a native German speaker 
G:  so it 's not a  not a issue for  
G:  It 's only  
G:  Onl 
G:  yeah . 
G:  No , only if you don't have lax vowels , I guess . 
G:  Right . 
G:  So it 's  like Japanese and Spanish 
G:  and  
G:  Yeah this is great . 
G:  This is really really helpful . 
G:  Right . 
G:  Right . 
G:  Yeah I mean cuz they  they have no idea , 
G:  right . 
G:  If you hear CTPD , I mean , they do pretty well 
G:  but it 's  
G:  you know how are  how are they gonna know ? 
G:  Yeah . 
G:  Right . 
G:  The recognizer , it is funny . 
G:  Kept getting PTA for PDA . 
G:  This is close , 
G:  right , 
G:  and the PTA was in these , uh , topics about children , 
G:  so , anyway . 
G:  Is the P - PTA working ? 
G:  I 'm wai 
G:  Well this is exactly how people will prove that these meetings do differ because we 're recording , right ? 
G:  Y no normally you don't go around saying , " Now you 've said it six times . 
G:  Now you 've said " 
G:  Yeah . 
G:  Well there 's not @ @ . 
G:  Right . 
G:  Huh . 
G:  But at some point  I mean , we probably shoul 
G:  But we should add it to the dictionar 
G:  No , to the pronunciation model . 
G:  Language , uh  
G:  Oh lan Oh OK - we OK 
G:  it 's in the language model , 
G:  w 
G:  yeah , but it so it 's the pronunciation model that has to have a pronunciation of " tickle " . 
G:  I 'm sorry ! 
G:  Oh , sorry . 
G:  What I meant is that there should be a pronunciation " tickle " for TCL as a word . 
G:  And that word in the  in , you know , it stays in the language model wherever it was . 
G:  Yeah you never would put " tickle " in the language model in that form , 
G:  yeah . 
G:  Right . 
G:  There 's actually a bunch of cases like this with people 's names and  
G:  Yes . 
G:  Yeah . 
G:  Yeah so th th there there 's a few cases like that where the um , the word needs to be spelled out in  in a consistent way as it would appear in the language , 
G:  but there 's not very many of these . 
G:  Tcl 's one of them . 
G:  Um , 
G:  y yeah . 
G:  Right . 
G:  Right . 
G:  We have this  there is this thing I was gonna talk to you about at some point about , you know , what do we do with the dictionary as we 're up updating the dictionary , 
G:  these changes have to be consistent with what 's in the  Like spelling people 's names and so forth . 
G:  If we make a spelling correction to their name , 
G:  like someone had Deborah Tannen 's name mispelled , 
G:  and since we know who that is , you know , we could correct it , 
G:  but  
G:  but we need to make sure we have the mispel 
G:  If it doesn't get corrected we have to have a pronunciation as a mispelled word in the dictionary . 
G:  Things like that . 
G:  So . 
G:  Right . Right . 
G:  So if there 's things that get corrected before we get them , it 's  it 's not an issue , 
G:  but if there 's things that um , we change later , then we always have to keep our  the dictionary up to date . 
G:  And then , yeah , in the case of " tickle " I guess we would just have a , you know , word " TCL " which  
G:  which normally would be an acronym , 
G:  you know , " TCL " 
G:  but just has another pronunciation . 
G:  Oh yeah . 
G:  Right , exactly . 
G:  Right . 
G:  Oh there is something spelled out " BEEEEEEP " 
G:  in the old  
G:  Thank you . 
G:  Because he was saying , " How many E 's do I have to allow for ? " 
G:  Right , thanks , 
G:  yeah . 
G:  Right . 
G:  Right . 
G:  Right . 
G:  Right . 
G:  Yeah . 
G:  Yeah . 
G:  Wait . 
G:  Oh , so you could do " grep minus V nums " . 
G:  So that 's the  
G:  yeah . 
G:  So there wouldn't be something like 
G:  i if somebody said something like , " Boy , I 'm really tired , OK . " and then started reading that would be on a separate line ? 
G:  OK great . 
G:  Cuz I was doing the " grep minus V " quick and dirty 
G:  and looked like that was working OK , 
G:  but  
G:  Great . 
G:  Now why do we  what 's the reason for having like the point five have the " NUMS " on it ? 
G:  Is that just like when they 're talking about their data or something ? 
G:  Or  
G:  These are all like inside the spontaneous  
G:  Oh OK . 
G:  I see . 
G:  OK . 
G:  So , I guess I 'd have one request here 
G:  which is just , um , maybe to make it more robust , 
G:  th that the tag , whatever you would choose for this type of " NUMS "  where it 's inside the spontaneous speech , is different than the tag that you use for the read speech . 
G:  Um , that way w if we make a mistake parsing , or something , we don't see the " point five " , 
G:  or  or it 's not there , then we 
G:  a Just  an And actually for things like " seven eighths " , or 
G:  people do fractions too I guess , 
G:  you  maybe you want one overall tag for sort of that would be similar to that , 
G:  or  
G:  As long as they 're sep as they 're different strings that we  that 'll make our p sort of processing more robust . 
G:  Cuz we really will get rid of everything that has the " NUMS " string in it . 
G:  Yeah . 
G:  It would probably be safer , if you 're willing , to have a separate tag 
G:  just because um , then we know for sure . 
G:  And we can also do counts on them without having to do the processing . 
G:  But you 're right , 
G:  we could do it this way , 
G:  it  it should work . 
G:  Um , 
G:  but it it 's probably not hard for a person to tell the difference 
G:  because one 's in the context of a  you know , a transcribed word string , 
G:  and  
G:  So  
G:  " Seven point five " . 
G:  The word . 
G:  Yeah . 
G:  And  and actually , you know the language  
G:  it 's the same point , actually , 
G:  the  the p you know , the word " to " and the word y th " going to " and " to go to " 
G:  those are two different " to 's " 
G:  and so there 's no distinction there . 
G:  It 's just  just the word " point " has  
G:  Yeah , every word has only one , yeah e one version 
G:  even if  even if it 's  
G:  A actually even like the word " read "  and " read " 
G:  Those are two different words . 
G:  They 're spelled the same way , right ? 
G:  And they 're still gonna be transcribed as READ . 
G:  So , 
G:  yeah , I  I like the idea of having this in there , 
G:  I just  I was a little bit worried that , um , the tag for removing the read speech  
G:  because i What if we have like " read letters " 
G:  or , I don't know , 
G:  like " read something " 
G:  like " read " 
G:  yeah , basically . 
G:  But other than that I it sounds great . 
G:  But that  that 's not hard . 
G:  I I think the harder part is making sure that the transc the transcription  
G:  So if you b merge two things , then you know that it 's the sum of the transcripts , 
G:  but if you split inside something , you don't where the word  which words moved . 
G:  And that 's wh that 's where it becomes a little bit  uh , having to rerun the processing . 
G:  The cutting of the waveforms is pretty trivial . 
G:  Right . 
