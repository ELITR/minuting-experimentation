A:  because that  that 'll really help us . 
A:  Why is it so cold in here ? 
A:  Well , I had a  just a quick question 
A:  but I know there was discussion of it at a previous meeting that I missed , 
A:  but just about the  the wish list item of getting good quality close - talking mikes on every speaker . 
A:  I mean , that was  
A:  Uh , we started  running recognition on  one conversation 
A:  but it 's the r  isn't working yet . 
A:  So , 
A:  But if anyone has  
A:  uh , the main thing would be if anyone has , um , knowledge about ways to , uh , post - process the wave forms that would give us better recognition , 
A:  that would be helpful to know about . 
A:  Ri - um , 
A:  yeah , the  
A:  And actually in addition to that , that the  the close talking mikes are worn in such a way as to best capture the signal . 
A:  And the reason here is just that for the people doing work 
A:  not on microphones but on sort of like dialogue and so forth , 
A:  uh  or and even on prosody , 
A:  which Don is gonna be working on soon , 
A:  it adds this extra , you know , vari variable for each speaker to  to deal with when the microphones aren't similar . 
A:  So  And I also talked to Mari this morning 
A:  and she also had a strong preference for doing that . 
A:  And in fact she said that that 's useful for them to know in starting to collect their data too . 
A:  Mm - hmm . 
A:  But we have more than one type of  
A:  I mean , for instance , you 're  
A:  Right . 
A:  But if we could actually standardize , you know , the  the microphones , uh , as much as possible that would be really helpful . 
A:  But y we could just record these signals separately and time align them with the start of the meeting . 
A:  Right . 
A:  Well , for short term research it 's just  there 's just so much effort that would have to be done up front n uh , 
A:  so  yeah , uniformity would be great . 
A:  Well Jane would know more about the transcribers . 
A:  Right . 
A:  So does the recognizer . 
A:  Even if  if you 're talking on someone else 's mike it 's still  you w 
A:  Right . 
A:  Right . 
A:  That 's OK . 
A:  Great , great . 
A:  Great , thank you very much . 
A:  It 's makes our job a lot easier . 
A:  You just search for Adam 's voice on each individual microphone , you pretty much know where everybody 's sitting . 
A:  Mm - hmm . 
A:  Right , 
A:  right . 
A:  Yeah . 
A:  Right . 
A:  I mean , it seems  
A:  it seems to me that there 's  you know , there are good political reasons for  for doing this , 
A:  just getting the data , 
A:  because there 's a number of sites  
A:  like right now SRI is probably gonna invest a lot of internal funding into recording meetings also , which is good , 
A:  um , but they 'll be recording with video 
A:  and they 'll be  
A:  You know , it 'd be nice if we can have at least , uh , make use of the data that we 're recording as we go 
A:  since it 's sort of  this is the first site that has really collected these really impromptu meetings , 
A:  um , and just have this other information available . 
A:  So , if we can get the investment in just for the infra infrastructure 
A:  and then , 
A:  I don't know , 
A:  save it out 
A:  or have whoever 's interested 
A:  save that data out , 
A:  transfer it there , 
A:  it 'd be g it 'd be good to have  have the recording . I think . 
A:  Well , if  
A:  Even if we 're not  
A:  I 'm not sure about video . 
A:  That 's sort of an  video has a little different nature since 
A:  right n right now we 're all being recorded but we 're not being taped . 
A:  Um , but it  definitely in the case of microphone arrays , since if there was a community interested in this , then  
A:  Mm - hmm . 
A:  Mm - hmm . 
A:  Well if there 's a way to say time  to sort of solve each of these f those  
A:  So suppose you can get an array in because there 's some person at Berkeley who 's interested and has some  equipment , 
A:  uh , and suppose we can  as we save it we can , you know , transfer it off to some other place that  that holds this  this data , 
A:  who 's interested , 
A:  and even if ICSI it itself isn't . 
A:  Um , and it  it seems like as long as we can time align the beginning , do we need to mix it with the rest ? 
A:  I don't know . 
A:  You know ? The 
A:  So  
A:  Yeah . 
A:  I mean it 's just  it 's worth considering as sort of 
A:  once you make the up front investment  and can sort of save it out each time , and  and not have to worry about the disk space factor , then it mi it might be worth having the data . 
A:  Right . 
A:  I see . 
A:  Mmm . 
A:  Mmm . 
A:  Right , I mean at least they 'd have the data and the transcripts , 
A:  and  
A:  Right . 
A:  OK . 
A:  Right , I mean , just  it 'd be nice if we have more information on the same data . 
A:  You know , 
A:  and  
A:  But it 's  if it 's impossible or if it 's a lot of effort then you have to just balance the two , 
A:  so  
A:  Right . 
A:  Right . Internally , 
A:  but I know there is interest from other places that are interested in looking at meeting data and having the video . 
A:  So it 's just  
A:  Right , that 's true . 
A:  Yeah . 
A:  You 're on recor you 're being recorded 
A:  and  
A:  Sorry , 
A:  can you explain what the ATLAS  
A:  I 'm not familiar with this ATLAS system . 
A:  Oh . Oh . 
A:  Mmm . 
A:  Hmm . 
A:  Yeah . 
A:  I  I think it 's sort of hard just playing the  you know , just having played the individual files . 
A:  And I  I mean , I know you . I know what your voice sounds like . I 'm sort of familiar with  
A:  Uh , it 's pretty hard to follow , especially 
A:  there are a lot of words that are so reduced phonetically that make sense when you know what the person was saying before . 
A:  Uh , it sort of depends where you are in  
A:  Mm - hmm . 
A:  Actually , are th so <laugh> are they giving any time markings ? 
A:  In other words , if  
A:  Yeah . 
A:  Cuz  
A:  OK . 
A:  Actu 
A:  yeah , Mar - Mari asked me the same question as sort of  
A:  um , you know , 
A:  yeah , 
A:  whether to  
A:  OK , so this is a , um , 
A:  and actually I should say this is what Don has b uh , he 's already been really helpful in , uh , chopping up these  
A:  So  so first of all you  
A:  um , I mean , for the SRI front - end , we really need to chop things up into pieces that are f not too huge . 
A:  Um , but second of all , uh  in general because some of these channels , 
A:  I 'd say , like , I don't know , at least half of them probably  on average are g are ha are  
A:  have a lot of cross - ta 
A:  sorry , some of the segments have a lot of cross - talk . 
A:  Um , it 's good to get sort of short segments if you 're gonna do recognition , 
A:  especially forced alignment . 
A:  So , 
A:  uh , Don has been taking a first stab actually using Jane 's first  the fir the meeting that Jane transcribed 
A:  which we did have some problems with , 
A:  and Thilo , uh , I think told me why this was , 
A:  but that people were switching microphones around  in the very beginning , 
A:  so  
A:  the SRI re 
A:  and they  
A:  They were not  
A:  Yeah . 
A:  So we have to sort of normalize  the front - end and so forth , and have these small segments . 
A:  So we 've taken that and chopped it into pieces based always on your  your , um , cuts that you made on the mixed signal . 
A:  And so that every  every speaker has the same cuts . 
A:  And if they have speech in it we run it through . 
A:  And if they don't have speech in it we don't run it through . 
A:  And we base that knowledge on the transcription . 
A:  Um , the problem is if we have no time marks , 
A:  then for forced alignment we actually don't know where  you know , in the signal the transcriber heard that word . 
A:  And so  
A:  I mean , if  if it 's a whole conversation and we get a long , uh , you know , par paragraph of  of talk , 
A:  uh , I don't know how they do this . 
A:  Um , we actually don't know which piece goes where . 
A:  And , um , I think with  
A:  No , we used the fact that  
A:  So when Jane transcribes them the way she has transcribers doing this , 
A:  whether it 's with the pre - segmentation or not , 
A:  they have a chunk and then they transcribes  the words in the chunk . 
A:  And maybe they choose the chunk or now they use a pre - segmentation and then correct it if necessary . 
A:  But there 's first a chunk and then a transcription . 
A:  Then a chunk , then a transcription . 
A:  That 's great , 
A:  cuz the recognizer can  
A:  Right , and it  it helps that it 's made based on sort of heuristics and human ear I think . 
A:  Th - but there 's going to be a real problem , 
A:  uh , even if we chop up based on speech silence these , uh , the transcripts from I B M , we don't actually know where the words were , 
A:  which segment they belonged to . 
A:  So that 's sort of what I 'm  worried about right now . 
A:  If you do a forced alignment on something really  
A:  well even if you do it on something really long you need to know  
A:  you can always chop it up 
A:  but you need to have a reference of which words went with which , uh , chop . 
A:  So  
A:  Yeah . 
A:  Maybe they have some  you know , maybe actually there is some , even if they 're not fine grained , maybe the transcribers  
A:  uh , I don't know , maybe it 's saved out in pieces or  or something . That would help . 
A:  But , 
A:  uh , it 's just an unknown right now . 
A:  So . 
A:  Right . But the  it is true that the segments  I haven't tried the segments that Thilo gave you 
A:  but the segments that in your first meeting are great . 
A:  I mean , that 's  that 's a good length . 
A:  Right , cuz  
A:  y yeah . 
A:  And actually as you get transcripts just , um , for new meetings ,  um , we can try  
A:  I mean , the  the more data we have to try the  the alignments on , um , the better . 
A:  So it 'd be good for  just to know as transcriptions are coming through the pipeline from the transcribers , 
A:  just to sort of  we 're playing around with sort of uh , parameters f on the recognizer , 
A:  cuz that would be helpful . 
A:  Especially as you get , en more voices . 
A:  The first meeting had I think just four people , 
A:  yeah . 
A:  Oh , great , 
A:  great . 
A:  So . 
A:  Mm - hmm . 
A:  Mmm . 
A:  Right . 
A:  Mmm . 
A:  So this is like gestural  uh , these g 
A:  Right . 
A:  OK . 
A:  Right . 
A:  But is  is the goal there to have this on meeting data , 
A:  like 
A:  so that you can do far field studies  of those gestures 
A:  or  um , 
A:  or is it because you think there 's a different kind of actual production in meetings  that people use ? 
A:  Or  ? 
A:  I see . 
A:  Uh - huh . 
A:  Right . 
A:  Mm - hmm . 
A:  Right . Yeah , that would be good . 
A:  Right . 
A:  That 's true . 
A:  I guess I wanted to , um , sort of make a pitch for trying to collect more meetings . 
A:  Um , 
A:  I actually I talked to Chuck Fillmore 
A:  and I think they 've what , vehemently said no before 
A:  but this time he wasn't vehement and he said you know , " well , Liz , come to the meeting tomorrow 
A:  and try to convince people " . 
A:  So I 'm gonna  try . Go to their meeting tomorrow 
A:  and see if we can try , uh , to convince them 
A:  because they have  
A:  And they have very interesting meetings from the point of view of a very different type of  of talk than we have here 
A:  and definitely than the front end meeting , probably . 
A:  Um  
A:  Well , yes and in terms of the  the fact that they 're describing abstract things 
A:  and , uh , just dialogue - wise , 
A:  right . 
A:  Um , so I 'll try . 
A:  And then the other thing is , 
A:  I don't know if this is at all useful , 
A:  but I asked Lila if I can maybe go around and talk to the different departments in this building 
A:  to see if there 's any groups that , for a free lunch , 
A:  if we can still offer that , 
A:  might be willing  
A:  non - ICSI , non - academic , 
A:  you know , like government people , 
A:  I don't know . 
A:  So . 
A:  Is  is it in these departments ? 
A:  Right , and then we could also  we might try advertising again 
A:  because I think it 'd be good if  if we can get a few different sort of non - internal types of meetings 
A:  and just also more data . 
A:  So . 
A:  So I actually wrote to him and he answered , " great , that sounds really interesting " . 
A:  But I never heard back 
A:  because we didn't actually advertise openly . 
A:  We a I mean w 
A:  I told  I d asked him privately . 
A:  Um , and it is a little bit of a trek for campus  folks . 
A:  Um , so it 's still worthwhile . 
A:  So  
A:  Exactly , 
A:  and  and  
A:  and I was thinking  
A:  Yeah . 
A:  Plus we could also get you know , a s a student . 
A:  And I 'm willing to try to learn . 
A:  I mean , I 'm  I would do my best . 
A:  Um , the other thing is that  there was a number of things at the transcription side that , um , transcribers can do , like dialogue act tagging , 
A:  disfluency tagging , 
A:  um , things that are in the speech that are actually something we 're y  working on for language modeling . 
A:  And Mari 's also interested in it , 
A:  Andreas as well . 
A:  So if you wanna process a utterance and the first thing they say is , " well " , and that " well " is coded as some kind of interrupt u tag . 
A:  Uh , and things like that , 
A:  um , 
A:  th 
A:  A lot of it can be done  
A:  Great . So a  a lot of this kind of  
A:  I think there 's a second pass 
A:  and I don't really know what would exist in it . 
A:  But there 's definitely a second pass worth doing to maybe encode some kinds of , you know , is it a question or not , 
A:  or  
A:  um , that maybe these transcribers could do . 
A:  So  
A:  Yeah . 
A:  That 'd be great . 
A:  Oh , yeah . 
A:  Oh , that we could . 
A:  OK . 
A:  I thought that all these people had sort of said " no " twice already . 
A:  If that 's not the case then  
A:  Just  
A:  OK . 
A:  Right . 
A:  Right . That was a big fear . 
A:  OK . 
A:  Mm - hmm . 
A:  Yeah , I mean , it  One thing that would be nice 
A:  and this  it sounds bizarre 
A:  but , I 'd really like to look at  to get some meetings where there 's a little bit of heated discussion , 
A:  like ar arguments and  or emotion , and things like that . 
A:  And so I was thinking if there 's any like Berkeley political groups or something . 
A:  I mean , that 'd be perfect . 
A:  Some group , " yes , we must  " 
A:  Well , you know , something  
A:  Um  
A:  Well , OK . 
A:  No , but maybe stu student , uh , groups or , um , film - makers , or som Something a little bit colorful . 
A:  And I don't mean that they 're angry 
A:  but just something with some more variation in prosodic contours and so forth would be neat . 
A:  So if anyone has ideas , 
A:  I 'm willing to do the leg work to go try to talk to people 
A:  but I don't really know which groups are worth pursuing . 
A:  Or  
A:  That 's  
A:  I was thinking , knowing the , uh , n National Institute of Standards , it is all  
A:  Yeah , 
A:  mm - hmm . 
A:  Exactly , that 's what I was  
A:  something where there  there is actually discussion where there 's no right or wrong answer but  but it 's a matter of opinion kind of thing . 
A:  Uh , anyway , if you  if you have ideas  
A:  Yeah , we could  
A:  I 'm  I 'm actually serious 
A:  because , uh , you know , we have the set up here 
A:  and  and that  that has a chance to give us some very interesting fun data . 
A:  So 
A:  if anyone has ideas , 
A:  if you know any groups that are m you know , 
A:  student groups c like clubs , things like that . 
A:  Not  not  
A:  Yeah . 
A:  " If you 're really angry at someone use our conference room . " 
A:  Oh , OK . 
A:  Really . 
A:  Or people who are really h 
A:  Well , far field mikes can pick up where they threw stuff on the wall . 
A:  Oh . <laugh> Yeah , right . 
A:  Well I think that  that was just sort of  
A:  I I already asked Thilo 
A:  but that , um , it would be helpful if I can stay in the loop somehow with , um , people who are doing any kind of post - processing , 
A:  whether it 's to separate speakers or to improve the signal - to - noise ratio , or both , 
A:  um , that we can sort of try out as we 're running recognition . 
A:  Um , so , i is that  Who else is work 
A:  I guess Dan Ellis and you 
A:  and Dave . 
A:  OK . 
A:  OK . 
A:  I don't know . 
A:  I 'm bad  
A:  It 's like   <laugh> like  
A:  It doesn't seem like big room acoustics problems to my ear 
A:  but I 'm not an expert . 
A:  It seems like a problem with cross - talk . 
A:  That  that may be true . 
A:  But I don't know how good it can get either by those  the  those methods  
A:  That 's true . 
A:  Oh , I don't know . 
A:  All I meant is just that as sort of  as this pipeline of research is going on we 're also experimenting with different ASR , uh , techniques . 
A:  And so it 'd be w good to know about it . 
A:  R right , although if they 're not talking , using the  the inhouse transcriptions , were sort of O K 
A:  because the t no one transcribed any words there 
A:  and we throw it out . 
A:  But if they 're talking at all and they 're not talking the whole time , 
A:  so you get some speech and then a " mm - hmm " , and some more speech , 
A:  so that whole thing is one chunk . 
A:  And the person in the middle who said only a little bit is picking up the speech around it , 
A:  that 's where it 's a big problem . 
A:  The energy , 
A:  right . Exactly . 
A:  Hmm . 
A:  Mm - hmm . 
A:  Well we try to find as close of start and end time of  as we can to the speech from an individual speaker , 
A:  because then we  we 're more guaranteed that the recognizer will  
A:  for the forced alignment which is just to give us the time boundaries , 
A:  because from those time boundaries then the plan is to compute prosodic features . 
A:  And the sort of more space you have that isn't the thing you 're trying to align the more errors we have . 
A:  Um , so , you know , that  that  it would help to have either pre - processing of a signal that creates very good signal - to - noise ratio , 
A:  which I don't know how possible this is for the lapel , 
A:  um , or to have very  to have closer , <breath> um , time  you know , synch times , basically , around the speech 
A:  that gets transcribed in it , or both . 
A:  And it 's just sort of a open world right now of exploring that . 
A:  So I just wanted to  see , you know , on the transcribing end from here things look good . 
A:  Uh , the IBM one is more  it 's an open question right now . 
A:  And then the issue of like global processing of some signal 
A:  and then , you know , before we chop it up is  is yet another way we can improve things in that . 
A:  Mm - hmm . 
A:  Right . You can , um  

A:  The problem is just that the acoustic  
A:  when the signal - to - noise ratio is too low , um , you  you 'll get , a uh  an alignment with the wrong duration pattern 
A:  or it  
A:  Yeah . It 's not the fact that you have like  
A:  I mean , what he did is allow you to have , uh , words that were in another segment move over to the  at the edges of  of segmentations . 
A:  Right , things  things near the boundaries where if you got your alignment wrong  
A:  cuz what they had done there is align and then chop . 
A:  Um , and this problem is a little bit j more global . 
A:  It 's that there are problems even in inside the alignments , 
A:  uh , because of the fact that there 's enough acoustic signal there t for the recognizer to  to eat , <laugh> as part of a word . 
A:  And it tends to do that . 
A:  S 
A:  So , uh , 
A:  but we probably will have to do something like that in addition . 
A:  Anyway . So , yeah , bottom  bottom line is just I wanted to make sure I can be aware of whoever 's working on these signal - processing techniques for , uh , detecting energies , 
A:  because that  that 'll really help us . 
