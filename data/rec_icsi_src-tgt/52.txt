{"dialogue": "none", "summary": "abstract: another weekly meeting on icsi 's meeting recorder group at berkeley , though the members are joined by a visiting researcher ."}
{"dialogue": "so , yeah , thethis past week i 've been main mainly occupied with , um , getting some results , u from the sri system trained on this short hub - five training set for the mean subtraction method . so the last week , uh , i showed some results with only speechdat - car so i was like looking into `` why , what is wrong with the ti - digits ? `` . and i found that , the noise estimation is a reason for the ti - digits to perform worse than the baseline . yeah , there are two figures showing actually the , mmm , um , performance of the current vad . well , i only say that thethis is , a summary of theof all the vts experiments", "summary": "abstract: the groups regulars reported progress on their work on mean subtraction , noise estimation , voice activity detection and the vector taylor series ."}
{"dialogue": "and then there 's um , another thing i wan na start looking at , um , <breath> wi is , um , the choice of the analysis window length . with thewith the htk set - up i should be able to do some experiments , on just varying that length , say between one and three seconds , in a few different reverberation conditions , i guess one thing that might also be an issue , uh , cuz part of what you 're doing is you 're getting aa spectrum over a bunch of different kinds of speech sounds . and so it might matter how fast someone was talking for instance . you know , if youififif there 's a lot of phones in one second maybe you 'll get aa really good sampling of all these different things , and <breath> and , uh , on the other hand if someone 's talking slowly maybe you 'd need more . a actually i was just thinking about what i was asking about earlier , wi which is about having <breath> less than say twelve seconds in the smartkom system to do the mean subtraction . you said in <breath> systems where you use cepstral mean subtraction , they concatenate utterances and , <breath> do you know how they address this issue of , um , testing versus training ? i think what they do is they do it always on - line , i mean , that you just take what you have from the past , that you calculate the mean of this and subtract the mean . and , um , soso in tha in that case , wh what do they do when they 're t um , performing the cepstral mean subtraction on the training data ? sobecause you 'd have hours and hours of training data . so do they cut it off and start over ? and so if you 're splitting things up into utterances so , for instance , in a dialogue system , where you 're gon na be asking , uh , you know , th for some information , there 's some initial th something . and i think the heuristics of exactly how people handle that and how they handle their training i 'm sure vary from place to place . so you 'dyouand so in training you would start over atat every new phone call or at every <breath> new speaker . itit seems to be the best whatwh wh whatwhat we can do in this moment is multi - condition training . and every when we now start introducing somesome noise reduction technique wewe introduce also somehow artificial distortions . and these artificial distortionsuh , i have the feeling that they are the reason whywhy we have the problems in this multi - condition training . that means the h m ms we trained , they arethey are based on gaussians , and if we introduce now thisthis u spectral subtraction , or wiener filtering stuff i mean , this is your noise estimate and you somehow subtract it or do whatever . and then i think what you do is you introduce somesome artificial distribution in this inin the models . soso , basically ourour position is <breath> that , um , we should n't be unduly constraining the latency at this point because we 're all still experimenting with trying to make the performance better in the presence of noise . uh , there is a minority in that group who is a arguingwho are arguing for <breath> um , uh , having a further constraining of the latency . so we 're s just continuing to keep aware of what the trade - offs are and , you know , whatwhat do we gain from having longer or shorter latencies ? well , france telecom waswaswas very short latency it was in the order of thirty milliseconds", "summary": "abstract: while on these topics , related areas discussed included recognition window length , training versus test set sizes , artificial distortion and latency concerns ."}
{"dialogue": "maybe youyou are leaving inin about two weeks carmen . whatwhat i would do is iii would pick @ @ the best consolation , which you think , and <breath> c createcreate all the results for the whole database that you get to the final number asas sunil did it and maybe also toto write somehow a document where you describe your approach , and what you have done . i was thinking to do that next week . i wi ii will do that next week .", "summary": "decisions: speaker fn002 is soon to be leaving the group , and so she will choose her best setup , run a complete set of experiments , and write up her work , procedure and results for next week ."}
{"dialogue": "so the other thing is thei 'm just looking at a little bit on the delay issue where the delay of the system is like a hundred and eighty millisecond . so <breath> i justjust tried another sk systemi mean , another filter which i 've like shown at the end . which is very similar to the existing uh , filter . onlyuh , only thing is that the phase isis like a totally nonlinear phase so it 's just likeit 's like a three percent relative degradation , butbut is thereis there a problem with the one hundred eighty milliseconds ?", "summary": "problems: new filters introduced to reduce latency by mn052 , performed slightly worse than those they replaced ."}
{"dialogue": "for italian and spanish it 'sth this value works good but not necessarily for finnish . but unfortunately there is , like , this forty millisecond latency yeah , so i would try to somewhat reduce this @ @ . i already know that if i completely remove this latency , so . <breath> um , itum there is a three percent hit on italian .", "summary": "problems: whereas mn007 has added some latency to the process which he feels he can reduce ."}
