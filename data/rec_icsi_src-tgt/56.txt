{"dialogue": "none", "summary": "abstract: icsi 's meeting recorder group met to discuss their progress in various aspects of the aurora project , but also to hear more about other developments relevant to the group ."}
{"dialogue": "yeah . so there was this conference call this morning , and the only topic on the agenda was just to discuss a and to come atuh , to get a decision about this latency problem .", "summary": "abstract: on the aurora project , there were reports on a project conference call , the status of the tandem neural networks , and progress with the mississippi state recognizer ."}
{"dialogue": "uh , yeah . there were like two hours ofdiscussions , and then suddenly , <breath> uh , people were tired , i guess , and they decided on < mike noise > a number , two hundred and twenty , included e including everything . so , currently d uh , we have system that has two hundred and thirty . we have to reduce it by ten milliseconds somehow . that 's not a problem , ii guess . w it 'sit 's p d primaryprimarily determined by the vad at this point , s so we can make the vad a little shorter . yeah . we probably should do that pretty soon so that we do n't get used to it being a certain way . uh , yeah . so , the second thing is the system that we have currently . oh , yes . we have , like , a system that gives sixty - two percent improvement , but <mouth> if you want to stick to the <breath> this latency well , it has a latency of two thirty , but <breath> if you want also to stick to the number <breath> of features thatlimit it to sixty , <breath> then we go a little bit down but it 's still sixty - one percent .", "summary": "abstract: the latency limit has been set , and the group 's system is performing very well , but is a little over ."}
{"dialogue": "um , while we 're still on aurora stuffmaybe you can talk a little about the status with the , uh , <breath> wall street journal <breath> things for it . so i 've , um , downloaded , uh , a couple of things from mississippi state . they wrote some scripts that sort of make it easy to run <breath> the system on the wall street journal , uh , data . um , so i have n't run the scripts yet . uh , i 'm waitingthere was one problem with part of it and i wrote a note to joe asking him about it . so i 'm waiting to hear from him . they 'rei i 'm still waiting for them torelease the , um , <mouth> multi - cpu version of their scripts , cuz right now their script only handles processing on a single cpu , which will take a really long time to run . so , as soon as they get that , then i 'lli 'll grab those too yeah . cuz we have to get started , yeah . i 'll go ahead and try to run it though with just the single cpu one , anditheythey , <breath> um , released like a smaller data set that you can use that only takes like sixteen hours to train and stuff . so i cani can run it on that just to make sure that the <breath> the thing works and everything . so it could bei mean , chuck and i had actually talked about this a couple times , andandover some lunches , i think , <breath> that , um , <mouth> one thing that we might wan na do the - there 's this question about , you know , what do you wan na scale ? suppose y you ca n't adjust <breath> these word insertion penalties and so forth , so you have to do everything at the level of the features . and , uh , one thing i had suggested at an earlier time was maybe some sort of scaling , some sort of root oror something of the , um , <mouth> uh , features . it occurred to me later , because what you really want to do is scale the , uh , @ @ the range of the likelihoods rather than but , i mean , i guess we still have n't had a <breath> a ruling back on this . and we may end up being in a situation where we just you know really ca n't change the <breath> word insertion penalty . but the other thing we could do <breath> isalso we could i mean , thisthis may not help us , <breath> uh , in the evaluation but it might help us in our understanding at least . we might , <breath> just run it with different insper insertion penalties , and show that , uh , `` well , ok , not changing it , <breath> playing the rules the way you wanted , we did this . but in fact if we did that , it made aa big difference . ``", "summary": "abstract: on the larger vocabulary task , there are still a few issues to resolve before work can really get started ."}
{"dialogue": "so michael kleinschmidt , who 's a phd student from germany , <breath> showed up this week . he 'll be here for about six months . and he 's done some work using <breath> an auditory modelof , um , <breath> human hearing , andusing that f uh , to generate speech recognition features . andhe did <breath> work back in germany <breath> with , um , a toy recognition system <breath> using , um , isolated <breath> digit recognition <breath> as the task . he w he 's coming here to u u use it on a <breath> uh , a real speech recognition system . th - this isbecause it 's , umthere are these different parameters for the shape of these <breath> basis functions , <breath> um <breath> there are a lot of different possible basis functions . and so he <breath> he actually does <breath> an optimization procedure to choose an <breath> an optimal set of basis functions out of all the possible ones . is , <breath> um , <mouth> he starts withhe has a set of m of them . i mean , he t he tries , um , <mouth> usingjust m minus one of them . so there are m possible subsets of this <breath> length - m vector . he tries classifying , using each of the m <breath> possible sub - vectors . whichever sub - vector , <breath> um , works thethe best , i guess , he says <breath> thethe fe feature that did n't use was the most useless feature , so we 'll throw it out and we 're gon na randomly select another featurefrom the set of possible basis functions . so i th i think it 'sit 'si think it 's kinda neat stuff . the thing that i wanted toto add to it also was to have us use this in a multi - stream way . soso that , um , <mouth> when you come up with these different things , <breath> and these different functions , <breath> you do n't necessarily just put them all into one huge vector , but perhaps < clears throat > you <breath> have some of them in one stream and some of them in another stream , and so forth . well , that sort of segues intowhatwhat i 'm doing . um , <breath> so , uh , the big picture is k um , <mouth> come up with a set of , <breath> uh , intermediate categories , then build intermediate category classifiers , then do recognition , um , so right now i 'm inin the phase where <breath> i 'm looking atat , um , deciding on a initial set of intermediate categories . and <breath> i 'm looking <breath> for data data - drivenmethods that can help me find , <breath> um , a set of intermediate categories <breath> of speech that , uh , will help me to discriminatelater down the line . and one of the ideas , <breath> um , that was to take atake a neural net traintrain an ordinary neural net <breath> to <breath> uh , to learn the posterior probabilities of phones . um , <mouth> the other onewas , <breath> um , to , <breath> uh , come up with aaa modelum , a graphical model , <breath> that treatsthe intermediate categories <breath> as hiddenhidden variables , latent variables , that we do n't know anything about , but that through , <breath> um , s statistical training and the em algorithm , <breath> um , at the end of the day , <breath> we have , umwe have learned something about thesethese latent , umlatent variables which happen to correspond to <breath> intermediate categories .", "summary": "abstract: the group heard of the plan of one of it 's member 's work into intermediate classifiers , and also of how a visiting research student 's work into auditory models can be applied to their work ."}
{"dialogue": "ho - how much memory d ? h how many ? i d i d uh , ii do n't kn remember exactly , yeah . i 'd like tosee that , cuz maybe i could think a little bit about it , cuz we <mouth> maybe we could make it a little smaller uh , i 'd like to see how far off we are . but i guess it 's still within their rules to havehave it on the , uh , t uh , server side .", "summary": "decisions: speaker me013 wants to know how much memory the tandem network takes up ."}
{"dialogue": "uh , yeah . there were like two hours ofdiscussions , and then suddenly , <breath> uh , people were tired , i guess , and they decided on < mike noise > a number , two hundred and twenty , included e including everything . so , currently d uh , we have system that has two hundred and thirty . we have to reduce it by ten milliseconds somehow . that 's not a problem , ii guess . w it 'sit 's p d primaryprimarily determined by the vad at this point , s so we can make the vad a little shorter . yeah . we probably should do that pretty soon so that we do n't get used to it being a certain way . uh , yeah . so , the second thing is the system that we have currently . oh , yes . we have , like , a system that gives sixty - two percent improvement , but <mouth> if you want to stick to the <breath> this latency well , it has a latency of two thirty , but <breath> if you want also to stick to the number <breath> of features thatlimit it to sixty , <breath> then we go a little bit down but it 's still sixty - one percent .", "summary": "problems: it is only a minor problem that the latency limit has been set below the current systems level , and also keeping the number of features within limits only drops performance a little ."}
{"dialogue": "uh , and if we drop the tandem network , then we have fifty - seven percent . uh , but th the two th two thirty includes the tandem network ? and i is the tandem network , uh , small enough that it will fit on the terminal size uh , no , i do n't think so . it 's stillin terms of computation , if we use , like , their way of computing thethe mapsthethe mips , <breath> i think it fits , but it 's , uh , m mainly a problem of memory . and i do n't know how muchthis can be discussed or not , because it 'sit could be in rom , so it 's maybe not that expensive . ho - how much memory d ? h how many ? i d i d uh , ii do n't kn remember exactly , yeah . i 'd like tosee that , cuz maybe i could think a little bit about it , cuz we <mouth> maybe we could make it a little smaller uh , i 'd like to see how far off we are . but i guess it 's still within their rules to havehave it on the , uh , t uh , server side .", "summary": "problems: a more significant problem is that the tandem approach may not fit in the memory space allowed , and removing it drops performance more ."}
{"dialogue": "yeah . the last thing is that i think we are getting close to human performance . well , that 's something i would like to investigate further , i did , like , umi did , uh , listen to the m most noisy utterances of the speechdat - car italian and tried to transcribe them . so this is a particular human . this isthis i this is stephane . that 's thethe flaw of the experiment . um , but what happens also is that if i listen to the , um <noise> a re - synthesized version of the speech andi re - synthesized this using a white noise that 's filtered by a lpc , uh , filter while our system is currently at seven percent . but still , uh , < breath-laugh > what happens isis that , <mouth> uh , the digit error rate on this is around one percent , well , you can argue , that , uhthat this is not speech , so the ear is not trained to recognize this . but s actually it sound likewhispering , there 's two problems there . i meani mean , soso the first is <breath> that by doing lpc - twelve with synthesized speech w like you 're saying , uh , it 's <breath> i i you 'reyou 're adding other degradation . so it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . and the second thing iswhich is m maybe more interestingis that , um , <breath> if you do it with whispered speech , you get this number . what if you haddone analysisre - synthesis and taken the pitch as well ? so now you put the pitch in . what would the percentage be then ? see , that 's the question . that would say at least for people , having the pitch is really , really important , i mean , th the thing is lpc is not aa really great representation of speech . uh , but i i do n't know . i do do n't wan na take you away from other things . yeah . i mean , it 's probably not worth your time . it 'sit 's a side thing andandand there 's a lot to do .", "summary": "problems: some of the group had issues with mn007 's approach to human performance testing , but this was considered more of a side issue ."}
