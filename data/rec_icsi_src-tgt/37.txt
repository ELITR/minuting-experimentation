{"dialogue": "so , uh , you 've got some , uh , xerox things to pass out ? ok , s so there is kind of summary of what has been done summary of experiments since , well , since last week and also since thewe 've started to runwork on this . um . so since last week we 've started to fill the column with um <mouth> uh features w with nets trained on plp with on - line normalization", "summary": "abstract: the main topic for discussion by the berkeley meeting recorder group was progress on the experiments run as part of the groups main project , a speech recogniser for the cellular industry ."}
{"dialogue": "so , uh , you 've got some , uh , xerox things to pass out ? yeah , i 'm sorry for the table , but as it grows in size , uh , it . uh , so for th the last column we use our imagination . ok , s so there is kind of summary of what has been done summary of experiments since , well , since last week and also since thewe 've started to runwork on this . um . so since last week we 've started to fill the column with um <mouth> uh features w with nets trained on plp with on - line normalization but with delta also , uhwhen we use the large training set using french , spanish , and english , you have one hundred and six without delta and eighty - nine with the delta . a and again all of these numbers are with a hundred percent being , uh , the baseline performance , and training with other languages is a little bit worse . we have a ninety - one number , so , it 's multi - english , and , yeah , and here the gap is still more important between using delta and not using delta . if y if i take the training s the large training set , it 'swe have one hundred and seventy - two , and one hundred and four when we use delta . uh . even if the contexts used is quite the same , except for the multi - english , which is always one of the best . then we started to work on a large dat database containing , uh , sentences from the french , from the spanish , from the timit , from spine , uh fromuh english digits , and from italian digits . anduh , actually we did this before knowing the result of all the data , uh , so we have to to redo the uhthe experiment training the net with , uh plp , but with delta . and first in the experiment - one ii doii use different mlp , yeah , and test across everything . so i guess the other thing is to takeyou knowif one were to take , uh , you know , a couple of the most successful of these , yeah , try all these different tests . we still have to work on finnish , um , basically , to make a decision on which mlp can be the best across the different languages . for the moment it 's the timit network , and perhaps the network trained on everything . uh , well , the next part of the document is , well , basically , a kind of summary of whateverything that has been done . so . we have seventy - nine m l ps trained on tenon ten different databases .", "summary": "abstract: this included reporting the results , and making conclusions to shape future work ."}
{"dialogue": "um , discussion with hynek , sunil and pratibha for trying to plug in their ourour networks with theirwithin their block diagram , uh , where to plug in thethe network , uh , after thethe feature , before as um a as a plugin or as a anoth another path , actually hynek would like to see , perhaps if you remember the block diagram there is , uh , temporal lda followed b by a spectral lda for each uh critical band . and he would like to replace these by a network which would , uh , make the system look like a trap . um , there are still open questions there , the future work is , well , try to connect to theto maketo plug in the system to the ogi where to put the mlp basically . so . uh , wewe wan na get their path running here , if so , we can add this other stuff . as an additional path yeah , the way we want to do it perhaps is tojust to get the vad labels and the final features . so they will send us thewell , provide us with the feature files , so weso . first thing of course we 'd wan na do there is to make sure that when we get those labels of final features is that we get the same results as them . without putting in a second path . yeah just th w i i just to make sure that wehavewe understand properly what things are , our very first thing to do is tois to double check that we get the exact same results as them on htk .", "summary": "abstract: also discussed were the details of the continued collaboration with project partner ogi ."}
{"dialogue": "so , on the msg uh problem um , i think that inin theum , in the shorttimesolution um , that is , um , trying to figure out what we can proceed forward with to make the greatest progress , i think it 's kind of in category that it 's , itit may be complicated . and uh it might beif someone 's interested in it , uh , certainly encourage anybody to look into it in the longer term , once we get out of this particular rushuh for results . but in the short term , unless you have somesome s strong idea of what 's wrong , butbut mymy guess would be that it 's something that is a simple thing that could take a while to find .", "summary": "decisions: further investigation into the lack of difference using msg features makes should not be made while they are on their current short time scale for results ."}
{"dialogue": "yeah , thatseems like a good thing to do , probably , not uh again a short - term sort of thing . the two of the main issues perhaps are still the language dependency <inbreath> and the noise dependency . and perhaps to try to reduce the language dependency , we should focus on finding some other kind of training targets . for moment you usewe use phonetic targets but we could also use articulatory targets , soft targets , and perhaps even , um use networks that does n't do classification but just regression and well , basically com com compute features and noit not , nnn , features without noise . i mean uh , transform the fea noisy features <inbreath> in other features that are not noisy .", "summary": "decisions: same goes for anything else that comes up and looks interesting , leave it for just now ."}
{"dialogue": "so i guess the other thing is to takeyou knowif one were to take , uh , you know , a couple of the most successful of these , yeah , and test across everything . yeah , try all these different tests . but one of the core quote '' open questions `` for that is um , um , if we take the uhyou know , the best ones here , maybe not just the best one , but the best few or something you want the most promising group from these other experiments . we know that there 's a mis there 's a uhaa loss in performance when the neural net is trained on conditions that are different thanthan , uh we 're gon na test on , but well , if you look over a range of these different tests um , how well do these different ways of combining the straight features with the mlp features , uh stand up over that range ? look at these different ways of combining it . and just looktake that case and then look over all the different things . how does thathow does that compare between the all the different test sets , and forand for the couple different ways that you have ofofof combining them . um . how well do they stand up , over the", "summary": "decisions: really should pick which results are looking the best at this stage , and take only them further ."}
{"dialogue": "but you have twotwo effects , the effect of changing language and the effect of training on something that 'sviterbi - aligned instead of handhand - labeled . do you think the alignments are bad ? i mean , have you looked at the alignments at all ? what the viterbi alignment 's doing ? might be interesting to look at it . yeah . butyeah . but , perhaps it 's not really thethe alignment that 's bad but thejust the ph phoneme string that 's used for the alignment the pronunciation models and so forth therethere might be errors just in theinin the ph string of phonemes . yeah , so this is not really the viterbi alignment ,", "summary": "decisions: someone should look closely at the non-timit databases , their viterbi alignments , and their phoneme strings to see is that is why timit is better ."}
{"dialogue": "so . uh , wewe wan na get their path running here , if so , we can add this other stuff . as an additional path yeah , the way we want to do it perhaps is tojust to get the vad labels and the final features . so they will send us thewell , provide us with the feature files , so weso . first thing of course we 'd wan na do there is to make sure that when we get those labels of final features is that we get the same results as them . without putting in a second path . yeah just th w i i just to make sure that wehavewe understand properly what things are , our very first thing to do is tois to double check that we get the exact same results as them on htk .", "summary": "decisions: need to get ogi 's system from them , and get it running like they do , before integrating into it ."}
{"dialogue": "so , it it 's stillit hurts youseems to hurt you a fair amount to add in this french and spanish . i wonder why well stephane was saying that they were n't hand - labeled , the french and the spanish . yeah ! as we mentioned , timit is the only that 's hand - labeled , and perhaps this is what makes the difference . yeah , the other are just viterbi - aligned . well , the timit network is still the best the fact that it 'sit 's hand - labeled . but you have twotwo effects , the effect of changing language and the effect of training on something that 'sviterbi - aligned instead of handhand - labeled . do you think the alignments are bad ? i mean , have you looked at the alignments at all ? what the viterbi alignment 's doing ? might be interesting to look at it . yeah . butyeah . but , perhaps it 's not really thethe alignment that 's bad but thejust the ph phoneme string that 's used for the alignment the pronunciation models and so forth therethere might be errors just in theinin the ph string of phonemes . yeah , so this is not really the viterbi alignment , we can , we can tell which training set gives the best result , but <mouth> we do n't know exactly why .", "summary": "problems: it is unclear if the english timit database is providing the best results because english is the best language or timit is the most accurately labelled dataset because it was hand labelled ."}
{"dialogue": "yeah , i 'm sorry for the table , but as it grows in size , uh , it . uh , so for th the last column we use our imagination . a and again all of these numbers are with a hundred percent being , uh , the baseline performance , and eighty - nine with the delta . uhwhen we use the large training set using french , spanish , and english , you have one hundred and six without delta yeah , so , it 's multi - english , we have a ninety - one number , and training with other languages is a little bit worse . if y if i take the training s the large training set , it 'swe have one hundred and seventy - two , and one hundred and four when we use delta . and this isthe results are on the other document . yeah , we ju just to be clear , the numbers here are uh recognition accuracy . yes , and the baselinethe baseline havei is eighty - two . baseline is eighty - two . yeah , eh , actually , if w we look at the table , the huge table , when you said the baseline system was uh , uh eighty - two percent , that was trained on what and tested on what ? that was , uh italian mismatched d uh , uh , digits , uh , is the testing , and the training is italian digits ? yeah . so the `` mismatch `` just refers to the noise andand , uh microphone and so forth , so , umso what that says is that in a matched condition , <sniff> we end up with a fair amount worse putting in the uh plp . now w woulddo we have a number , i suppose for the matched ii do n't mean matched , but uh use of italiantraining in italian digits for plp only ? so this isbasically this is in the table . uhso the number is fifty - two , another table . fifty - two percent . fift - sono , it 'sit 's the no , fifty - two percent of eighty - two ? ofofof uheighteen eighty . of eighteen . so it 'sit 's error rate , basically . it 's plus six . it 's er error rate ratio . oh this is accuracy ! yeah . uh , so we have nineninelet 's say ninety percent . oh , i 'm sorry , i k i keep getting confused because this is accuracy . yeah , sorry .", "summary": "problems: the results table was very large and difficult to follow ; it was unclear which of the numbers were error or accuracy rate , and straight rates or percentages of the baseline ."}
{"dialogue": "here the problem seems to be is that we do n't have a hug a really huge net with a really huge amount of training data . but we have s ffor this kind of task , i would think , sort of a modest amount . i mean , a million frames actually is n't that much . we have a modest amount ofof uh training data from a couple different conditions , and then uhinyeah , thatand the real situation is that there 's enormous variability that we anticipate in the test set in terms of language , and noise type uh , and uh , uh , channel characteristic ,", "summary": "problems: there is very limited training data , over only a few conditions ."}
{"dialogue": "and then uhinyeah , thatand the real situation is that there 's enormous variability that we anticipate in the test set in terms of language , and noise type uh , and uh , uh , channel characteristic ,", "summary": "problems: test and real data is likely to encompass much more variability ."}
