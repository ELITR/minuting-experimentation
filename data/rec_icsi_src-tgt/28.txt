{"dialogue": "and the interesting thing is that even though , <inbreath> yes , it 's a digits task and that 's a relatively small number of words and there 's a bunch of digits that you train on , <inbreath> it 's just not as good as having aa l very large amount of data and training up aaa nice good big <inbreath> hmm .", "summary": "abstract: the berkeley meeting recorder group discussed efforts to train and test the aurora group 's htk-based recognition system on icsi 's digits corpus ."}
{"dialogue": "two items , which was , uh , digits and possibly stuff onon , uh , forced alignment , so wewe only r hav i only looked at actually alignments from one meeting that we chose ,", "summary": "abstract: members also discussed efforts to produce forced alignments from a selection of meeting recorder data ."}
{"dialogue": "uh , but the other is that , um , the digits <inbreath> recorded here in this room with these close mikes , i uh , are actually a lot harder than thestudio - recording ti - digits . if you have only one utterance per speaker you might actually screw up on estimating thethe warping , uh , factor . well , i know there were some speaker labelling problems , um , after interruptions . but you 're actually saying that certain , uh , speakers were mis mis - identified .", "summary": "abstract: performance in both tasks was adversely affected by the manner of recording conditions implemented and difficulties attributing utterances to the appropriate speakers ."}
{"dialogue": "andandw wewe were able to get some definite improvement on the forced alignments by looking at them first and then realizing the kinds of errorsthat were occurring so just sort of working through a bunch of debugging kinds of issues .", "summary": "abstract: while debugging efforts resulted in improved forced alignments , dealing with mixed channel speech and speaker overlap remains a key objective for future work ."}
{"dialogue": "soso the keything that 's missing here is basically the ability to feed , you know , other features <outbreath> i into the recognizer and also then to train the system . we want to <inbreath> have the ability to feed it different features .", "summary": "abstract: the group is additionally focused on a continued ability to feed different features into the recognizer and then train the system accordingly ."}
{"dialogue": "yeah , bu although i 'd bei think it 'd be interesting to just take this exact actual system and try it out on ti - digits .", "summary": "decisions: for comparing meeting recorder digits results , it was decided that the aurora htk-based system should be tested on data from the ti digits corpus ."}
{"dialogue": "so , we might have to modify that script to recognize the , um , speakers , <inbreath> um , in thein the , uh , um , <mouth> ti - digitsdatabase . because we may have to do an extract to get theamount of data per speaker about right .", "summary": "decisions: the script for extracting speaker id information will require modifications to obtain a more accurate estimation of the amount of data recorded per speaker ."}
{"dialogue": "yeah . ii know what i was thinking was that maybe , uh , i i we could actually t t try at least looking at , uh , some of thethe large vocabulary speech from a far microphone , but i 'm saying if you do the same kind of limited thing <inbreath> as people have done in switchboard evaluations or asa could we do exactly the same thing that we 're doing now , but do it with a far - field mike ? but you use the acoustics from the far - field mike .", "summary": "decisions: subsequent recognition experiments will look at large vocabulary speech from a far-field microphone ( as performed in switchboard evaluations ) ."}
{"dialogue": "so , <inbreath> we would need a hand - marked , um , <mouth> word - level alignments or at least sort of the boundaries of the speech betw you know , between the speakers . and tune the parameters of theof the model , uh , to op to get the bestperformance .", "summary": "decisions: hand-marked , word-level alignments are needed to reveal speaker boundaries and tune the parameters of the model ."}
{"dialogue": "you know , interface - wise if you 're looking at speech , you wan na be able to know really where the words are . um , and see if you can in maybe incorporate it into the transcriber tool some way , yeah , it wou the advantage would just be that when you brought up a bin you would be ableif you were zoomed in enough in transcriber to see all the words , you would be able to , like , have the words sort of located in time ,", "summary": "decisions: modifications to the transcriber tool are required for allowing transcribers to simultaneously view the signal in xwaves and see where words are located in time ."}
{"dialogue": "none", "summary": "problems: digits training needs to be performed on a larger data set ."}
{"dialogue": "um , also you had the adaptation in the sri system , which we did n't have in this . so there was a significant loss from not doing the adaptation .", "summary": "problems: a significant loss in recognition resulted from not having included the type of phone-loop adaptation found in the sri system ."}
{"dialogue": "uh , but the other is that , um , the digits <inbreath> recorded here in this room with these close mikes , i uh , are actually a lot harder than thestudio - recording ti - digits .", "summary": "problems: recognition performance was worse for digits recorded in closed microphone conditions versus those recorded in a studio ( e.g . ti-digits ) ."}
{"dialogue": "uh , but the other is that , um , the digits <inbreath> recorded here in this room with these close mikes , i uh , are actually a lot harder than thestudio - recording ti - digits . i suspect that to get sort of the last bit out of these higher - quality recordings you would have to in fact , uh , use models that , uh , were trained on wider - band data . that 's where the most m acoustic mismatch is between the currently used models and thethe r the set up here . the near versus far .", "summary": "problems: a mismatch between the manner in which data were collected and the models used for doing recognition -- -e . g . bandwidth parameterization and the use of near- versus far-field microphones -- -was identified ."}
{"dialogue": "if you have only one utterance per speaker you might actually screw up on estimating thethe warping , uh , factor .", "summary": "problems: too little data per speaker can have a negative effect on vtl estimation ."}
{"dialogue": "because it 's further away from most of the people reading digits .", "summary": "problems: the pzm channel selected for obtaining digits data was too far away from most of the speakers ."}
{"dialogue": "you know , as liz said thewe f enforce the fact that , uh , the foreground speech has to be continuous . things like words that do occur just by themselvesa alone , like backchannels or something that we did allow to have background speech around it those would be able to do that , but the rest would be constrained .", "summary": "problems: current speech alignment techniques assume that foreground speech must be continuous and , barring some isolated words and backchannels , can not cope with overlapping background speech ."}
{"dialogue": "we probably want to adapt at least the foreground speaker . but , i guess andreas tried adapting both the foreground and a background generic speaker , and that 's actually a little bit of a f funky model . like , it gives you some weird alignments , just because often the background speakers match better to the foreground than the foreground speaker .", "summary": "problems: performing adaptations on both the foreground and background speaker produced a new variety of misalignments , a problem resulting , in part , from the fact that background speakers often match better to foreground conditionss ."}
{"dialogue": "tha - there are some cases like where thethe wrong speakeruh , these ca not a lot , but where thethe wrong personthethe speech is addre attached to the wrong speaker", "summary": "problems: transcribers occasionally misidentified speakers and omitted backchannels that were more hidden in the mixed signal ."}
