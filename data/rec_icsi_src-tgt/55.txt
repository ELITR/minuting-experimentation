{"dialogue": "none", "summary": "abstract: icsi 's meeting recorder group at berkeley meets to discuss , for the most part , progress on the aurora project ."}
{"dialogue": "um , i 've been playing with , first , the , um , vad . um , < clears throat > so it 's exactly the same approach ,", "summary": "abstract: the main areas being worked on were the voice activity detector and the tandem data streams ."}
{"dialogue": "but well , we could probably put the delta , um , <mouth> before on - line normalization . what if you used a smaller window for the delta ? i mean , i guess there 's a lot of things you could do to so if youif you put the delta before the , uh , ana on - lineif uhthenthen it could go in parallel . cuz the time constant of the on - line normalization is pretty long compared to the delta window , and you could experiment with cutting various pieces of these back a bit , i mean , we 're s we 're notwe 're not in terrible shape . well , what 's yourwhat 's your thought about what to do next with it ? i 'm surprised , because i expected the neural net to help more when there is more mismatch , as it was the case for the well , we mightuh , we might have to experiment with , uh better training sets . ithe other thing is , i mean , before you found that was the best configuration , but you might have to retest those things now that we have differentthe rest of it is different , for instance , what 's the effect of just putting the neural net on without the o otherother path ? i mean , you know what the straight features do . in the , uma lot of the , umthe hub - five systems , um , recently have been using lda . andand they , umthey run lda on the features right before they train the models . uh , this lda is different from the lda that you are talking about . the lda that yousaying is , like , you take a block of features , like nine frames or something , and then do an lda on it , and then reduce the dimensionality to something like twenty - four or something like that . so this is a two dimensional tile . and the lda that we are f applying is only in time , so it 's likemore like a filtering in time , but what if you putran the other kind of lda , uh , on your features right before they go into the hmm ? but it 'sit 's like a nonlinear discriminant analysis . the tandem stuff is kind of like i nonlinear lda . but i mean , w but the other features that you have , um , th the non - tandem ones , well , in the proposal , they were transformed u using pca , yeah , it might be that lda could be better . the uh , other thing i was wondering was , um , if the neural net , um , has anybecause of the different noise con unseen noise conditions for the neural net , where , like , you train it on those four noise conditions , while you are feeding it with , like , a additionalsome four plus somef few more conditions which it has n't seen , actually , instead of just h having c uh , those cleaned up t cepstrum , sh should we feed some additional information , likethethe i mean , should we f feed the vad flag , also , at the input so that itit has some additional discriminating information at the input ? we have the vad information also available at the back - end . so if it is something the neural net is not able to discriminate the classes so , by having an additional , uh , feature which says `` this is speech and this is nonspeech `` , i mean , it certainly helps in some unseen noise conditions for the neural net . so you 're saying , feed that , also , intothe neural net . yeah . so it it 's anadditional discriminating information . the other thingyou could do is just , um , p modify the , uh , output probabilities of theof the , uh , uh , um , neural net , tandem neural net , based on the fact that you have a silence probability .", "summary": "abstract: the group discussed possible further investigations that arose from these areas , including better linking the two ."}
{"dialogue": "and actually it brought up a question which may be relevant to the aurora stuff too . um , i know that when you figured out the filters that we 're using for the mel scale , there was some experimentation that went on atat , uhat ogi . but one of the differences that we found between the two systems that we were using , thethe aurora htk system baseline systemand the system that we werethethe uh , other system we were using , the uh , the sri system , was that the sri system had maybe a , um , hundred hertz high - pass . still , it 's possible that we 're getting in some more noise . so i wonder , is it @ @ was theretheir experimentation with , uh , say , throwing away that filter or something ? so i think whenwhen he gets done with his prelim study i think <laugh> one of the next things we 'd want to do is to take this , uhuh , noise , uh , processing stuff andand , uhuh , synthesize some speech from it .", "summary": "abstract: they also consider how aspects of an absent member 's work might be applied to the current project ."}
{"dialogue": "so i wo n't be here for uh , i 'm leaving next wednesday . i 'm leavingleaving next wednesday . so next week i wo n't , and the week after i wo n't , cuz i 'll be in finland . by that time you 'll beuh , you 'll both be gonefrom here . so it 'll be a few weeks , really , before we have a meeting of the same cast of characters . and then uh , uh , we 'll start up again with dave anddave and barry and stephane and us on the , uh , twentieth .", "summary": "abstract: the meeting closed with a discussion of upcoming absences , and how meetings would continue ."}
{"dialogue": "is there any word yet about the issues about , um , adjustments for different feature sets or anything ? you asked me to write to him uh , i 'lli 'll d i 'll double check that and ask him again . it 's like thatthat could r turn out to be an important issue for us .", "summary": "decisions: speaker me018 must confirm what is needed to work with the new software in terms of adjustments with someone further up the project chain ."}
{"dialogue": "for instance , what 's the effect of just putting the neural net on without the o otherother path ? i mean , you know what the straight features do . when youinin the old experiments when you ran with the neural net only , and did n't have this side path , um , uh , with thethe pure features as well , did it make things better to have the neural net ? it wasb a little bit worse . until you put the second path in with the pure features , the neural net was n't helping at all . it was helping , uh , if the features are b were bad , as soon as we added lda on - line normalization , and < clears throat > all these things , then well , i still think it would be k sort of interesting to see what would happen if you just had the neural net without the side thing . andand the thing ii have in mind is , uh , maybe you 'll see that the results are not just a little bit worse . maybe that they 're a lot worse . but if on the ha other hand , uh , it 's , say , somewhere in between what you 're seeing now andandand , uh , what you 'd have with just the pure features , then maybe there is some problem of aof a , uh , combination of these things , or correlation between them somehow . if it really is that the net is hurting you at the moment , then i think the issue is to focus onon , uh , improving thethe net .", "summary": "decisions: the system at it 's current stage employs the neural networks and second stream , but the group leader would like the network investigated separately , incase it is hurting performance ."}
{"dialogue": "is there any word yet about the issues about , um , adjustments for different feature sets or anything ? it 's like thatthat could r turn out to be an important issue for us . cuz they have , uh , already frozen those in i insertion penalties and all those stuff is whati feel . and they have these tables with , uh , various language model weights , insertion penalties . so now , we may come back to the situation where we may be looking for a modification of the features to account for the fact that we ca n't modify these parameters . but it 's still worth , i think , justsinceyou know , just chatting with joe about the issue .", "summary": "problems: there are worries regarding the need to make adjustments so the new software can handle the group 's different feature set ."}
{"dialogue": "but the problem is still that the latency is too large . thethe latency of the vad is two hundred and twenty milliseconds . wh - what 's the baseline you need to be under ? well , we do n't know . they 're still arguing about it . i mean , if it 's twoifif it 's , uhif it 's two - fifty , then we could keep the delta where it is if we shaved off twenty . if it 's two hundred , if we shaved off twenty , we couldwe could , uh , meet it by moving the delta back . so , how do you know that what you have is too much if they 're still deciding ? i mean , the main thing is that since that we got burned last time , andyou know , by not worrying about it very much , we 're just staying conscious of it . i mean , ififif a week before we have to be done someone says , `` well , you have to have fifty milliseconds less than you have now `` , it would be pretty frantic around here .", "summary": "problems: the system , whilst improved , also has increased latency , and while the limit has not been set , the group need to reduce it ."}
{"dialogue": "you 're just using the full ninety features ? from the networks , it 's twenty - eight . and from the other side it 's forty - five . so it 'syou have seventy - three features , there 's a klt after the neural network , asas before . that 's how you get down to twenty - eight ? i wanted to do something very similar to the proposal as a firstfirst try . but we have tofor sure , we have to go down , because the limit is now sixty features . we have to find a way to decrease the number of features . they felt they wanted to set a limit . so they chose sixty . iii think it 's kind of r arbitrary too .", "summary": "problems: likewise the number of features the use in their system , since this has been set at an arbitrarily low value ."}
{"dialogue": "yeah , actually < clears throat > to s eh , what i observed in the hm case is that the number of deletion dramatically increases . itit doubles . when i added the num the neural network it doubles the number of deletions . yeah , so i do n't you know <laugh> how to interpret that , me either . andand didan other numbers stay the same ? they p stayed the same , did they increase the number of deletions even for the cases that got better ? no . so it 's only the highly mismatched ? now the only thing thatthat bothers me about all this is that iiithethe fact i i it 's sort of bothersome that you 're getting more deletions . so i might maybe look at , is it due to the fact that um , the probability of the silence at the output of the network , is , tootoo high", "summary": "problems: there has been an increase in the number of deletion in the errors , which is of some concern ."}
